{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msdsn/GaitLab/blob/main/ConvLstm_(LR%3D0_05)_Fusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjEAYJIjdbNH"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYVtSgOrdjmf",
        "outputId": "8aae4ea5-51ea-4a02-8744-31f7dbb25570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1d-DGCtaex6YZNg2ad2aPzRWW0xn4bVj3\n",
            "From (redirected): https://drive.google.com/uc?id=1d-DGCtaex6YZNg2ad2aPzRWW0xn4bVj3&confirm=t&uuid=e13f2461-6b52-4d5e-8abf-7eca141da97b\n",
            "To: /content/GaitDatasetB-silh.zip\n",
            "100% 659M/659M [00:03<00:00, 183MB/s]\n",
            "Archive:  /content/GaitDatasetB-silh.zip\n",
            "   creating: /content/GaitDatasetB/GaitDatasetB-silh/\n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/001.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/002.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/003.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/004.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/005.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/006.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/007.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/008.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/009.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/010.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/011.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/012.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/013.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/014.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/015.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/016.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/017.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/018.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/019.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/020.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/021.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/022.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/023.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/024.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/025.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/026.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/027.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/028.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/029.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/030.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/031.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/032.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/033.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/034.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/035.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/036.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/037.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/038.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/039.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/040.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/041.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/042.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/043.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/044.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/045.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/046.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/047.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/048.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/049.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/050.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/051.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/052.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/053.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/054.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/055.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/056.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/057.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/058.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/059.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/060.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/061.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/062.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/063.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/064.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/065.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/066.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/067.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/068.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/069.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/070.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/071.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/072.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/073.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/074.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/075.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/076.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/077.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/078.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/079.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/080.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/081.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/082.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/083.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/084.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/085.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/086.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/087.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/088.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/089.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/090.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/091.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/092.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/093.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/094.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/095.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/096.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/097.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/098.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/099.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/100.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/101.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/102.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/103.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/104.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/105.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/106.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/107.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/108.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/109.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/110.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/111.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/112.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/113.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/114.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/115.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/116.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/117.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/118.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/119.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/120.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/121.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/122.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/123.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/124.tar.gz  \n"
          ]
        }
      ],
      "source": [
        "!gdown 1d-DGCtaex6YZNg2ad2aPzRWW0xn4bVj3\n",
        "!unzip \"/content/GaitDatasetB-silh.zip\" -d \"/content/GaitDatasetB\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwP-PDlBdlXh"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "import os\n",
        "\n",
        "def unzip_tar_gz(folder_path):\n",
        "  for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".tar.gz\"):\n",
        "      filepath = os.path.join(folder_path, filename)\n",
        "      try:\n",
        "        with tarfile.open(filepath, \"r:gz\") as tar:\n",
        "          tar.extractall(folder_path)\n",
        "          #print(f\"Extracted: {filename}\")\n",
        "        # remove tar.gz\n",
        "        os.remove(filepath)\n",
        "      except Exception as e:\n",
        "        print(f\"Error extracting {filename}: {e}\")\n",
        "\n",
        "unzip_tar_gz(\"/content/GaitDatasetB/GaitDatasetB-silh\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRPyzXUCdr7S",
        "outputId": "934dec26-5dca-44ec-f559-d5cad34e8211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'OpenGait'...\n",
            "remote: Enumerating objects: 1848, done.\u001b[K\n",
            "remote: Counting objects: 100% (485/485), done.\u001b[K\n",
            "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
            "remote: Total 1848 (delta 385), reused 362 (delta 344), pack-reused 1363 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1848/1848), 20.23 MiB | 40.29 MiB/s, done.\n",
            "Resolving deltas: 100% (1163/1163), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ShiqiYu/OpenGait.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqUtnw6Sdw85",
        "outputId": "6a202017-ee25-49ef-fc67-ad27c463f633"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretreating: 100% 13593/13593 [08:09<00:00, 27.76folder/s]\n"
          ]
        }
      ],
      "source": [
        "!python \"/content/OpenGait/datasets/pretreatment.py\" --input_path \"/content/GaitDatasetB/GaitDatasetB-silh\" --output_path CASIA-B-pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb-TXUMBd0vF",
        "outputId": "2af9da05-a3b9-4d13-8d7c-68b6a048246e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Collecting kornia\n",
            "  Downloading kornia-0.7.4-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.67.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.26.4)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\n",
            "Collecting kornia-rs>=0.1.0 (from kornia)\n",
            "  Downloading kornia_rs-0.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (24.2)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.5.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9.1->kornia) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Downloading kornia-0.7.4-py2.py3-none-any.whl (899 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.4/899.4 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kornia-rs, kornia\n",
            "Successfully installed kornia-0.7.4 kornia-rs-0.1.7\n"
          ]
        }
      ],
      "source": [
        "pip install tqdm pyyaml tensorboard opencv-python kornia einops torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBUB8E-Gd346"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import os.path as osp\n",
        "import json\n",
        "import copy\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import torch.utils.data as tordata\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision.models.resnet import ResNet, BasicBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBeK28oYd9Ax"
      },
      "outputs": [],
      "source": [
        "class DataSet(tordata.Dataset):\n",
        "    def __init__(self, data_set):\n",
        "        \"\"\"\n",
        "            seqs_info: the list with each element indicating\n",
        "                            a certain gait sequence presented as [label, type, view, paths];\n",
        "        \"\"\"\n",
        "        self.data_set = data_set\n",
        "        self.__dataset_parser()\n",
        "        self.label_list = [seq_info[0] for seq_info in self.seqs_info]\n",
        "        self.types_list = [seq_info[1] for seq_info in self.seqs_info]\n",
        "        self.views_list = [seq_info[2] for seq_info in self.seqs_info]\n",
        "\n",
        "        self.label_set = sorted(list(set(self.label_list)))\n",
        "        self.types_set = sorted(list(set(self.types_list)))\n",
        "        self.views_set = sorted(list(set(self.views_list)))\n",
        "        self.seqs_data = [None] * len(self)\n",
        "        self.indices_dict = {label: [] for label in self.label_set}\n",
        "        for i, seq_info in enumerate(self.seqs_info):\n",
        "            self.indices_dict[seq_info[0]].append(i)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs_info)\n",
        "\n",
        "    def __loader__(self, pth):\n",
        "        if pth.endswith('.pkl'):\n",
        "            with open(pth, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            f.close()\n",
        "        return data\n",
        "    def __getitem__(self, idx):\n",
        "        if self.seqs_data[idx] is None:\n",
        "            data = self.__loader__(self.seqs_info[idx][-1])\n",
        "            self.seqs_data[idx] = data\n",
        "        else:\n",
        "            data = self.seqs_data[idx]\n",
        "        seq_info = self.seqs_info[idx]\n",
        "        return data, seq_info\n",
        "\n",
        "    def __load_all_data(self):\n",
        "        for idx in range(len(self)):\n",
        "            self.__getitem__(idx)\n",
        "\n",
        "    def __dataset_parser(self):\n",
        "        dataset_root = '/content/CASIA-B-pkl'\n",
        "\n",
        "\n",
        "        def get_seqs_info_list(label_set):\n",
        "            seqs_info_list = []\n",
        "            for lab in label_set:\n",
        "                for typ in sorted(os.listdir(osp.join(dataset_root, lab))):\n",
        "                    for vie in sorted(os.listdir(osp.join(dataset_root, lab, typ))):\n",
        "                        seq_info = [lab, typ, vie]\n",
        "                        seq_path = osp.join(dataset_root, *seq_info)\n",
        "                        dir = os.listdir(seq_path)\n",
        "                        flag = False\n",
        "                        for file in dir:\n",
        "                            if file.endswith(\".pkl\"):\n",
        "                                pth = osp.join(seq_path, file)\n",
        "                                seqs_info_list.append([*seq_info, pth])\n",
        "                                flag = True\n",
        "                                break;\n",
        "                        if not flag:\n",
        "                            print(\n",
        "                                'Find no .pkl file in %s-%s-%s.' % (lab, typ, vie))\n",
        "            return seqs_info_list\n",
        "\n",
        "        self.seqs_info = get_seqs_info_list(self.data_set) #if training else get_seqs_info_list(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SIi_MeXd_CN"
      },
      "outputs": [],
      "source": [
        "class CollateFn(object):\n",
        "    def __init__(self, label_set):\n",
        "        self.label_set = label_set\n",
        "        self.frames_num = 30\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # list of (data, seq_info), data if array of pkl files in a view, seq_info is => (lab, type, view, paths-(pkl files))\n",
        "        batch_size = len(batch)\n",
        "        seqs_batch, labs_batch, typs_batch, vies_batch = [], [], [], []\n",
        "        for bt in batch:\n",
        "            seq_len = len(bt[0])\n",
        "            replace= seq_len < self.frames_num\n",
        "            indices = list(range(seq_len))\n",
        "            indices = np.random.choice(indices, size=self.frames_num, replace=replace)\n",
        "            # numpy.ndarray with numpy.array() before converting to a tensor\n",
        "            seq_numpy = np.array([bt[0][i] for i in indices])\n",
        "            seq_tensor = torch.tensor(seq_numpy)\n",
        "            seqs_batch.append(seq_tensor)\n",
        "            lab_numpy = np.array([self.label_set.index(bt[1][0])])\n",
        "            lab_tensor = torch.tensor(lab_numpy)\n",
        "            labs_batch.append(lab_tensor)\n",
        "            typs_batch.append(bt[1][1])\n",
        "            vies_batch.append(bt[1][2])\n",
        "        seqs_batch = torch.stack(seqs_batch).float()\n",
        "        seqs_batch = seqs_batch.unsqueeze(2)\n",
        "        seqs_batch = seqs_batch / 255.0\n",
        "\n",
        "        labs_batch = torch.stack(labs_batch).squeeze(1).long()\n",
        "\n",
        "        batch = [seqs_batch, labs_batch, typs_batch, vies_batch]\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-R9RM1xeD_H"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch.utils.data as tordata\n",
        "class TripletSampler(tordata.sampler.Sampler):\n",
        "    def __init__(self, dataset, batch_size, batch_shuffle=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        if len(self.batch_size) != 2:\n",
        "            raise ValueError(\n",
        "                \"batch_size should be (P x K) not {}\".format(batch_size))\n",
        "        self.batch_shuffle = batch_shuffle\n",
        "        self.world_size = 1\n",
        "        self.rank = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            sample_indices = []\n",
        "            pid_list = random_sample_list(\n",
        "                self.dataset.label_set, self.batch_size[0])\n",
        "\n",
        "            for pid in pid_list:\n",
        "                indices = self.dataset.indices_dict[pid]\n",
        "                indices = random_sample_list(\n",
        "                    indices, k=self.batch_size[1])\n",
        "                sample_indices += indices\n",
        "\n",
        "            if self.batch_shuffle:\n",
        "                sample_indices = random_sample_list(\n",
        "                    sample_indices, len(sample_indices))\n",
        "\n",
        "            total_batch_size = self.batch_size[0] * self.batch_size[1]\n",
        "            sample_indices += sample_indices[:(total_batch_size - len(sample_indices))]\n",
        "            yield sample_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "def random_sample_list(obj_list, k):\n",
        "    if len(obj_list) < k:\n",
        "        idx = random.choices(range(len(obj_list)), k=k)\n",
        "    else:\n",
        "        idx = random.sample(range(len(obj_list)), k=k)\n",
        "    return [obj_list[i] for i in idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-hCSXaweH5p"
      },
      "outputs": [],
      "source": [
        "with open('/content/OpenGait/datasets/CASIA-B/CASIA-B.json', \"rb\") as f:\n",
        "    partition = json.load(f)\n",
        "train_set = partition[\"TRAIN_SET\"]\n",
        "test_set = partition[\"TEST_SET\"]\n",
        "label_list = os.listdir('/content/CASIA-B-pkl')\n",
        "train_set = [label for label in train_set if label in label_list]\n",
        "test_set = [label for label in test_set if label in label_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi4IMVtGeL8g"
      },
      "outputs": [],
      "source": [
        "dataset = DataSet(data_set=train_set)\n",
        "sampler = TripletSampler(dataset, batch_size=[8, 16])\n",
        "collate_fn = CollateFn(dataset.label_set)\n",
        "train_loader = tordata.DataLoader(dataset, batch_sampler=sampler, collate_fn=collate_fn, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53IizD2c3BWP",
        "outputId": "76cec236-7e6a-465c-e5eb-761d8c370799"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 30, 1, 64, 64])\n",
            "torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "for batch in train_loader:\n",
        "  seqs, labs, types, views = batch\n",
        "  print(seqs.shape) # (n, s, c, h, w)\n",
        "  print(labs.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qa7rVfO3BvP"
      },
      "source": [
        "# Create Your Own Model Here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXv-IUyY36KJ"
      },
      "outputs": [],
      "source": [
        "class HorizontalPoolingPyramid():\n",
        "    \"\"\"\n",
        "        Horizontal Pyramid Matching for Person Re-identification\n",
        "        Arxiv: https://arxiv.org/abs/1804.05275\n",
        "        Github: https://github.com/SHI-Labs/Horizontal-Pyramid-Matching\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.bin_num = [16]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "            x  : [n, c, h, w]\n",
        "            ret: [n, c, p]\n",
        "        \"\"\"\n",
        "        n, c = x.size()[:2]\n",
        "        features = []\n",
        "        for b in self.bin_num:\n",
        "            z = x.view(n, c, b, -1)\n",
        "            z = z.mean(-1) + z.max(-1)[0]\n",
        "            features.append(z)\n",
        "        return torch.cat(features, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Rf5zrmVuGhY"
      },
      "source": [
        "### Resnetin çıktısı: `n, s, c, h, w`\n",
        "### Biz bu verktörü `(n * h * w), s, c` olarak grupladığımız zaman şunu sağlamış oluyoruz:\n",
        "\n",
        "Her sampledaki her spatial location için frame boyunca kanal bilgisi.\n",
        "\n",
        "`Yani yürüyüşlerdeki her piksel için zaman x kanal(512)` bilgisi.\n",
        "\n",
        "### Bunu LSTM ne yapabilir.\n",
        "\n",
        "LSTM sonunda biz her Spatial Location(h,w) için, Sequencetaki Temporal Dependenci'leri yakalamış oluruz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDw993KNePEw"
      },
      "outputs": [],
      "source": [
        "class CustomModel(ResNet):\n",
        "    def __init__(self):\n",
        "      super(CustomModel, self).__init__(BasicBlock, [1, 1, 1, 1])\n",
        "      layers = [1, 1, 1, 1]\n",
        "      self.inplanes = 64\n",
        "      self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "      self.layer1 = self._make_layer(BasicBlock, 64, layers[0])\n",
        "      self.layer2 = self._make_layer(BasicBlock, 128, layers[1], dilate=False)\n",
        "      self.layer3 = self._make_layer(BasicBlock, 256, layers[2], dilate=False)\n",
        "      self.layer4 = self._make_layer(BasicBlock, 512, layers[3], dilate=False)\n",
        "      self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=1, batch_first=True)\n",
        "      self.fusion_conv = nn.Conv2d(512+256, 256, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "      self.hpp = HorizontalPoolingPyramid()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # x = torch.Size([128*30, 1, 64, 64]) (n*s, c, h, w)\n",
        "        x = self.conv1(x) # torch.Size([3840, 64, 32, 32]) (n*s, c=64, h, w)\n",
        "        x = self.bn1(x) # torch.Size([3840, 64, 32, 32])\n",
        "        x = self.relu(x) # torch.Size([3840, 64, 32, 32])\n",
        "        x = self.maxpool(x) # torch.Size([3840, 64, 16, 16])\n",
        "\n",
        "        x = self.layer1(x) # torch.Size([3840, 64, 16, 16])\n",
        "        x = self.layer2(x) # torch.Size([3840, 128, 16, 16])\n",
        "        x = self.layer3(x) # torch.Size([3840, 256, 16, 16])\n",
        "        x = self.layer4(x) # torch.Size([3840, 512, 16, 16]) (n*s, c=512, h, w)\n",
        "\n",
        "        n_s, c, h, w = x.size()\n",
        "\n",
        "        # --- LSTM Yolu ---\n",
        "        x1 = x.view(128, 30, c, h, w) # (n, s, c, h, w) = torch.Size([128, 30, 512, 16, 16])\n",
        "        # Permute to [n, h, w, s, c]\n",
        "        x1 = x1.permute(0, 3, 4, 1, 2)  # [n, h, w, s, c]\n",
        "        n, h, w, s, c = x1.shape\n",
        "        # Flatten batch and spatial dimensions\n",
        "        x1 = x1.contiguous().view(n * h * w, s, c)  # [n*h*w, s, c]\n",
        "        # Pass through LSTM\n",
        "        x1, _ = self.lstm(x1)  # x: [n*h*w, s, hidden_size]\n",
        "        # Get embedding from the last time step\n",
        "        x1 = x1[:, -1, :]  # [n*h*w, hidden_size]\n",
        "        # Reshape back to [n, h, w, hidden_size]\n",
        "        x1 = x1.view(n, h, w, -1)\n",
        "        # Permute to [n, hidden_size, h, w]\n",
        "        x1 = x1.permute(0, 3, 1, 2)  # [n, hidden_size=256, h, w]\n",
        "\n",
        "        # --- Torch Max Yolu ---\n",
        "        x2 = x.reshape(128, 30, 512, 16, 16).transpose(1, 2).contiguous()\n",
        "        x2 = torch.max(x2, dim=2)[0]  # (n, c, h, w)\n",
        "\n",
        "        # --- Füzyon ---\n",
        "        x_concat = torch.cat((x1, x2), dim=1)  # (n, c + hidden_size, h, w)\n",
        "        x_fused = self.fusion_conv(x_concat)\n",
        "\n",
        "        x = self.hpp(x_fused) # # [n, c=256, p=16]\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImiuWkH3eoNX"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laj-m2ChepP7"
      },
      "outputs": [],
      "source": [
        "# Iterative Triplet Loss Implementation\n",
        "class TripletLossIterative(nn.Module):\n",
        "    def __init__(self, margin=0.2):\n",
        "        super(TripletLossIterative, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, embeddings, labels):\n",
        "        n, c, p = embeddings.size()\n",
        "        embeddings = embeddings.permute(2, 0, 1).contiguous().float()  # [p, n, c]\n",
        "        device = embeddings.device\n",
        "        total_loss = 0.0\n",
        "        total_triplets = 0\n",
        "\n",
        "        for part in range(p):  # Loop over each part\n",
        "            emb_part = embeddings[part]  # [n, c]\n",
        "            dist_mat = torch.cdist(emb_part, emb_part, p=2)  # [n, n]\n",
        "\n",
        "            for i in range(n):  # Loop over each anchor\n",
        "                anchor_label = labels[i]\n",
        "                anchor_dist = dist_mat[i]  # Distances from anchor to all samples\n",
        "\n",
        "                # Positive and negative indices\n",
        "                pos_indices = torch.where((labels == anchor_label) & (torch.arange(n).to(device) != i))[0]\n",
        "                neg_indices = torch.where(labels != anchor_label)[0]\n",
        "\n",
        "                if len(pos_indices) == 0 or len(neg_indices) == 0:\n",
        "                    continue  # Skip if no valid positives or negatives\n",
        "\n",
        "                ap_distances = anchor_dist[pos_indices]  # Distances to positives\n",
        "                an_distances = anchor_dist[neg_indices]  # Distances to negatives\n",
        "\n",
        "                # Compute losses for all combinations of positives and negatives\n",
        "                ap_expanded = ap_distances.unsqueeze(1)  # [num_pos, 1]\n",
        "                an_expanded = an_distances.unsqueeze(0)  # [1, num_neg]\n",
        "                loss_values = F.relu(ap_expanded - an_expanded + self.margin)  # [num_pos, num_neg]\n",
        "\n",
        "                total_loss += loss_values.sum()\n",
        "                total_triplets += loss_values.numel()\n",
        "\n",
        "        if total_triplets > 0:\n",
        "            triplet_loss = total_loss / total_triplets\n",
        "        else:\n",
        "            triplet_loss = torch.tensor(0.0, requires_grad=True).to(device)\n",
        "        return triplet_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVQsJei94WDz"
      },
      "source": [
        "# Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfCDgrXa5E2V"
      },
      "outputs": [],
      "source": [
        "test_dataset = DataSet(data_set=test_set)\n",
        "test_collate_fn = CollateFn(test_dataset.label_set)\n",
        "test_loader = tordata.DataLoader(test_dataset, collate_fn=test_collate_fn, num_workers=2, drop_last=True, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1v3Ko17tFOm"
      },
      "outputs": [],
      "source": [
        "def eval(probe_seq, gallery_seq, features, labels, views, seq_types):\n",
        "\n",
        "    view_list = np.unique(views)\n",
        "    acc_list = []\n",
        "\n",
        "    for probe_view in view_list:\n",
        "        # Create masks for probe and gallery samples\n",
        "        probe_mask = np.isin(seq_types, probe_seq) & (views == probe_view)\n",
        "        gallery_mask = np.isin(seq_types, gallery_seq)\n",
        "\n",
        "        probe_features = features[probe_mask]  # [num_probe_samples, feature_dim, num_parts]\n",
        "        probe_labels = labels[probe_mask]\n",
        "\n",
        "        gallery_features = features[gallery_mask]\n",
        "        gallery_labels = labels[gallery_mask]\n",
        "        gallery_views = views[gallery_mask]\n",
        "\n",
        "        # Exclude gallery samples with the same view as the probe\n",
        "        non_identical_view_mask = gallery_views != probe_view\n",
        "        gallery_features = gallery_features[non_identical_view_mask]\n",
        "        gallery_labels = gallery_labels[non_identical_view_mask]\n",
        "\n",
        "        if len(probe_features) == 0:\n",
        "            continue  # Skip if no probe samples for this view\n",
        "\n",
        "        num_parts = probe_features.shape[2]\n",
        "        distances_per_part = []\n",
        "\n",
        "        for part in range(num_parts):\n",
        "            # Extract features for the current part\n",
        "            probe_part_features = probe_features[:, :, part]  # [num_probe_samples, feature_dim]\n",
        "            gallery_part_features = gallery_features[:, :, part]  # [num_gallery_samples, feature_dim]\n",
        "\n",
        "            # Compute distances for this part\n",
        "            distances_part = cdist(probe_part_features, gallery_part_features, metric='euclidean')  # [num_probe_samples, num_gallery_samples]\n",
        "            distances_per_part.append(distances_part)\n",
        "\n",
        "        # Aggregate distances over parts (e.g., mean)\n",
        "        distances = np.mean(distances_per_part, axis=0)  # [num_probe_samples, num_gallery_samples]\n",
        "\n",
        "        # Find nearest neighbors\n",
        "        min_indices = np.argmin(distances, axis=1)  # [num_probe_samples]\n",
        "        predicted_labels = gallery_labels[min_indices]  # [num_probe_samples]\n",
        "\n",
        "        # Compute accuracy\n",
        "        correct = (predicted_labels == probe_labels)\n",
        "        acc = np.mean(np.array(correct))\n",
        "        acc_list.append(acc)\n",
        "\n",
        "    # Compute mean accuracy over all views\n",
        "    mean_acc = np.mean(acc_list) if acc_list else 0.0\n",
        "    return mean_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hLquXp0-4UEd",
        "outputId": "bb994546-a7a0-4916-d5c0-787d273e1645"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-7fe9c5d0f0b5>:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-19-7fe9c5d0f0b5>:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean loss for 10 iterations: 0.3281306281685829\n",
            "Mean loss for 20 iterations: 0.2479355439543724\n",
            "Accuracy Bag: 0.480011754211781\n",
            "Accuracy NM: 0.7003710575139147\n",
            "Accuracy CL: 0.16512059369202223\n",
            "Mean loss for 30 iterations: 0.2089890661338965\n",
            "Mean loss for 40 iterations: 0.18511469271034003\n",
            "Accuracy Bag: 0.6268005881742661\n",
            "Accuracy NM: 0.8089053803339518\n",
            "Accuracy CL: 0.23562152133580708\n",
            "Mean loss for 50 iterations: 0.1683318692445755\n",
            "Mean loss for 60 iterations: 0.1577143780887127\n",
            "Accuracy Bag: 0.7271877198239497\n",
            "Accuracy NM: 0.9044526901669759\n",
            "Accuracy CL: 0.32653061224489793\n",
            "Mean loss for 70 iterations: 0.14813393216047968\n",
            "Mean loss for 80 iterations: 0.13907563630491496\n",
            "Accuracy Bag: 0.7270844550935593\n",
            "Accuracy NM: 0.9128014842300556\n",
            "Accuracy CL: 0.3283858998144713\n",
            "Mean loss for 90 iterations: 0.13303629437254536\n",
            "Mean loss for 100 iterations: 0.12762538868933915\n",
            "Accuracy Bag: 0.7225229360722734\n",
            "Accuracy NM: 0.9248608534322821\n",
            "Accuracy CL: 0.34879406307977734\n",
            "Mean loss for 110 iterations: 0.12244712483476508\n",
            "Mean loss for 120 iterations: 0.11739151263609529\n",
            "Accuracy Bag: 0.7604805973396146\n",
            "Accuracy NM: 0.9332096474953618\n",
            "Accuracy CL: 0.42300556586270877\n",
            "Mean loss for 130 iterations: 0.11336237997389757\n",
            "Mean loss for 140 iterations: 0.10947513141270195\n",
            "Accuracy Bag: 0.7902325282161939\n",
            "Accuracy NM: 0.9452690166975883\n",
            "Accuracy CL: 0.38589981447124305\n",
            "Mean loss for 150 iterations: 0.10586603437860807\n",
            "Mean loss for 160 iterations: 0.10228893093299121\n",
            "Accuracy Bag: 0.7781448553974997\n",
            "Accuracy NM: 0.9434137291280148\n",
            "Accuracy CL: 0.43970315398886833\n",
            "Mean loss for 170 iterations: 0.09957882900010137\n",
            "Mean loss for 180 iterations: 0.09708881227092611\n",
            "Accuracy Bag: 0.7772351790962048\n",
            "Accuracy NM: 0.9294990723562154\n",
            "Accuracy CL: 0.4814471243042671\n",
            "Mean loss for 190 iterations: 0.09418182079925348\n",
            "Mean loss for 200 iterations: 0.09158428183756769\n",
            "Accuracy Bag: 0.8134999363410128\n",
            "Accuracy NM: 0.948051948051948\n",
            "Accuracy CL: 0.4860853432282003\n",
            "Mean loss for 210 iterations: 0.08951770086728392\n",
            "Mean loss for 220 iterations: 0.08739206054332581\n",
            "Accuracy Bag: 0.8032103642627139\n",
            "Accuracy NM: 0.9545454545454544\n",
            "Accuracy CL: 0.40074211502782936\n",
            "Mean loss for 230 iterations: 0.08543958787036979\n",
            "Mean loss for 240 iterations: 0.08373699697355429\n",
            "Accuracy Bag: 0.8152982302801557\n",
            "Accuracy NM: 0.961038961038961\n",
            "Accuracy CL: 0.48330241187384054\n",
            "Mean loss for 250 iterations: 0.08216815262287855\n",
            "Mean loss for 260 iterations: 0.08066553944148697\n",
            "Accuracy Bag: 0.825493328171063\n",
            "Accuracy NM: 0.9517625231910946\n",
            "Accuracy CL: 0.44990723562152124\n",
            "Mean loss for 270 iterations: 0.07904018259572762\n",
            "Mean loss for 280 iterations: 0.07754297444064702\n",
            "Accuracy Bag: 0.7976632418324747\n",
            "Accuracy NM: 0.9601113172541743\n",
            "Accuracy CL: 0.5111317254174397\n",
            "Mean loss for 290 iterations: 0.07626078685681367\n",
            "Mean loss for 300 iterations: 0.07476737773045898\n",
            "Accuracy Bag: 0.8190089986180493\n",
            "Accuracy NM: 0.9536178107606678\n",
            "Accuracy CL: 0.5426716141001855\n",
            "Mean loss for 310 iterations: 0.07349985293563335\n",
            "Mean loss for 320 iterations: 0.07233398662065156\n",
            "Accuracy Bag: 0.8328967041646116\n",
            "Accuracy NM: 0.9591836734693878\n",
            "Accuracy CL: 0.5278293135435993\n",
            "Mean loss for 330 iterations: 0.07111462937159972\n",
            "Mean loss for 340 iterations: 0.070050904072602\n",
            "Accuracy Bag: 0.8394085645391042\n",
            "Accuracy NM: 0.961038961038961\n",
            "Accuracy CL: 0.5120593692022264\n",
            "Mean loss for 350 iterations: 0.06901258117918457\n",
            "Mean loss for 360 iterations: 0.06787362379642824\n",
            "Accuracy Bag: 0.8031635135665127\n",
            "Accuracy NM: 0.9564007421150277\n",
            "Accuracy CL: 0.48515769944341364\n",
            "Mean loss for 370 iterations: 0.06679402792171853\n",
            "Mean loss for 380 iterations: 0.06591445797270065\n",
            "Accuracy Bag: 0.8320635345672165\n",
            "Accuracy NM: 0.9684601113172543\n",
            "Accuracy CL: 0.5602968460111316\n",
            "Mean loss for 390 iterations: 0.06496020617106786\n",
            "Mean loss for 400 iterations: 0.06411238086642697\n",
            "Accuracy Bag: 0.8217839122243995\n",
            "Accuracy NM: 0.9666048237476808\n",
            "Accuracy CL: 0.538961038961039\n",
            "Mean loss for 410 iterations: 0.06324794513923003\n",
            "Mean loss for 420 iterations: 0.06239590490724714\n",
            "Accuracy Bag: 0.8534182750945369\n",
            "Accuracy NM: 0.963821892393321\n",
            "Accuracy CL: 0.5807050092764379\n",
            "Mean loss for 430 iterations: 0.06156237585210177\n",
            "Mean loss for 440 iterations: 0.06083567013583061\n",
            "Accuracy Bag: 0.853323414509651\n",
            "Accuracy NM: 0.9591836734693877\n",
            "Accuracy CL: 0.5547309833024118\n",
            "Mean loss for 450 iterations: 0.06004599909608563\n",
            "Mean loss for 460 iterations: 0.05929148598655087\n",
            "Accuracy Bag: 0.8617196388651738\n",
            "Accuracy NM: 0.9666048237476809\n",
            "Accuracy CL: 0.5445269016697588\n",
            "Mean loss for 470 iterations: 0.05855414581465277\n",
            "Mean loss for 480 iterations: 0.05792117078672163\n",
            "Accuracy Bag: 0.8636410970056706\n",
            "Accuracy NM: 0.9684601113172543\n",
            "Accuracy CL: 0.5658627087198516\n",
            "Mean loss for 490 iterations: 0.057189471401958444\n",
            "Mean loss for 500 iterations: 0.05656712332367897\n",
            "Accuracy Bag: 0.8552071988944395\n",
            "Accuracy NM: 0.9666048237476809\n",
            "Accuracy CL: 0.5871985157699443\n",
            "Mean loss for 510 iterations: 0.05592556133610653\n",
            "Mean loss for 520 iterations: 0.05529226437390137\n",
            "Accuracy Bag: 0.8533712311995885\n",
            "Accuracy NM: 0.9684601113172541\n",
            "Accuracy CL: 0.5575139146567718\n",
            "Mean loss for 530 iterations: 0.05470442424862171\n",
            "Mean loss for 540 iterations: 0.05407862516499504\n",
            "Accuracy Bag: 0.8311546310609103\n",
            "Accuracy NM: 0.9666048237476809\n",
            "Accuracy CL: 0.5686456400742116\n",
            "Mean loss for 550 iterations: 0.05350893354551359\n",
            "Mean loss for 560 iterations: 0.052902429073583335\n",
            "Accuracy Bag: 0.8691604954156834\n",
            "Accuracy NM: 0.976808905380334\n",
            "Accuracy CL: 0.5769944341372913\n",
            "Mean loss for 570 iterations: 0.05232569067984035\n",
            "Mean loss for 580 iterations: 0.05178163021256955\n",
            "Accuracy Bag: 0.8515826937971803\n",
            "Accuracy NM: 0.9684601113172541\n",
            "Accuracy CL: 0.5565862708719851\n",
            "Mean loss for 590 iterations: 0.05122871794816801\n",
            "Mean loss for 600 iterations: 0.05068441745514671\n",
            "Accuracy Bag: 0.8765908226344696\n",
            "Accuracy NM: 0.9675324675324677\n",
            "Accuracy CL: 0.5640074211502782\n",
            "Mean loss for 610 iterations: 0.05016263665051245\n",
            "Mean loss for 620 iterations: 0.04969182980487183\n",
            "Accuracy Bag: 0.8180536308130364\n",
            "Accuracy NM: 0.9693877551020408\n",
            "Accuracy CL: 0.5732838589981447\n",
            "Mean loss for 630 iterations: 0.049261820805628624\n",
            "Mean loss for 640 iterations: 0.048793190452852284\n",
            "Accuracy Bag: 0.8524147042020921\n",
            "Accuracy NM: 0.9740259740259739\n",
            "Accuracy CL: 0.5983302411873841\n",
            "Mean loss for 650 iterations: 0.04831084116576956\n",
            "Mean loss for 660 iterations: 0.04786276130541933\n",
            "Accuracy Bag: 0.8645027634182809\n",
            "Accuracy NM: 0.9777365491651206\n",
            "Accuracy CL: 0.6233766233766235\n",
            "Mean loss for 670 iterations: 0.047401866308455144\n",
            "Mean loss for 680 iterations: 0.04693548827678622\n",
            "Accuracy Bag: 0.8533714243983358\n",
            "Accuracy NM: 0.9721706864564006\n",
            "Accuracy CL: 0.62152133580705\n",
            "Mean loss for 690 iterations: 0.046494674571938274\n",
            "Mean loss for 700 iterations: 0.0461080428997853\n",
            "Accuracy Bag: 0.8487230625401733\n",
            "Accuracy NM: 0.9666048237476809\n",
            "Accuracy CL: 0.6549165120593692\n",
            "Mean loss for 710 iterations: 0.04568967217133499\n",
            "Mean loss for 720 iterations: 0.04530310715005423\n",
            "Accuracy Bag: 0.8608015584183747\n",
            "Accuracy NM: 0.9786641929499073\n",
            "Accuracy CL: 0.6335807050092764\n",
            "Mean loss for 730 iterations: 0.044919069630591425\n",
            "Mean loss for 740 iterations: 0.04448579095329183\n",
            "Accuracy Bag: 0.8496797827364169\n",
            "Accuracy NM: 0.9740259740259739\n",
            "Accuracy CL: 0.572356215213358\n",
            "Mean loss for 750 iterations: 0.044102885356793804\n",
            "Mean loss for 760 iterations: 0.04370073943374384\n",
            "Accuracy Bag: 0.8524904381110031\n",
            "Accuracy NM: 0.9740259740259739\n",
            "Accuracy CL: 0.6187384044526902\n",
            "Mean loss for 770 iterations: 0.043325835258349196\n",
            "Mean loss for 780 iterations: 0.04293874681282502\n",
            "Accuracy Bag: 0.8561826593691635\n",
            "Accuracy NM: 0.9693877551020407\n",
            "Accuracy CL: 0.5667903525046383\n",
            "Mean loss for 790 iterations: 0.042584331740943504\n",
            "Mean loss for 800 iterations: 0.04224412398354616\n",
            "Accuracy Bag: 0.8988931064175407\n",
            "Accuracy NM: 0.9777365491651208\n",
            "Accuracy CL: 0.6261595547309834\n",
            "Mean loss for 810 iterations: 0.04191844463520856\n",
            "Mean loss for 820 iterations: 0.041579030404753255\n",
            "Accuracy Bag: 0.8543090179186041\n",
            "Accuracy NM: 0.9814471243042674\n",
            "Accuracy CL: 0.5862708719851576\n",
            "Mean loss for 830 iterations: 0.04121679224029004\n",
            "Mean loss for 840 iterations: 0.040894629065102585\n",
            "Accuracy Bag: 0.8775760396459286\n",
            "Accuracy NM: 0.9768089053803339\n",
            "Accuracy CL: 0.6419294990723562\n",
            "Mean loss for 850 iterations: 0.04055984909481862\n",
            "Mean loss for 860 iterations: 0.04022239246947128\n",
            "Accuracy Bag: 0.8598366272753742\n",
            "Accuracy NM: 0.976808905380334\n",
            "Accuracy CL: 0.6419294990723562\n",
            "Mean loss for 870 iterations: 0.03992209623128861\n",
            "Mean loss for 880 iterations: 0.03964847418934699\n",
            "Accuracy Bag: 0.889608071225423\n",
            "Accuracy NM: 0.9768089053803339\n",
            "Accuracy CL: 0.6447124304267162\n",
            "Mean loss for 890 iterations: 0.03937035004001404\n",
            "Mean loss for 900 iterations: 0.03908621995968537\n",
            "Accuracy Bag: 0.8719151231535754\n",
            "Accuracy NM: 0.9721706864564008\n",
            "Accuracy CL: 0.6233766233766234\n",
            "Mean loss for 910 iterations: 0.03883915873766355\n",
            "Mean loss for 920 iterations: 0.03860545056598747\n",
            "Accuracy Bag: 0.8886511578304322\n",
            "Accuracy NM: 0.9730983302411874\n",
            "Accuracy CL: 0.6187384044526902\n",
            "Mean loss for 930 iterations: 0.03834064854959887\n",
            "Mean loss for 940 iterations: 0.03805578180410443\n",
            "Accuracy Bag: 0.8672105404599715\n",
            "Accuracy NM: 0.9749536178107608\n",
            "Accuracy CL: 0.6716141001855288\n",
            "Mean loss for 950 iterations: 0.03779115104165517\n",
            "Mean loss for 960 iterations: 0.03754073082624624\n",
            "Accuracy Bag: 0.8599029910450448\n",
            "Accuracy NM: 0.9777365491651205\n",
            "Accuracy CL: 0.6289424860853432\n",
            "Mean loss for 970 iterations: 0.03725083482490142\n",
            "Mean loss for 980 iterations: 0.03698019283319994\n",
            "Accuracy Bag: 0.880377711278769\n",
            "Accuracy NM: 0.976808905380334\n",
            "Accuracy CL: 0.6539888682745826\n",
            "Mean loss for 990 iterations: 0.036724884701290664\n",
            "Mean loss for 1000 iterations: 0.03646591934701428\n",
            "Accuracy Bag: 0.8886326107506988\n",
            "Accuracy NM: 0.9777365491651206\n",
            "Accuracy CL: 0.6363636363636364\n",
            "Mean loss for 1010 iterations: 0.03622116201538777\n",
            "Mean loss for 1020 iterations: 0.035974636190004794\n",
            "Accuracy Bag: 0.8942453241556201\n",
            "Accuracy NM: 0.976808905380334\n",
            "Accuracy CL: 0.6410018552875695\n",
            "Mean loss for 1030 iterations: 0.035738658956568504\n",
            "Mean loss for 1040 iterations: 0.0355029261438176\n",
            "Accuracy Bag: 0.8710634064764471\n",
            "Accuracy NM: 0.9758812615955473\n",
            "Accuracy CL: 0.6567717996289425\n",
            "Mean loss for 1050 iterations: 0.03526921478321864\n",
            "Mean loss for 1060 iterations: 0.035047521917061564\n",
            "Accuracy Bag: 0.8691694791574295\n",
            "Accuracy NM: 0.9777365491651206\n",
            "Accuracy CL: 0.6261595547309834\n",
            "Mean loss for 1070 iterations: 0.03480576485173481\n",
            "Mean loss for 1080 iterations: 0.034570530174015505\n",
            "Accuracy Bag: 0.874736114661138\n",
            "Accuracy NM: 0.9823747680890539\n",
            "Accuracy CL: 0.6753246753246753\n",
            "Mean loss for 1090 iterations: 0.03433384012063504\n",
            "Mean loss for 1100 iterations: 0.03409530467099764\n",
            "Accuracy Bag: 0.8951349077872041\n",
            "Accuracy NM: 0.9740259740259739\n",
            "Accuracy CL: 0.62430426716141\n",
            "Mean loss for 1110 iterations: 0.033861259017391385\n",
            "Mean loss for 1120 iterations: 0.03363418703582803\n",
            "Accuracy Bag: 0.891405592369577\n",
            "Accuracy NM: 0.9814471243042672\n",
            "Accuracy CL: 0.6539888682745827\n",
            "Mean loss for 1130 iterations: 0.033404495224224784\n",
            "Mean loss for 1140 iterations: 0.03319136791560276\n",
            "Accuracy Bag: 0.8747361146611382\n",
            "Accuracy NM: 0.979591836734694\n",
            "Accuracy CL: 0.6484230055658627\n",
            "Mean loss for 1150 iterations: 0.03297520142332043\n",
            "Mean loss for 1160 iterations: 0.0327645033882963\n",
            "Accuracy Bag: 0.8710354892574734\n",
            "Accuracy NM: 0.9777365491651206\n",
            "Accuracy CL: 0.6465677179962893\n",
            "Mean loss for 1170 iterations: 0.03255746177294188\n",
            "Mean loss for 1180 iterations: 0.0323483281930656\n",
            "Accuracy Bag: 0.8839465751368281\n",
            "Accuracy NM: 0.9777365491651206\n",
            "Accuracy CL: 0.666048237476809\n",
            "Mean loss for 1190 iterations: 0.03213993802337962\n",
            "Mean loss for 1200 iterations: 0.03194520627109644\n",
            "Accuracy Bag: 0.8887364550773309\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.6688311688311689\n",
            "Mean loss for 1210 iterations: 0.0317452063318342\n",
            "Mean loss for 1220 iterations: 0.03155220371449641\n",
            "Accuracy Bag: 0.8858865803570585\n",
            "Accuracy NM: 0.9758812615955473\n",
            "Accuracy CL: 0.6576994434137291\n",
            "Mean loss for 1230 iterations: 0.031363837177887925\n",
            "Mean loss for 1240 iterations: 0.031181402505946254\n",
            "Accuracy Bag: 0.8859158499672625\n",
            "Accuracy NM: 0.9786641929499074\n",
            "Accuracy CL: 0.6799628942486085\n",
            "Mean loss for 1250 iterations: 0.03099522546492517\n",
            "Mean loss for 1260 iterations: 0.030790069732364148\n",
            "Accuracy Bag: 0.8793930738635517\n",
            "Accuracy NM: 0.9833024118738405\n",
            "Accuracy CL: 0.6428571428571428\n",
            "Mean loss for 1270 iterations: 0.030609552688918656\n",
            "Mean loss for 1280 iterations: 0.03041778009301197\n",
            "Accuracy Bag: 0.8905162019367403\n",
            "Accuracy NM: 0.9805194805194805\n",
            "Accuracy CL: 0.6335807050092764\n",
            "Mean loss for 1290 iterations: 0.030237780336666013\n",
            "Mean loss for 1300 iterations: 0.03006876283307345\n",
            "Accuracy Bag: 0.8802646900116442\n",
            "Accuracy NM: 0.9851576994434136\n",
            "Accuracy CL: 0.6855287569573284\n",
            "Mean loss for 1310 iterations: 0.029895167310459127\n",
            "Mean loss for 1320 iterations: 0.029715641063045372\n",
            "Accuracy Bag: 0.8961394446733849\n",
            "Accuracy NM: 0.9786641929499073\n",
            "Accuracy CL: 0.62708719851577\n",
            "Mean loss for 1330 iterations: 0.029576010624282903\n",
            "Mean loss for 1340 iterations: 0.02943447402264895\n",
            "Accuracy Bag: 0.8840031823697644\n",
            "Accuracy NM: 0.9814471243042672\n",
            "Accuracy CL: 0.608534322820037\n",
            "Mean loss for 1350 iterations: 0.029269002793115322\n",
            "Mean loss for 1360 iterations: 0.02911556715381962\n",
            "Accuracy Bag: 0.8802832370913773\n",
            "Accuracy NM: 0.9888682745825604\n",
            "Accuracy CL: 0.6558441558441558\n",
            "Mean loss for 1370 iterations: 0.028940614551836014\n",
            "Mean loss for 1380 iterations: 0.028775093272713055\n",
            "Accuracy Bag: 0.8969618917403093\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.6539888682745826\n",
            "Mean loss for 1390 iterations: 0.02861067171362664\n",
            "Mean loss for 1400 iterations: 0.028481914805847088\n",
            "Accuracy Bag: 0.8895885581519534\n",
            "Accuracy NM: 0.9786641929499073\n",
            "Accuracy CL: 0.6697588126159554\n",
            "Mean loss for 1410 iterations: 0.028342637195952026\n",
            "Mean loss for 1420 iterations: 0.028196304812404312\n",
            "Accuracy Bag: 0.8886136772734712\n",
            "Accuracy NM: 0.9814471243042672\n",
            "Accuracy CL: 0.6688311688311688\n",
            "Mean loss for 1430 iterations: 0.028043296661755476\n",
            "Mean loss for 1440 iterations: 0.027903319524678713\n",
            "Accuracy Bag: 0.8951821448808999\n",
            "Accuracy NM: 0.9749536178107605\n",
            "Accuracy CL: 0.6484230055658627\n",
            "Mean loss for 1450 iterations: 0.027756086439536563\n",
            "Mean loss for 1460 iterations: 0.027618997272627097\n",
            "Accuracy Bag: 0.8774431189078397\n",
            "Accuracy NM: 0.9795918367346939\n",
            "Accuracy CL: 0.6669758812615956\n",
            "Mean loss for 1470 iterations: 0.027498346645714475\n",
            "Mean loss for 1480 iterations: 0.02735825158652497\n",
            "Accuracy Bag: 0.8905353286127152\n",
            "Accuracy NM: 0.9833024118738404\n",
            "Accuracy CL: 0.6725417439703154\n",
            "Mean loss for 1490 iterations: 0.02721557459362701\n",
            "Mean loss for 1500 iterations: 0.027076745125154653\n",
            "Accuracy Bag: 0.8895885581519536\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.6855287569573284\n",
            "Mean loss for 1510 iterations: 0.026942831175463484\n",
            "Mean loss for 1520 iterations: 0.026813754691178665\n",
            "Accuracy Bag: 0.8988934928150353\n",
            "Accuracy NM: 0.9823747680890537\n",
            "Accuracy CL: 0.6743970315398887\n",
            "Mean loss for 1530 iterations: 0.026681435734240547\n",
            "Mean loss for 1540 iterations: 0.0265523553596108\n",
            "Accuracy Bag: 0.8746787346332131\n",
            "Accuracy NM: 0.9888682745825604\n",
            "Accuracy CL: 0.6465677179962894\n",
            "Mean loss for 1550 iterations: 0.026419989316453858\n",
            "Mean loss for 1560 iterations: 0.026297929951658423\n",
            "Accuracy Bag: 0.8728903904295522\n",
            "Accuracy NM: 0.9805194805194806\n",
            "Accuracy CL: 0.6771799628942486\n",
            "Mean loss for 1570 iterations: 0.02618927132682341\n",
            "Mean loss for 1580 iterations: 0.02606775276598674\n",
            "Accuracy Bag: 0.8728527166738439\n",
            "Accuracy NM: 0.9879406307977738\n",
            "Accuracy CL: 0.6706864564007421\n",
            "Mean loss for 1590 iterations: 0.02596243226859695\n",
            "Mean loss for 1600 iterations: 0.025858249611628706\n",
            "Accuracy Bag: 0.8997547728301512\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.6641929499072355\n",
            "Mean loss for 1610 iterations: 0.025752736105705085\n",
            "Mean loss for 1620 iterations: 0.02564169078430069\n",
            "Accuracy Bag: 0.871015976184004\n",
            "Accuracy NM: 0.9805194805194806\n",
            "Accuracy CL: 0.6818181818181818\n",
            "Mean loss for 1630 iterations: 0.02554397122474513\n",
            "Mean loss for 1640 iterations: 0.025429011385024684\n",
            "Accuracy Bag: 0.872805286381401\n",
            "Accuracy NM: 0.9833024118738404\n",
            "Accuracy CL: 0.6465677179962895\n",
            "Mean loss for 1650 iterations: 0.02531106691906285\n",
            "Mean loss for 1660 iterations: 0.025194354489002466\n",
            "Accuracy Bag: 0.884032065582474\n",
            "Accuracy NM: 0.9795918367346936\n",
            "Accuracy CL: 0.686456400742115\n",
            "Mean loss for 1670 iterations: 0.025075371555208847\n",
            "Mean loss for 1680 iterations: 0.024962750739013287\n",
            "Accuracy Bag: 0.8923902297847939\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.7040816326530611\n",
            "Mean loss for 1690 iterations: 0.024843491650511616\n",
            "Mean loss for 1700 iterations: 0.024732098887427983\n",
            "Accuracy Bag: 0.8775852165864215\n",
            "Accuracy NM: 0.976808905380334\n",
            "Accuracy CL: 0.7003710575139147\n",
            "Mean loss for 1710 iterations: 0.024624682012918182\n",
            "Mean loss for 1720 iterations: 0.024512785001587514\n",
            "Accuracy Bag: 0.8988745593378075\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.7050092764378478\n",
            "Mean loss for 1730 iterations: 0.024391887873691113\n",
            "Mean loss for 1740 iterations: 0.024280690380182095\n",
            "Accuracy Bag: 0.9016861807061299\n",
            "Accuracy NM: 0.9833024118738405\n",
            "Accuracy CL: 0.6641929499072355\n",
            "Mean loss for 1750 iterations: 0.0241703824860576\n",
            "Mean loss for 1760 iterations: 0.024067364471094067\n",
            "Accuracy Bag: 0.8905819861101694\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6781076066790352\n",
            "Mean loss for 1770 iterations: 0.0239616955888305\n",
            "Mean loss for 1780 iterations: 0.023858156181830036\n",
            "Accuracy Bag: 0.9026801882605873\n",
            "Accuracy NM: 0.9805194805194806\n",
            "Accuracy CL: 0.6873840445269018\n",
            "Mean loss for 1790 iterations: 0.023753177679427404\n",
            "Mean loss for 1800 iterations: 0.023644258421263657\n",
            "Accuracy Bag: 0.9062773557351144\n",
            "Accuracy NM: 0.9823747680890538\n",
            "Accuracy CL: 0.6966604823747681\n",
            "Mean loss for 1810 iterations: 0.02354199476945291\n",
            "Mean loss for 1820 iterations: 0.023438910547973958\n",
            "Accuracy Bag: 0.8812868079838222\n",
            "Accuracy NM: 0.9795918367346939\n",
            "Accuracy CL: 0.6985157699443414\n",
            "Mean loss for 1830 iterations: 0.023336132582152374\n",
            "Mean loss for 1840 iterations: 0.02323170200100361\n",
            "Accuracy Bag: 0.907242286878115\n",
            "Accuracy NM: 0.9814471243042672\n",
            "Accuracy CL: 0.699443413729128\n",
            "Mean loss for 1850 iterations: 0.023127323194356585\n",
            "Mean loss for 1860 iterations: 0.023027042197238813\n",
            "Accuracy Bag: 0.8877613810001012\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.7244897959183674\n",
            "Mean loss for 1870 iterations: 0.022927581006661056\n",
            "Mean loss for 1880 iterations: 0.022831804820019355\n",
            "Accuracy Bag: 0.8989313597694907\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.6484230055658627\n",
            "Mean loss for 1890 iterations: 0.022737579593784793\n",
            "Mean loss for 1900 iterations: 0.02263911455373378\n",
            "Accuracy Bag: 0.9007397966428626\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.6512059369202227\n",
            "Mean loss for 1910 iterations: 0.022538860708643336\n",
            "Mean loss for 1920 iterations: 0.022440551830974678\n",
            "Accuracy Bag: 0.8859343970469958\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.699443413729128\n",
            "Mean loss for 1930 iterations: 0.022342920068885476\n",
            "Mean loss for 1940 iterations: 0.022249368019702065\n",
            "Accuracy Bag: 0.8886896043811294\n",
            "Accuracy NM: 0.984230055658627\n",
            "Accuracy CL: 0.6781076066790352\n",
            "Mean loss for 1950 iterations: 0.0221543701696926\n",
            "Mean loss for 1960 iterations: 0.0220626210938778\n",
            "Accuracy Bag: 0.8951165539062178\n",
            "Accuracy NM: 0.9833024118738405\n",
            "Accuracy CL: 0.683673469387755\n",
            "Mean loss for 1970 iterations: 0.02196719780786502\n",
            "Mean loss for 1980 iterations: 0.021878086836453564\n",
            "Accuracy Bag: 0.8858770170190708\n",
            "Accuracy NM: 0.9805194805194806\n",
            "Accuracy CL: 0.6846011131725418\n",
            "Mean loss for 1990 iterations: 0.02178986807431015\n",
            "Mean loss for 2000 iterations: 0.0216967134239967\n",
            "Accuracy Bag: 0.8924374668784897\n",
            "Accuracy NM: 0.9833024118738405\n",
            "Accuracy CL: 0.689239332096475\n",
            "Mean loss for 2010 iterations: 0.02160941090577268\n",
            "Mean loss for 2020 iterations: 0.02152708348302899\n",
            "Accuracy Bag: 0.8933086966290876\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.7096474953617812\n",
            "Mean loss for 2030 iterations: 0.021441699481532527\n",
            "Mean loss for 2040 iterations: 0.021353761422819934\n",
            "Accuracy Bag: 0.9063720231212532\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.6771799628942486\n",
            "Mean loss for 2050 iterations: 0.02128041689585122\n",
            "Mean loss for 2060 iterations: 0.02120186304091476\n",
            "Accuracy Bag: 0.9072141764603944\n",
            "Accuracy NM: 0.9795918367346939\n",
            "Accuracy CL: 0.6966604823747681\n",
            "Mean loss for 2070 iterations: 0.02111948490213684\n",
            "Mean loss for 2080 iterations: 0.021043479894993094\n",
            "Accuracy Bag: 0.9211485395044104\n",
            "Accuracy NM: 0.9833024118738405\n",
            "Accuracy CL: 0.6530612244897959\n",
            "Mean loss for 2090 iterations: 0.020964514995472836\n",
            "Mean loss for 2100 iterations: 0.020884554589533113\n",
            "Accuracy Bag: 0.908198427478117\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.6948051948051949\n",
            "Mean loss for 2110 iterations: 0.02080460672936583\n",
            "Mean loss for 2120 iterations: 0.020728712359098456\n",
            "Accuracy Bag: 0.8998209434010747\n",
            "Accuracy NM: 0.9814471243042672\n",
            "Accuracy CL: 0.6641929499072355\n",
            "Mean loss for 2130 iterations: 0.02064972806939453\n",
            "Mean loss for 2140 iterations: 0.020569541495756863\n",
            "Accuracy Bag: 0.8895791880127132\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.7077922077922079\n",
            "Mean loss for 2150 iterations: 0.02049190784024811\n",
            "Mean loss for 2160 iterations: 0.020418338494858256\n",
            "Accuracy Bag: 0.9072614135540902\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.6966604823747682\n",
            "Mean loss for 2170 iterations: 0.020336989204781258\n",
            "Mean loss for 2180 iterations: 0.02025817975769281\n",
            "Accuracy Bag: 0.9035602085541836\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6901669758812616\n",
            "Mean loss for 2190 iterations: 0.02017945966951243\n",
            "Mean loss for 2200 iterations: 0.020103595271198586\n",
            "Accuracy Bag: 0.9072426732756095\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.6929499072356216\n",
            "Mean loss for 2210 iterations: 0.020027872586329952\n",
            "Mean loss for 2220 iterations: 0.019953939499782383\n",
            "Accuracy Bag: 0.8988363059858575\n",
            "Accuracy NM: 0.9860853432282005\n",
            "Accuracy CL: 0.7059369202226345\n",
            "Mean loss for 2230 iterations: 0.019884027015666307\n",
            "Mean loss for 2240 iterations: 0.019809836649177927\n",
            "Accuracy Bag: 0.89890324935177\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.7179962894248608\n",
            "Mean loss for 2250 iterations: 0.019738984589258\n",
            "Mean loss for 2260 iterations: 0.019664828600058026\n",
            "Accuracy Bag: 0.894236147215127\n",
            "Accuracy NM: 0.9814471243042672\n",
            "Accuracy CL: 0.6873840445269017\n",
            "Mean loss for 2270 iterations: 0.019591941759333893\n",
            "Mean loss for 2280 iterations: 0.01952092031534177\n",
            "Accuracy Bag: 0.9006831894099266\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.7059369202226345\n",
            "Mean loss for 2290 iterations: 0.01945221387289615\n",
            "Mean loss for 2300 iterations: 0.01938186841816945\n",
            "Accuracy Bag: 0.9100724553261706\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6799628942486087\n",
            "Mean loss for 2310 iterations: 0.01931384056652856\n",
            "Mean loss for 2320 iterations: 0.019244702634123628\n",
            "Accuracy Bag: 0.8989311665707436\n",
            "Accuracy NM: 0.9860853432282005\n",
            "Accuracy CL: 0.7031539888682746\n",
            "Mean loss for 2330 iterations: 0.019179901783029817\n",
            "Mean loss for 2340 iterations: 0.019112490508959103\n",
            "Accuracy Bag: 0.8830950516584471\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6437847866419295\n",
            "Mean loss for 2350 iterations: 0.019044780545209118\n",
            "Mean loss for 2360 iterations: 0.018979609808544195\n",
            "Accuracy Bag: 0.9091356346008911\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.6873840445269017\n",
            "Mean loss for 2370 iterations: 0.018913740149619575\n",
            "Mean loss for 2380 iterations: 0.01884787433872199\n",
            "Accuracy Bag: 0.910895868386831\n",
            "Accuracy NM: 0.9879406307977738\n",
            "Accuracy CL: 0.7124304267161411\n",
            "Mean loss for 2390 iterations: 0.01878226853049337\n",
            "Mean loss for 2400 iterations: 0.018715309926313543\n",
            "Accuracy Bag: 0.8914623928012602\n",
            "Accuracy NM: 0.9897959183673468\n",
            "Accuracy CL: 0.6985157699443415\n",
            "Mean loss for 2410 iterations: 0.01864785862537352\n",
            "Mean loss for 2420 iterations: 0.01858493546189929\n",
            "Accuracy Bag: 0.8980226494619319\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7003710575139146\n",
            "Mean loss for 2430 iterations: 0.018518487353681744\n",
            "Mean loss for 2440 iterations: 0.018455654987087282\n",
            "Accuracy Bag: 0.8822331920470895\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7105751391465678\n",
            "Mean loss for 2450 iterations: 0.018394034035499114\n",
            "Mean loss for 2460 iterations: 0.01833087635021638\n",
            "Accuracy Bag: 0.9016006902604841\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.705009276437848\n",
            "Mean loss for 2470 iterations: 0.018269184127384947\n",
            "Mean loss for 2480 iterations: 0.018206575261255267\n",
            "Accuracy Bag: 0.9044314383047816\n",
            "Accuracy NM: 0.9916512059369204\n",
            "Accuracy CL: 0.7207792207792209\n",
            "Mean loss for 2490 iterations: 0.01814227198841084\n",
            "Mean loss for 2500 iterations: 0.018079820574331096\n",
            "Accuracy Bag: 0.8960531814327504\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.6586270871985158\n",
            "Mean loss for 2510 iterations: 0.018017279485957622\n",
            "Mean loss for 2520 iterations: 0.017954405451526993\n",
            "Accuracy Bag: 0.8794024440027919\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.7003710575139146\n",
            "Mean loss for 2530 iterations: 0.017891735916615834\n",
            "Mean loss for 2540 iterations: 0.017830102902646434\n",
            "Accuracy Bag: 0.9119382722274676\n",
            "Accuracy NM: 0.9805194805194806\n",
            "Accuracy CL: 0.7003710575139147\n",
            "Mean loss for 2550 iterations: 0.017769651119665737\n",
            "Mean loss for 2560 iterations: 0.017710824084201705\n",
            "Accuracy Bag: 0.9248396015700876\n",
            "Accuracy NM: 0.9805194805194805\n",
            "Accuracy CL: 0.6938775510204082\n",
            "Mean loss for 2570 iterations: 0.017650058998881227\n",
            "Mean loss for 2580 iterations: 0.01758906385261663\n",
            "Accuracy Bag: 0.8905632458316887\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.7040816326530613\n",
            "Mean loss for 2590 iterations: 0.017531964904965804\n",
            "Mean loss for 2600 iterations: 0.0174739460895832\n",
            "Accuracy Bag: 0.8988653823973144\n",
            "Accuracy NM: 0.9833024118738404\n",
            "Accuracy CL: 0.702226345083488\n",
            "Mean loss for 2610 iterations: 0.01741422029656815\n",
            "Mean loss for 2620 iterations: 0.01735651276835493\n",
            "Accuracy Bag: 0.889607298430434\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.7115027829313545\n",
            "Mean loss for 2630 iterations: 0.01730043335772162\n",
            "Mean loss for 2640 iterations: 0.017243572478257578\n",
            "Accuracy Bag: 0.8951448575226859\n",
            "Accuracy NM: 0.9879406307977738\n",
            "Accuracy CL: 0.7003710575139146\n",
            "Mean loss for 2650 iterations: 0.01718914203484156\n",
            "Mean loss for 2660 iterations: 0.017134810935445936\n",
            "Accuracy Bag: 0.8961294949379031\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6966604823747681\n",
            "Mean loss for 2670 iterations: 0.017081480747379175\n",
            "Mean loss for 2680 iterations: 0.01703049876336651\n",
            "Accuracy Bag: 0.9183929457727824\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.7059369202226345\n",
            "Mean loss for 2690 iterations: 0.01697834102355972\n",
            "Mean loss for 2700 iterations: 0.01692769545809521\n",
            "Accuracy Bag: 0.8961197384011683\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.692022263450835\n",
            "Mean loss for 2710 iterations: 0.01687969113697944\n",
            "Mean loss for 2720 iterations: 0.01682934031591953\n",
            "Accuracy Bag: 0.8941600269087217\n",
            "Accuracy NM: 0.9814471243042672\n",
            "Accuracy CL: 0.7068645640074211\n",
            "Mean loss for 2730 iterations: 0.01677607399852428\n",
            "Mean loss for 2740 iterations: 0.01672184134537803\n",
            "Accuracy Bag: 0.9063242064313158\n",
            "Accuracy NM: 0.9860853432282005\n",
            "Accuracy CL: 0.7319109461966604\n",
            "Mean loss for 2750 iterations: 0.016672073657539758\n",
            "Mean loss for 2760 iterations: 0.016622225260589797\n",
            "Accuracy Bag: 0.892371103108819\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.7198515769944341\n",
            "Mean loss for 2770 iterations: 0.016571022870543576\n",
            "Mean loss for 2780 iterations: 0.016522094952076568\n",
            "Accuracy Bag: 0.9156107872134114\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.7152133580705009\n",
            "Mean loss for 2790 iterations: 0.016472322146673636\n",
            "Mean loss for 2800 iterations: 0.016423484144865403\n",
            "Accuracy Bag: 0.8951827244771415\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.6985157699443415\n",
            "Mean loss for 2810 iterations: 0.01637612518856925\n",
            "Mean loss for 2820 iterations: 0.01632794155258312\n",
            "Accuracy Bag: 0.9035032149237533\n",
            "Accuracy NM: 0.9833024118738404\n",
            "Accuracy CL: 0.7448979591836733\n",
            "Mean loss for 2830 iterations: 0.016279891688913903\n",
            "Mean loss for 2840 iterations: 0.016231393027402178\n",
            "Accuracy Bag: 0.9137078761526478\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.6910946196660482\n",
            "Mean loss for 2850 iterations: 0.01618544800539926\n",
            "Mean loss for 2860 iterations: 0.016140113417828502\n",
            "Accuracy Bag: 0.8877617673975955\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.7328385899814471\n",
            "Mean loss for 2870 iterations: 0.01609281257964356\n",
            "Mean loss for 2880 iterations: 0.016043957944849453\n",
            "Accuracy Bag: 0.8961577985543712\n",
            "Accuracy NM: 0.9823747680890538\n",
            "Accuracy CL: 0.7040816326530612\n",
            "Mean loss for 2890 iterations: 0.01599762733583822\n",
            "Mean loss for 2900 iterations: 0.015950983217776882\n",
            "Accuracy Bag: 0.9063048865565936\n",
            "Accuracy NM: 0.9925788497217067\n",
            "Accuracy CL: 0.7319109461966605\n",
            "Mean loss for 2910 iterations: 0.015904717452810514\n",
            "Mean loss for 2920 iterations: 0.015856781447615653\n",
            "Accuracy Bag: 0.8978990988630834\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.7161410018552876\n",
            "Mean loss for 2930 iterations: 0.01580994379581127\n",
            "Mean loss for 2940 iterations: 0.015762042880516032\n",
            "Accuracy Bag: 0.8922859990606677\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7170686456400742\n",
            "Mean loss for 2950 iterations: 0.01571786030616919\n",
            "Mean loss for 2960 iterations: 0.015673056543193918\n",
            "Accuracy Bag: 0.9109243652020463\n",
            "Accuracy NM: 0.9823747680890538\n",
            "Accuracy CL: 0.6743970315398887\n",
            "Mean loss for 2970 iterations: 0.01562727612676687\n",
            "Mean loss for 2980 iterations: 0.01558450848174136\n",
            "Accuracy Bag: 0.908170317060396\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.6938775510204082\n",
            "Mean loss for 2990 iterations: 0.015540235551153185\n",
            "Mean loss for 3000 iterations: 0.015497491944493959\n",
            "Accuracy Bag: 0.9016572974934202\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.7393320964749536\n",
            "Mean loss for 3010 iterations: 0.015453186510948765\n",
            "Mean loss for 3020 iterations: 0.015410062254686756\n",
            "Accuracy Bag: 0.8961008049239406\n",
            "Accuracy NM: 0.9860853432282005\n",
            "Accuracy CL: 0.7300556586270872\n",
            "Mean loss for 3030 iterations: 0.015367584063611208\n",
            "Mean loss for 3040 iterations: 0.015323804681716674\n",
            "Accuracy Bag: 0.913764483385584\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.6948051948051948\n",
            "Mean loss for 3050 iterations: 0.015279855967573745\n",
            "Mean loss for 3060 iterations: 0.015237754965883914\n",
            "Accuracy Bag: 0.9109528620172616\n",
            "Accuracy NM: 0.9860853432282005\n",
            "Accuracy CL: 0.7161410018552875\n",
            "Mean loss for 3070 iterations: 0.0151968627829383\n",
            "Mean loss for 3080 iterations: 0.015155620073781321\n",
            "Accuracy Bag: 0.9156203505513989\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.7170686456400742\n",
            "Mean loss for 3090 iterations: 0.015114466009654397\n",
            "Mean loss for 3100 iterations: 0.01507259653245602\n",
            "Accuracy Bag: 0.9062580358603923\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.6846011131725418\n",
            "Mean loss for 3110 iterations: 0.015030232725552302\n",
            "Mean loss for 3120 iterations: 0.014987668147165468\n",
            "Accuracy Bag: 0.9101204652148553\n",
            "Accuracy NM: 0.9870129870129869\n",
            "Accuracy CL: 0.7152133580705009\n",
            "Mean loss for 3130 iterations: 0.014946715052701912\n",
            "Mean loss for 3140 iterations: 0.014904649084173277\n",
            "Accuracy Bag: 0.9072241261958761\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.7374768089053803\n",
            "Mean loss for 3150 iterations: 0.014862863530658023\n",
            "Mean loss for 3160 iterations: 0.014821401396532369\n",
            "Accuracy Bag: 0.9109442646730102\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.7170686456400742\n",
            "Mean loss for 3170 iterations: 0.014778932882243348\n",
            "Mean loss for 3180 iterations: 0.014738148129327331\n",
            "Accuracy Bag: 0.9193117990145705\n",
            "Accuracy NM: 0.9860853432282005\n",
            "Accuracy CL: 0.7179962894248608\n",
            "Mean loss for 3190 iterations: 0.01469636794531296\n",
            "Mean loss for 3200 iterations: 0.014654267045498272\n",
            "Accuracy Bag: 0.9007577641263542\n",
            "Accuracy NM: 0.984230055658627\n",
            "Accuracy CL: 0.6957328385899815\n",
            "Mean loss for 3210 iterations: 0.014614291434996613\n",
            "Mean loss for 3220 iterations: 0.014575128861685263\n",
            "Accuracy Bag: 0.9230128108157296\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.6771799628942486\n",
            "Mean loss for 3230 iterations: 0.014537059778469072\n",
            "Mean loss for 3240 iterations: 0.014498310432746723\n",
            "Accuracy Bag: 0.895220784630344\n",
            "Accuracy NM: 0.9860853432282005\n",
            "Accuracy CL: 0.6651205936920223\n",
            "Mean loss for 3250 iterations: 0.014458938407523629\n",
            "Mean loss for 3260 iterations: 0.014421403527563721\n",
            "Accuracy Bag: 0.9025652350059903\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.6929499072356214\n",
            "Mean loss for 3270 iterations: 0.01438437475968296\n",
            "Mean loss for 3280 iterations: 0.01434591533223106\n",
            "Accuracy Bag: 0.9072805402300649\n",
            "Accuracy NM: 0.9888682745825604\n",
            "Accuracy CL: 0.6975881261595548\n",
            "Mean loss for 3290 iterations: 0.014309058817254597\n",
            "Mean loss for 3300 iterations: 0.014272336057576026\n",
            "Accuracy Bag: 0.8858305527203639\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.6901669758812616\n",
            "Mean loss for 3310 iterations: 0.014236260411528985\n",
            "Mean loss for 3320 iterations: 0.014199947600604626\n",
            "Accuracy Bag: 0.9072612203553428\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7198515769944341\n",
            "Mean loss for 3330 iterations: 0.01416362929289885\n",
            "Mean loss for 3340 iterations: 0.014128563163129677\n",
            "Accuracy Bag: 0.9322896350610904\n",
            "Accuracy NM: 0.9916512059369204\n",
            "Accuracy CL: 0.7328385899814471\n",
            "Mean loss for 3350 iterations: 0.01409336010055832\n",
            "Mean loss for 3360 iterations: 0.014059705507705595\n",
            "Accuracy Bag: 0.8970569455239427\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.7161410018552875\n",
            "Mean loss for 3370 iterations: 0.014027246350423538\n",
            "Mean loss for 3380 iterations: 0.013995710077904613\n",
            "Accuracy Bag: 0.9100257978287165\n",
            "Accuracy NM: 0.9833024118738404\n",
            "Accuracy CL: 0.6808905380333953\n",
            "Mean loss for 3390 iterations: 0.01396242311779117\n",
            "Mean loss for 3400 iterations: 0.013930738342143527\n",
            "Accuracy Bag: 0.9081701238616489\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.7031539888682745\n",
            "Mean loss for 3410 iterations: 0.013895825357468255\n",
            "Mean loss for 3420 iterations: 0.013861814892143814\n",
            "Accuracy Bag: 0.9081888641401293\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.7031539888682746\n",
            "Mean loss for 3430 iterations: 0.013826683733119146\n",
            "Mean loss for 3440 iterations: 0.013792035120634462\n",
            "Accuracy Bag: 0.9137551132463436\n",
            "Accuracy NM: 0.9823747680890539\n",
            "Accuracy CL: 0.7031539888682745\n",
            "Mean loss for 3450 iterations: 0.013756881660192975\n",
            "Mean loss for 3460 iterations: 0.013722569105153798\n",
            "Accuracy Bag: 0.9156293342931449\n",
            "Accuracy NM: 0.984230055658627\n",
            "Accuracy CL: 0.7365491651205937\n",
            "Mean loss for 3470 iterations: 0.013687712656877873\n",
            "Mean loss for 3480 iterations: 0.013653636867277144\n",
            "Accuracy Bag: 0.9127702826323794\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.6846011131725418\n",
            "Mean loss for 3490 iterations: 0.013620795688602238\n",
            "Mean loss for 3500 iterations: 0.013587321266257536\n",
            "Accuracy Bag: 0.9026608683858651\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6706864564007421\n",
            "Mean loss for 3510 iterations: 0.013554107378765296\n",
            "Mean loss for 3520 iterations: 0.013521814614821118\n",
            "Accuracy Bag: 0.8979469155530208\n",
            "Accuracy NM: 0.9795918367346939\n",
            "Accuracy CL: 0.693877551020408\n",
            "Mean loss for 3530 iterations: 0.013488330054431613\n",
            "Mean loss for 3540 iterations: 0.013455347952294482\n",
            "Accuracy Bag: 0.9016582634871562\n",
            "Accuracy NM: 0.9879406307977738\n",
            "Accuracy CL: 0.7152133580705008\n",
            "Mean loss for 3550 iterations: 0.01342217312595302\n",
            "Mean loss for 3560 iterations: 0.013389277528669604\n",
            "Accuracy Bag: 0.9007872269353056\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.6855287569573284\n",
            "Mean loss for 3570 iterations: 0.013359309622702575\n",
            "Mean loss for 3580 iterations: 0.013327619528852197\n",
            "Accuracy Bag: 0.8793938466585408\n",
            "Accuracy NM: 0.9879406307977735\n",
            "Accuracy CL: 0.702226345083488\n",
            "Mean loss for 3590 iterations: 0.01329710781601002\n",
            "Mean loss for 3600 iterations: 0.013267297386313052\n",
            "Accuracy Bag: 0.9007397966428626\n",
            "Accuracy NM: 0.9860853432282005\n",
            "Accuracy CL: 0.7309833024118738\n",
            "Mean loss for 3610 iterations: 0.01323690250088813\n",
            "Mean loss for 3620 iterations: 0.013207306683239417\n",
            "Accuracy Bag: 0.9007681002593309\n",
            "Accuracy NM: 0.984230055658627\n",
            "Accuracy CL: 0.7263450834879407\n",
            "Mean loss for 3630 iterations: 0.013177483514541075\n",
            "Mean loss for 3640 iterations: 0.01314708120294349\n",
            "Accuracy Bag: 0.8952494746443065\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.7300556586270872\n",
            "Mean loss for 3650 iterations: 0.013115645787026415\n",
            "Mean loss for 3660 iterations: 0.013083355381578789\n",
            "Accuracy Bag: 0.9220949235676776\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.7504638218923934\n",
            "Mean loss for 3670 iterations: 0.013051259707693106\n",
            "Mean loss for 3680 iterations: 0.01302003105226069\n",
            "Accuracy Bag: 0.9035131646592351\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7170686456400741\n",
            "Mean loss for 3690 iterations: 0.012989779216897368\n",
            "Mean loss for 3700 iterations: 0.012958195687899354\n",
            "Accuracy Bag: 0.8895598681379911\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7226345083487941\n",
            "Mean loss for 3710 iterations: 0.012926364544255978\n",
            "Mean loss for 3720 iterations: 0.01289628804636298\n",
            "Accuracy Bag: 0.8868149969368339\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.7105751391465677\n",
            "Mean loss for 3730 iterations: 0.012866072025073283\n",
            "Mean loss for 3740 iterations: 0.012836176563906582\n",
            "Accuracy Bag: 0.911861958722315\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.7105751391465677\n",
            "Mean loss for 3750 iterations: 0.01280602091428979\n",
            "Mean loss for 3760 iterations: 0.012776706874826639\n",
            "Accuracy Bag: 0.8970284487087272\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6846011131725418\n",
            "Mean loss for 3770 iterations: 0.012749182228229024\n",
            "Mean loss for 3780 iterations: 0.012719580164450534\n",
            "Accuracy Bag: 0.8868151901355812\n",
            "Accuracy NM: 0.9814471243042672\n",
            "Accuracy CL: 0.7012987012987012\n",
            "Mean loss for 3790 iterations: 0.012692076451073543\n",
            "Mean loss for 3800 iterations: 0.012663820637887846\n",
            "Accuracy Bag: 0.8979188051353\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.6641929499072355\n",
            "Mean loss for 3810 iterations: 0.01263598001036051\n",
            "Mean loss for 3820 iterations: 0.012607090728680777\n",
            "Accuracy Bag: 0.8997834628441136\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.7179962894248608\n",
            "Mean loss for 3830 iterations: 0.012578947562792741\n",
            "Mean loss for 3840 iterations: 0.012551635918293868\n",
            "Accuracy Bag: 0.893318066768328\n",
            "Accuracy NM: 0.9879406307977738\n",
            "Accuracy CL: 0.6576994434137292\n",
            "Mean loss for 3850 iterations: 0.012525381778507469\n",
            "Mean loss for 3860 iterations: 0.012499004445128309\n",
            "Accuracy Bag: 0.9072711700908247\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.7012987012987013\n",
            "Mean loss for 3870 iterations: 0.012471619201861671\n",
            "Mean loss for 3880 iterations: 0.012442772565825096\n",
            "Accuracy Bag: 0.894236147215127\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.7124304267161411\n",
            "Mean loss for 3890 iterations: 0.012413977413509487\n",
            "Mean loss for 3900 iterations: 0.012387227710927479\n",
            "Accuracy Bag: 0.8942174069366463\n",
            "Accuracy NM: 0.9860853432282005\n",
            "Accuracy CL: 0.7096474953617812\n",
            "Mean loss for 3910 iterations: 0.012361251105047653\n",
            "Mean loss for 3920 iterations: 0.012335214799027105\n",
            "Accuracy Bag: 0.9053875789047833\n",
            "Accuracy NM: 0.9833024118738407\n",
            "Accuracy CL: 0.6975881261595547\n",
            "Mean loss for 3930 iterations: 0.012308503115124901\n",
            "Mean loss for 3940 iterations: 0.012281967177003182\n",
            "Accuracy Bag: 0.9007300401061279\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7319109461966604\n",
            "Mean loss for 3950 iterations: 0.012256393301215234\n",
            "Mean loss for 3960 iterations: 0.012230424278602709\n",
            "Accuracy Bag: 0.9119472559692134\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7068645640074211\n",
            "Mean loss for 3970 iterations: 0.012204086332026545\n",
            "Mean loss for 3980 iterations: 0.012177059888320093\n",
            "Accuracy Bag: 0.9072327235401277\n",
            "Accuracy NM: 0.9870129870129869\n",
            "Accuracy CL: 0.6614100185528757\n",
            "Mean loss for 3990 iterations: 0.012151239157264397\n",
            "Mean loss for 4000 iterations: 0.012124980864155078\n",
            "Accuracy Bag: 0.8961103682619279\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7393320964749536\n",
            "Mean loss for 4010 iterations: 0.012098262631031713\n",
            "Mean loss for 4020 iterations: 0.012071448975367632\n",
            "Accuracy Bag: 0.9072992805085455\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7198515769944343\n",
            "Mean loss for 4030 iterations: 0.012044953312429458\n",
            "Mean loss for 4040 iterations: 0.012019343391987397\n",
            "Accuracy Bag: 0.8877045805684177\n",
            "Accuracy NM: 0.9897959183673468\n",
            "Accuracy CL: 0.7384044526901669\n",
            "Mean loss for 4050 iterations: 0.011994427424409272\n",
            "Mean loss for 4060 iterations: 0.011968264255721868\n",
            "Accuracy Bag: 0.8858401160583514\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.7189239332096474\n",
            "Mean loss for 4070 iterations: 0.011943302441659956\n",
            "Mean loss for 4080 iterations: 0.011917687776679254\n",
            "Accuracy Bag: 0.9044499853845148\n",
            "Accuracy NM: 0.9879406307977735\n",
            "Accuracy CL: 0.6864564007421151\n",
            "Mean loss for 4090 iterations: 0.011891791508306064\n",
            "Mean loss for 4100 iterations: 0.011865873710204785\n",
            "Accuracy Bag: 0.902528527244018\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.7217068645640073\n",
            "Mean loss for 4110 iterations: 0.011839769857994665\n",
            "Mean loss for 4120 iterations: 0.011813929718125808\n",
            "Accuracy Bag: 0.888575230722774\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.7096474953617812\n",
            "Mean loss for 4130 iterations: 0.011788203855264018\n",
            "Mean loss for 4140 iterations: 0.011762759907769094\n",
            "Accuracy Bag: 0.9174094675500485\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7003710575139146\n",
            "Mean loss for 4150 iterations: 0.011737055590916957\n",
            "Mean loss for 4160 iterations: 0.011711095029214937\n",
            "Accuracy Bag: 0.8729188872447673\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.6734693877551021\n",
            "Mean loss for 4170 iterations: 0.011686207309425434\n",
            "Mean loss for 4180 iterations: 0.011661350785118376\n",
            "Accuracy Bag: 0.898845869323845\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.7087198515769945\n",
            "Mean loss for 4190 iterations: 0.011637050623390495\n",
            "Mean loss for 4200 iterations: 0.011611935460799335\n",
            "Accuracy Bag: 0.9044976088757051\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.7402597402597404\n",
            "Mean loss for 4210 iterations: 0.011587501159732118\n",
            "Mean loss for 4220 iterations: 0.011562805188510383\n",
            "Accuracy Bag: 0.895173161139154\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7170686456400742\n",
            "Mean loss for 4230 iterations: 0.011539731380199033\n",
            "Mean loss for 4240 iterations: 0.011516333931845197\n",
            "Accuracy Bag: 0.8997928329833538\n",
            "Accuracy NM: 0.984230055658627\n",
            "Accuracy CL: 0.702226345083488\n",
            "Mean loss for 4250 iterations: 0.011491721905826126\n",
            "Mean loss for 4260 iterations: 0.0114679752757461\n",
            "Accuracy Bag: 0.9035227279972227\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.7328385899814471\n",
            "Mean loss for 4270 iterations: 0.011443875790273075\n",
            "Mean loss for 4280 iterations: 0.011420600774431265\n",
            "Accuracy Bag: 0.9202115255803835\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7161410018552876\n",
            "Mean loss for 4290 iterations: 0.011397523907664376\n",
            "Mean loss for 4300 iterations: 0.011374379278463177\n",
            "Accuracy Bag: 0.9034840882477781\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.7161410018552876\n",
            "Mean loss for 4310 iterations: 0.01135092598220737\n",
            "Mean loss for 4320 iterations: 0.011328060387054225\n",
            "Accuracy Bag: 0.8951450507214331\n",
            "Accuracy NM: 0.9851576994434136\n",
            "Accuracy CL: 0.7411873840445268\n",
            "Mean loss for 4330 iterations: 0.011304843816639373\n",
            "Mean loss for 4340 iterations: 0.011281327486834473\n",
            "Accuracy Bag: 0.88768564709119\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.7198515769944343\n",
            "Mean loss for 4350 iterations: 0.011258213443593951\n",
            "Mean loss for 4360 iterations: 0.011235553268205033\n",
            "Accuracy Bag: 0.9025853276757012\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.7319109461966605\n",
            "Mean loss for 4370 iterations: 0.011213035821796526\n",
            "Mean loss for 4380 iterations: 0.011190999771118697\n",
            "Accuracy Bag: 0.8923902297847941\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.6910946196660482\n",
            "Mean loss for 4390 iterations: 0.011168416466536494\n",
            "Mean loss for 4400 iterations: 0.011147121786585062\n",
            "Accuracy Bag: 0.8710161693827512\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.6410018552875695\n",
            "Mean loss for 4410 iterations: 0.011126054429450857\n",
            "Mean loss for 4420 iterations: 0.011105722442129864\n",
            "Accuracy Bag: 0.9313617980775565\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.7133580705009276\n",
            "Mean loss for 4430 iterations: 0.011084933373510115\n",
            "Mean loss for 4440 iterations: 0.011064489459192726\n",
            "Accuracy Bag: 0.9007394102453684\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.7244897959183674\n",
            "Mean loss for 4450 iterations: 0.011044490482384114\n",
            "Mean loss for 4460 iterations: 0.01102527304180345\n",
            "Accuracy Bag: 0.8952202050341026\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.6808905380333953\n",
            "Mean loss for 4470 iterations: 0.011006587966214641\n",
            "Mean loss for 4480 iterations: 0.010988076174053925\n",
            "Accuracy Bag: 0.9026226150339152\n",
            "Accuracy NM: 0.9879406307977738\n",
            "Accuracy CL: 0.7161410018552876\n",
            "Mean loss for 4490 iterations: 0.0109680364451109\n",
            "Mean loss for 4500 iterations: 0.010948787725727825\n",
            "Accuracy Bag: 0.9304528945712505\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.7207792207792209\n",
            "Mean loss for 4510 iterations: 0.010930145402819547\n",
            "Mean loss for 4520 iterations: 0.010910604597189472\n",
            "Accuracy Bag: 0.9137365661666105\n",
            "Accuracy NM: 0.9916512059369204\n",
            "Accuracy CL: 0.7179962894248608\n",
            "Mean loss for 4530 iterations: 0.010890756122384115\n",
            "Mean loss for 4540 iterations: 0.010870294079817508\n",
            "Accuracy Bag: 0.9202771165550655\n",
            "Accuracy NM: 0.9907235621521334\n",
            "Accuracy CL: 0.6836734693877551\n",
            "Mean loss for 4550 iterations: 0.010850746756714416\n",
            "Mean loss for 4560 iterations: 0.01083230436202907\n",
            "Accuracy Bag: 0.9210911594764855\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.706864564007421\n",
            "Mean loss for 4570 iterations: 0.01081214875381308\n",
            "Mean loss for 4580 iterations: 0.010793548974648395\n",
            "Accuracy Bag: 0.9016289938769522\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.6966604823747681\n",
            "Mean loss for 4590 iterations: 0.010774021783188896\n",
            "Mean loss for 4600 iterations: 0.010754320519364504\n",
            "Accuracy Bag: 0.8988749457353019\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.6910946196660482\n",
            "Mean loss for 4610 iterations: 0.010735958120311532\n",
            "Mean loss for 4620 iterations: 0.010716746401375495\n",
            "Accuracy Bag: 0.9007298469073807\n",
            "Accuracy NM: 0.9879406307977738\n",
            "Accuracy CL: 0.699443413729128\n",
            "Mean loss for 4630 iterations: 0.010697162748075199\n",
            "Mean loss for 4640 iterations: 0.01067863588510465\n",
            "Accuracy Bag: 0.898855819059327\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7077922077922079\n",
            "Mean loss for 4650 iterations: 0.010658724234612805\n",
            "Mean loss for 4660 iterations: 0.010638952979525324\n",
            "Accuracy Bag: 0.8877049669659122\n",
            "Accuracy NM: 0.9888682745825604\n",
            "Accuracy CL: 0.6614100185528757\n",
            "Mean loss for 4670 iterations: 0.010619080231441619\n",
            "Mean loss for 4680 iterations: 0.010599492082698047\n",
            "Accuracy Bag: 0.9053214083338598\n",
            "Accuracy NM: 0.9888682745825604\n",
            "Accuracy CL: 0.7578849721706864\n",
            "Mean loss for 4690 iterations: 0.0105796075646803\n",
            "Mean loss for 4700 iterations: 0.010559568361777372\n",
            "Accuracy Bag: 0.8914245258468046\n",
            "Accuracy NM: 0.9916512059369204\n",
            "Accuracy CL: 0.7319109461966604\n",
            "Mean loss for 4710 iterations: 0.01053959938966372\n",
            "Mean loss for 4720 iterations: 0.010519853471138912\n",
            "Accuracy Bag: 0.8970477685834495\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.7244897959183674\n",
            "Mean loss for 4730 iterations: 0.010500052789221621\n",
            "Mean loss for 4740 iterations: 0.010480448159155017\n",
            "Accuracy Bag: 0.8922477457087177\n",
            "Accuracy NM: 0.9879406307977738\n",
            "Accuracy CL: 0.7189239332096474\n",
            "Mean loss for 4750 iterations: 0.01046074400617018\n",
            "Mean loss for 4760 iterations: 0.010441638256602078\n",
            "Accuracy Bag: 0.8886421740886864\n",
            "Accuracy NM: 0.9833024118738405\n",
            "Accuracy CL: 0.7012987012987012\n",
            "Mean loss for 4770 iterations: 0.010421920503359468\n",
            "Mean loss for 4780 iterations: 0.010402354106297166\n",
            "Accuracy Bag: 0.9016861807061299\n",
            "Accuracy NM: 0.9888682745825604\n",
            "Accuracy CL: 0.7374768089053803\n",
            "Mean loss for 4790 iterations: 0.0103823763919758\n",
            "Mean loss for 4800 iterations: 0.01036317271945639\n",
            "Accuracy Bag: 0.8988839294770478\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7179962894248608\n",
            "Mean loss for 4810 iterations: 0.010344332618021219\n",
            "Mean loss for 4820 iterations: 0.010325556511423624\n",
            "Accuracy Bag: 0.911862151921062\n",
            "Accuracy NM: 0.9953617810760669\n",
            "Accuracy CL: 0.7031539888682746\n",
            "Mean loss for 4830 iterations: 0.010307376372797114\n",
            "Mean loss for 4840 iterations: 0.010289083246294647\n",
            "Accuracy Bag: 0.89517412713289\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.6892393320964749\n",
            "Mean loss for 4850 iterations: 0.010270740674903434\n",
            "Mean loss for 4860 iterations: 0.010252347131087991\n",
            "Accuracy Bag: 0.8970290283049689\n",
            "Accuracy NM: 0.9879406307977738\n",
            "Accuracy CL: 0.7124304267161411\n",
            "Mean loss for 4870 iterations: 0.010234909929379699\n",
            "Mean loss for 4880 iterations: 0.010216995563907821\n",
            "Accuracy Bag: 0.8886511578304321\n",
            "Accuracy NM: 0.9925788497217067\n",
            "Accuracy CL: 0.702226345083488\n",
            "Mean loss for 4890 iterations: 0.010200458457972593\n",
            "Mean loss for 4900 iterations: 0.010186162415030056\n",
            "Accuracy Bag: 0.8691515116739378\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.6781076066790352\n",
            "Mean loss for 4910 iterations: 0.010169912812605528\n",
            "Mean loss for 4920 iterations: 0.010152297961418314\n",
            "Accuracy Bag: 0.9025474607212458\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7365491651205937\n",
            "Mean loss for 4930 iterations: 0.010135042666931401\n",
            "Mean loss for 4940 iterations: 0.01011727357204348\n",
            "Accuracy Bag: 0.9146355199374345\n",
            "Accuracy NM: 0.9833024118738404\n",
            "Accuracy CL: 0.663265306122449\n",
            "Mean loss for 4950 iterations: 0.010099765404243366\n",
            "Mean loss for 4960 iterations: 0.01008243773040381\n",
            "Accuracy Bag: 0.8932616527341392\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.7031539888682745\n",
            "Mean loss for 4970 iterations: 0.01006470389700304\n",
            "Mean loss for 4980 iterations: 0.010046796550512913\n",
            "Accuracy Bag: 0.9072235465996346\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.7217068645640075\n",
            "Mean loss for 4990 iterations: 0.010029868409571258\n",
            "Mean loss for 5000 iterations: 0.010012226347459364\n",
            "Accuracy Bag: 0.8979281752745404\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.6901669758812616\n",
            "Mean loss for 5010 iterations: 0.009994979631738387\n",
            "Mean loss for 5020 iterations: 0.009977441033868715\n",
            "Accuracy Bag: 0.8970288351062217\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.7179962894248609\n",
            "Mean loss for 5030 iterations: 0.00996009005990403\n",
            "Mean loss for 5040 iterations: 0.009943170278431411\n",
            "Accuracy Bag: 0.9415664593064513\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.725417439703154\n",
            "Mean loss for 5050 iterations: 0.009926255750340193\n",
            "Mean loss for 5060 iterations: 0.009909418491027645\n",
            "Accuracy Bag: 0.9211391693651702\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.7551020408163265\n",
            "Mean loss for 5070 iterations: 0.009891871107072056\n",
            "Mean loss for 5080 iterations: 0.009875239503022085\n",
            "Accuracy Bag: 0.8895694314759784\n",
            "Accuracy NM: 0.9805194805194805\n",
            "Accuracy CL: 0.7068645640074211\n",
            "Mean loss for 5090 iterations: 0.009858443082924013\n",
            "Mean loss for 5100 iterations: 0.009841986721225753\n",
            "Accuracy Bag: 0.9080947763502323\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.7523191094619666\n",
            "Mean loss for 5110 iterations: 0.009825535207967353\n",
            "Mean loss for 5120 iterations: 0.00980911920152323\n",
            "Accuracy Bag: 0.9026419349086372\n",
            "Accuracy NM: 0.9879406307977738\n",
            "Accuracy CL: 0.679035250463822\n",
            "Mean loss for 5130 iterations: 0.009792099415044256\n",
            "Mean loss for 5140 iterations: 0.009775443462085216\n",
            "Accuracy Bag: 0.9053682590300614\n",
            "Accuracy NM: 0.9897959183673468\n",
            "Accuracy CL: 0.6985157699443415\n",
            "Mean loss for 5150 iterations: 0.009758717641694973\n",
            "Mean loss for 5160 iterations: 0.009741707673745185\n",
            "Accuracy Bag: 0.911028982323667\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.7421150278293136\n",
            "Mean loss for 5170 iterations: 0.00972476148697477\n",
            "Mean loss for 5180 iterations: 0.009708163488447162\n",
            "Accuracy Bag: 0.8858680332773251\n",
            "Accuracy NM: 0.9879406307977738\n",
            "Accuracy CL: 0.7115027829313545\n",
            "Mean loss for 5190 iterations: 0.009691563372460202\n",
            "Mean loss for 5200 iterations: 0.009675256289402358\n",
            "Accuracy Bag: 0.9127985862488472\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.6966604823747682\n",
            "Mean loss for 5210 iterations: 0.009658920937454065\n",
            "Mean loss for 5220 iterations: 0.009642562258865848\n",
            "Accuracy Bag: 0.8905261516722223\n",
            "Accuracy NM: 0.9805194805194805\n",
            "Accuracy CL: 0.686456400742115\n",
            "Mean loss for 5230 iterations: 0.009626697125134257\n",
            "Mean loss for 5240 iterations: 0.009610964199839686\n",
            "Accuracy Bag: 0.9034752977047795\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.6883116883116884\n",
            "Mean loss for 5250 iterations: 0.009596732351011486\n",
            "Mean loss for 5260 iterations: 0.009580715955957126\n",
            "Accuracy Bag: 0.9007388306491267\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.7115027829313543\n",
            "Mean loss for 5270 iterations: 0.009564665892480964\n",
            "Mean loss for 5280 iterations: 0.009548183496639412\n",
            "Accuracy Bag: 0.9035888985681461\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.6892393320964749\n",
            "Mean loss for 5290 iterations: 0.009532150751573851\n",
            "Mean loss for 5300 iterations: 0.009515980848708114\n",
            "Accuracy Bag: 0.8877615741988483\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.7244897959183674\n",
            "Mean loss for 5310 iterations: 0.009499726422874168\n",
            "Mean loss for 5320 iterations: 0.009483521867495208\n",
            "Accuracy Bag: 0.907252236613597\n",
            "Accuracy NM: 0.9888682745825604\n",
            "Accuracy CL: 0.6910946196660483\n",
            "Mean loss for 5330 iterations: 0.009467800958149242\n",
            "Mean loss for 5340 iterations: 0.009452504095093723\n",
            "Accuracy Bag: 0.9249059653397584\n",
            "Accuracy NM: 0.9907235621521338\n",
            "Accuracy CL: 0.7532467532467532\n",
            "Mean loss for 5350 iterations: 0.009437073648270225\n",
            "Mean loss for 5360 iterations: 0.009421536957307214\n",
            "Accuracy Bag: 0.9044402288477801\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7263450834879406\n",
            "Mean loss for 5370 iterations: 0.009405730132474126\n",
            "Mean loss for 5380 iterations: 0.009390325797351183\n",
            "Accuracy Bag: 0.8829902413380792\n",
            "Accuracy NM: 0.9907235621521338\n",
            "Accuracy CL: 0.7096474953617812\n",
            "Mean loss for 5390 iterations: 0.009374901270257903\n",
            "Mean loss for 5400 iterations: 0.009360284376778981\n",
            "Accuracy Bag: 0.9183646421563144\n",
            "Accuracy NM: 0.9935064935064936\n",
            "Accuracy CL: 0.7105751391465677\n",
            "Mean loss for 5410 iterations: 0.009344958615141888\n",
            "Mean loss for 5420 iterations: 0.009329724599250418\n",
            "Accuracy Bag: 0.9090602870894745\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.7217068645640073\n",
            "Mean loss for 5430 iterations: 0.009314413362765932\n",
            "Mean loss for 5440 iterations: 0.00929932377308593\n",
            "Accuracy Bag: 0.8923908093810357\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.7170686456400742\n",
            "Mean loss for 5450 iterations: 0.009284696775558512\n",
            "Mean loss for 5460 iterations: 0.009270246577618756\n",
            "Accuracy Bag: 0.8997834628441136\n",
            "Accuracy NM: 0.9833024118738404\n",
            "Accuracy CL: 0.6883116883116883\n",
            "Mean loss for 5470 iterations: 0.009256005480588508\n",
            "Mean loss for 5480 iterations: 0.00924247266454327\n",
            "Accuracy Bag: 0.9016574906921673\n",
            "Accuracy NM: 0.9916512059369204\n",
            "Accuracy CL: 0.7012987012987012\n",
            "Mean loss for 5490 iterations: 0.009228595075597125\n",
            "Mean loss for 5500 iterations: 0.009214403134794503\n",
            "Accuracy Bag: 0.898817565707377\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.693877551020408\n",
            "Mean loss for 5510 iterations: 0.009200281874463576\n",
            "Mean loss for 5520 iterations: 0.009186803607274445\n",
            "Accuracy Bag: 0.9202205093221294\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7040816326530611\n",
            "Mean loss for 5530 iterations: 0.009173182988949107\n",
            "Mean loss for 5540 iterations: 0.009159943276206943\n",
            "Accuracy Bag: 0.9276510297396627\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.6790352504638218\n",
            "Mean loss for 5550 iterations: 0.009147497743280575\n",
            "Mean loss for 5560 iterations: 0.009135101912279881\n",
            "Accuracy Bag: 0.9044408084440217\n",
            "Accuracy NM: 0.984230055658627\n",
            "Accuracy CL: 0.6419294990723562\n",
            "Mean loss for 5570 iterations: 0.00912267439068879\n",
            "Mean loss for 5580 iterations: 0.0091110095686675\n",
            "Accuracy Bag: 0.9220666199512095\n",
            "Accuracy NM: 0.9833024118738405\n",
            "Accuracy CL: 0.6966604823747681\n",
            "Mean loss for 5590 iterations: 0.009098747221737695\n",
            "Mean loss for 5600 iterations: 0.009086997575819364\n",
            "Accuracy Bag: 0.907204806321154\n",
            "Accuracy NM: 0.9907235621521334\n",
            "Accuracy CL: 0.6688311688311688\n",
            "Mean loss for 5610 iterations: 0.009075290325296227\n",
            "Mean loss for 5620 iterations: 0.009063353705461973\n",
            "Accuracy Bag: 0.9007390238478737\n",
            "Accuracy NM: 0.9823747680890538\n",
            "Accuracy CL: 0.7077922077922078\n",
            "Mean loss for 5630 iterations: 0.00905065422790668\n",
            "Mean loss for 5640 iterations: 0.009037677424990077\n",
            "Accuracy Bag: 0.8885947437962433\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.6558441558441559\n",
            "Mean loss for 5650 iterations: 0.009025194612468193\n",
            "Mean loss for 5660 iterations: 0.009012576833798969\n",
            "Accuracy Bag: 0.9100347815704626\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.6623376623376623\n",
            "Mean loss for 5670 iterations: 0.008999496427218564\n",
            "Mean loss for 5680 iterations: 0.008986262020419378\n",
            "Accuracy Bag: 0.9007013500921656\n",
            "Accuracy NM: 0.9823747680890538\n",
            "Accuracy CL: 0.6706864564007422\n",
            "Mean loss for 5690 iterations: 0.008972886133870386\n",
            "Mean loss for 5700 iterations: 0.0089593943193368\n",
            "Accuracy Bag: 0.8979375454137805\n",
            "Accuracy NM: 0.9833024118738405\n",
            "Accuracy CL: 0.6985157699443415\n",
            "Mean loss for 5710 iterations: 0.008946378580146228\n",
            "Mean loss for 5720 iterations: 0.008932469949927413\n",
            "Accuracy Bag: 0.9202304590576111\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7179962894248609\n",
            "Mean loss for 5730 iterations: 0.008919278558833385\n",
            "Mean loss for 5740 iterations: 0.008906508082814879\n",
            "Accuracy Bag: 0.901676230970648\n",
            "Accuracy NM: 0.9823747680890537\n",
            "Accuracy CL: 0.6827458256029686\n",
            "Mean loss for 5750 iterations: 0.008894033680682617\n",
            "Mean loss for 5760 iterations: 0.008881374117544914\n",
            "Accuracy Bag: 0.9016391368111815\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6938775510204082\n",
            "Mean loss for 5770 iterations: 0.008869397443958999\n",
            "Mean loss for 5780 iterations: 0.008857038948164989\n",
            "Accuracy Bag: 0.89522976837209\n",
            "Accuracy NM: 0.9897959183673468\n",
            "Accuracy CL: 0.7012987012987013\n",
            "Mean loss for 5790 iterations: 0.00884540560754068\n",
            "Mean loss for 5800 iterations: 0.008833468933281156\n",
            "Accuracy Bag: 0.8932798134163779\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.7161410018552876\n",
            "Mean loss for 5810 iterations: 0.008820697351466731\n",
            "Mean loss for 5820 iterations: 0.008809475210480592\n",
            "Accuracy Bag: 0.8979371590162862\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.6549165120593692\n",
            "Mean loss for 5830 iterations: 0.008797442468293478\n",
            "Mean loss for 5840 iterations: 0.008786314516161327\n",
            "Accuracy Bag: 0.9136985060134076\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.6948051948051949\n",
            "Mean loss for 5850 iterations: 0.008776115560319582\n",
            "Mean loss for 5860 iterations: 0.00876450369694223\n",
            "Accuracy Bag: 0.9173903408740738\n",
            "Accuracy NM: 0.9935064935064936\n",
            "Accuracy CL: 0.7207792207792207\n",
            "Mean loss for 5870 iterations: 0.008752828674632172\n",
            "Mean loss for 5880 iterations: 0.008740477237949322\n",
            "Accuracy Bag: 0.9025853276757012\n",
            "Accuracy NM: 0.9860853432282005\n",
            "Accuracy CL: 0.7226345083487941\n",
            "Mean loss for 5890 iterations: 0.008728044138574899\n",
            "Mean loss for 5900 iterations: 0.008715797413011787\n",
            "Accuracy Bag: 0.9025948910136887\n",
            "Accuracy NM: 0.9870129870129869\n",
            "Accuracy CL: 0.6836734693877551\n",
            "Mean loss for 5910 iterations: 0.00870313288284418\n",
            "Mean loss for 5920 iterations: 0.008690771586739051\n",
            "Accuracy Bag: 0.9007204767681404\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.7189239332096474\n",
            "Mean loss for 5930 iterations: 0.008678135847559252\n",
            "Mean loss for 5940 iterations: 0.008665750712761008\n",
            "Accuracy Bag: 0.9155635501197157\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.7170686456400741\n",
            "Mean loss for 5950 iterations: 0.008653718385531884\n",
            "Mean loss for 5960 iterations: 0.008641461001733226\n",
            "Accuracy Bag: 0.9072331099376219\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.7430426716141002\n",
            "Mean loss for 5970 iterations: 0.008629123395044335\n",
            "Mean loss for 5980 iterations: 0.00861669187359798\n",
            "Accuracy Bag: 0.9071856796451789\n",
            "Accuracy NM: 0.9860853432282005\n",
            "Accuracy CL: 0.7031539888682746\n",
            "Mean loss for 5990 iterations: 0.008605064206947795\n",
            "Mean loss for 6000 iterations: 0.008592985791869675\n",
            "Accuracy Bag: 0.9154969931512976\n",
            "Accuracy NM: 0.9925788497217067\n",
            "Accuracy CL: 0.6846011131725418\n",
            "Mean loss for 6010 iterations: 0.008580705997532256\n",
            "Mean loss for 6020 iterations: 0.008568409091188781\n",
            "Accuracy Bag: 0.8960633243669794\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.7207792207792207\n",
            "Mean loss for 6030 iterations: 0.008555691991775829\n",
            "Mean loss for 6040 iterations: 0.008543111081329071\n",
            "Accuracy Bag: 0.8942076503999118\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.7022263450834879\n",
            "Mean loss for 6050 iterations: 0.00853053487661489\n",
            "Mean loss for 6060 iterations: 0.008517730477766543\n",
            "Accuracy Bag: 0.9007397966428626\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6994434137291281\n",
            "Mean loss for 6070 iterations: 0.008505190731027863\n",
            "Mean loss for 6080 iterations: 0.008492900616925238\n",
            "Accuracy Bag: 0.9035036013212476\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.6725417439703153\n",
            "Mean loss for 6090 iterations: 0.008480539584061845\n",
            "Mean loss for 6100 iterations: 0.008468474304433779\n",
            "Accuracy Bag: 0.9155920469349309\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7337662337662337\n",
            "Mean loss for 6110 iterations: 0.008455937671885806\n",
            "Mean loss for 6120 iterations: 0.008443524893076981\n",
            "Accuracy Bag: 0.8998308931365565\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.735621521335807\n",
            "Mean loss for 6130 iterations: 0.008431136891169214\n",
            "Mean loss for 6140 iterations: 0.008418294990090926\n",
            "Accuracy Bag: 0.9062484725224048\n",
            "Accuracy NM: 0.9833024118738404\n",
            "Accuracy CL: 0.7087198515769945\n",
            "Mean loss for 6150 iterations: 0.008405540383731326\n",
            "Mean loss for 6160 iterations: 0.008393204806828168\n",
            "Accuracy Bag: 0.8951735475366484\n",
            "Accuracy NM: 0.9823747680890538\n",
            "Accuracy CL: 0.7040816326530612\n",
            "Mean loss for 6170 iterations: 0.008381120300561384\n",
            "Mean loss for 6180 iterations: 0.008369358926812755\n",
            "Accuracy Bag: 0.9109155746590477\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.6975881261595547\n",
            "Mean loss for 6190 iterations: 0.008357079105263822\n",
            "Mean loss for 6200 iterations: 0.008345168081357242\n",
            "Accuracy Bag: 0.8960920143809421\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.7087198515769945\n",
            "Mean loss for 6210 iterations: 0.008333246435252267\n",
            "Mean loss for 6220 iterations: 0.008321685310876351\n",
            "Accuracy Bag: 0.8970672816569188\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.7161410018552874\n",
            "Mean loss for 6230 iterations: 0.00831000963414692\n",
            "Mean loss for 6240 iterations: 0.008298370438037987\n",
            "Accuracy Bag: 0.8988839294770478\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6929499072356216\n",
            "Mean loss for 6250 iterations: 0.008286970067280344\n",
            "Mean loss for 6260 iterations: 0.008275814870859276\n",
            "Accuracy Bag: 0.8970194649669814\n",
            "Accuracy NM: 0.9823747680890538\n",
            "Accuracy CL: 0.7161410018552876\n",
            "Mean loss for 6270 iterations: 0.008264643868967666\n",
            "Mean loss for 6280 iterations: 0.008254379995840536\n",
            "Accuracy Bag: 0.8830659752469899\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.6484230055658626\n",
            "Mean loss for 6290 iterations: 0.008243402280052235\n",
            "Mean loss for 6300 iterations: 0.008232591083441447\n",
            "Accuracy Bag: 0.9081324501059406\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.699443413729128\n",
            "Mean loss for 6310 iterations: 0.008221550051260535\n",
            "Mean loss for 6320 iterations: 0.008210964644664111\n",
            "Accuracy Bag: 0.8979279820757932\n",
            "Accuracy NM: 0.9823747680890539\n",
            "Accuracy CL: 0.6929499072356216\n",
            "Mean loss for 6330 iterations: 0.008200122288845356\n",
            "Mean loss for 6340 iterations: 0.008190108940223076\n",
            "Accuracy Bag: 0.9155820971994489\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.6317254174397032\n",
            "Mean loss for 6350 iterations: 0.008181827449263624\n",
            "Mean loss for 6360 iterations: 0.00817227078031173\n",
            "Accuracy Bag: 0.9099496775223113\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.706864564007421\n",
            "Mean loss for 6370 iterations: 0.008162201329467385\n",
            "Mean loss for 6380 iterations: 0.008151871898274673\n",
            "Accuracy Bag: 0.8951733543379011\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.7068645640074213\n",
            "Mean loss for 6390 iterations: 0.008141837190974763\n",
            "Mean loss for 6400 iterations: 0.008130955644028291\n",
            "Accuracy Bag: 0.9192739320601149\n",
            "Accuracy NM: 0.9814471243042672\n",
            "Accuracy CL: 0.7087198515769945\n",
            "Mean loss for 6410 iterations: 0.008120323808256438\n",
            "Mean loss for 6420 iterations: 0.008109439512648739\n",
            "Accuracy Bag: 0.8793555933065907\n",
            "Accuracy NM: 0.9805194805194805\n",
            "Accuracy CL: 0.7040816326530612\n",
            "Mean loss for 6430 iterations: 0.008098011572565015\n",
            "Mean loss for 6440 iterations: 0.008087062431981654\n",
            "Accuracy Bag: 0.9007493599808502\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.6948051948051949\n",
            "Mean loss for 6450 iterations: 0.00807613441730983\n",
            "Mean loss for 6460 iterations: 0.008064850505832302\n",
            "Accuracy Bag: 0.8933274369075682\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.7319109461966604\n",
            "Mean loss for 6470 iterations: 0.008053669432523428\n",
            "Mean loss for 6480 iterations: 0.00804306831982732\n",
            "Accuracy Bag: 0.9035032149237533\n",
            "Accuracy NM: 0.9879406307977738\n",
            "Accuracy CL: 0.7523191094619666\n",
            "Mean loss for 6490 iterations: 0.00803226567195382\n",
            "Mean loss for 6500 iterations: 0.008022010545129887\n",
            "Accuracy Bag: 0.8915098230937031\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.6762523191094619\n",
            "Mean loss for 6510 iterations: 0.008011573538457412\n",
            "Mean loss for 6520 iterations: 0.008001146256153049\n",
            "Accuracy Bag: 0.8914534090595143\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.7142857142857143\n",
            "Mean loss for 6530 iterations: 0.007990743683098073\n",
            "Mean loss for 6540 iterations: 0.007979827415490371\n",
            "Accuracy Bag: 0.9081423998414224\n",
            "Accuracy NM: 0.9907235621521334\n",
            "Accuracy CL: 0.6985157699443413\n",
            "Mean loss for 6550 iterations: 0.007968974967952817\n",
            "Mean loss for 6560 iterations: 0.007958508017802115\n",
            "Accuracy Bag: 0.8783992595078416\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.6883116883116883\n",
            "Mean loss for 6570 iterations: 0.007948129307973032\n",
            "Mean loss for 6580 iterations: 0.007937326884013246\n",
            "Accuracy Bag: 0.880274639747126\n",
            "Accuracy NM: 0.9897959183673468\n",
            "Accuracy CL: 0.7217068645640075\n",
            "Mean loss for 6590 iterations: 0.007926799236534112\n",
            "Mean loss for 6600 iterations: 0.00791629774674551\n",
            "Accuracy Bag: 0.9007394102453682\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7170686456400741\n",
            "Mean loss for 6610 iterations: 0.007905392697026021\n",
            "Mean loss for 6620 iterations: 0.007894620200381874\n",
            "Accuracy Bag: 0.891396608627831\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7207792207792209\n",
            "Mean loss for 6630 iterations: 0.007883675823329321\n",
            "Mean loss for 6640 iterations: 0.007872862906081439\n",
            "Accuracy Bag: 0.8941986666581659\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.725417439703154\n",
            "Mean loss for 6650 iterations: 0.0078621663243284\n",
            "Mean loss for 6660 iterations: 0.007851552070280524\n",
            "Accuracy Bag: 0.8961009981226877\n",
            "Accuracy NM: 0.9851576994434136\n",
            "Accuracy CL: 0.7198515769944341\n",
            "Mean loss for 6670 iterations: 0.00784102016495081\n",
            "Mean loss for 6680 iterations: 0.007830609885677622\n",
            "Accuracy Bag: 0.9090796069641965\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7059369202226344\n",
            "Mean loss for 6690 iterations: 0.007820085858692042\n",
            "Mean loss for 6700 iterations: 0.0078095658944405824\n",
            "Accuracy Bag: 0.871934829425792\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.6883116883116882\n",
            "Mean loss for 6710 iterations: 0.00779920590115263\n",
            "Mean loss for 6720 iterations: 0.00778846663581806\n",
            "Accuracy Bag: 0.8821674078736605\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.7430426716141002\n",
            "Mean loss for 6730 iterations: 0.007777801339025151\n",
            "Mean loss for 6740 iterations: 0.007767275792017131\n",
            "Accuracy Bag: 0.9063242064313158\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.7207792207792207\n",
            "Mean loss for 6750 iterations: 0.007756745469459997\n",
            "Mean loss for 6760 iterations: 0.007746207131520931\n",
            "Accuracy Bag: 0.888698394924128\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.712430426716141\n",
            "Mean loss for 6770 iterations: 0.007735612612676835\n",
            "Mean loss for 6780 iterations: 0.007725078655031616\n",
            "Accuracy Bag: 0.8980046819784402\n",
            "Accuracy NM: 0.9907235621521338\n",
            "Accuracy CL: 0.7671614100185529\n",
            "Mean loss for 6790 iterations: 0.007714524725485284\n",
            "Mean loss for 6800 iterations: 0.007703845227597947\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-7fe9c5d0f0b5>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-4a1a719490db>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeddings, labels)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;31m# Positive and negative indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mpos_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0manchor_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mneg_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0manchor_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from einops import rearrange\n",
        "from torch.cuda.amp import autocast\n",
        "from torch.cuda.amp import GradScaler\n",
        "from torch.optim import SGD\n",
        "\n",
        "model = CustomModel()\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "scaler = GradScaler()\n",
        "optimizer = SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.05,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "loss_fn = TripletLossIterative()\n",
        "iter = 0\n",
        "losses = np.array([])\n",
        "for batch in train_loader:\n",
        "  seqs, labs, _, _ = batch\n",
        "  if torch.cuda.is_available():\n",
        "    seqs = seqs.cuda()\n",
        "    labs = labs.cuda()\n",
        "  optimizer.zero_grad()\n",
        "  with autocast():\n",
        "    seqs = seqs.reshape(-1, 1, 64, 64)\n",
        "    output = model(seqs)\n",
        "\n",
        "  loss = loss_fn(output, labs)\n",
        "  scaler.scale(loss).backward()\n",
        "  scaler.step(optimizer)\n",
        "  scaler.update()\n",
        "  torch.cuda.empty_cache()\n",
        "  losses = np.append(losses, loss.item())\n",
        "  iter += 1\n",
        "  if iter % 10 == 0:\n",
        "    print(f\"Mean loss for {iter} iterations: {losses.mean()}\")\n",
        "  if iter % 20 == 0:\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "    seq_types = []\n",
        "    views = []\n",
        "    with torch.no_grad():\n",
        "      for batch in test_loader:\n",
        "        test_seqs, test_labs, batch_types, batch_views = batch\n",
        "        if torch.cuda.is_available():\n",
        "          test_seqs = test_seqs.cuda()\n",
        "          test_labs = test_labs.cuda()\n",
        "        test_seqs = test_seqs.reshape(-1, 1, 64, 64)\n",
        "        test_output = model(test_seqs)\n",
        "        features.append(test_output.cpu())\n",
        "        labels.append(test_labs.cpu())\n",
        "        seq_types.append(batch_types)\n",
        "        views.append(batch_views)\n",
        "    features = torch.cat(features, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "    views = np.array(views).reshape(-1)\n",
        "    seq_types = np.array(seq_types).reshape(-1)\n",
        "    gallery_seq = ['nm-01', 'nm-02', 'nm-03', 'nm-04']\n",
        "    probe_seq = ['bg-01', 'bg-02']\n",
        "    acc = eval(probe_seq, gallery_seq, features, labels, views, seq_types)\n",
        "    print(f\"Accuracy Bag: {acc}\")\n",
        "    probe_seq = ['nm-05', 'nm-06']\n",
        "    acc = eval(probe_seq, gallery_seq, features, labels, views, seq_types)\n",
        "    print(f\"Accuracy NM: {acc}\")\n",
        "    probe_seq = ['cl-01','cl-02']\n",
        "    acc = eval(probe_seq, gallery_seq, features, labels, views, seq_types)\n",
        "    print(f\"Accuracy CL: {acc}\")\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNJZm_RUntbf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk-nc_XRafZM"
      },
      "source": [
        "Lr=0.1 için resnet9 modeli fazla büyük bir model olmadığı ve parametre sayısı az olduğu için çok dallanma göstermeyebiliyor ve her model çalıştırıldığında farklı sonuçlar elde edilmiyor. Model varacağı max acc noktasına ulaşabiliyor.\n",
        "\n",
        "resnet9 + lstm kullanildiginda model bir calistiginda cok iyi sonuc ve baska gun calistirildiginda cok daha dusuk sonuclar verebiliyor.\n",
        "referans1 resim=> day1 (resnet9 + lstm) for 2000 iter\n",
        "referans2 resim=> day2 (resnet9 + lstm) for 2000 iter\n",
        "bu iki resimde goruldugu uzere lr=0.1 oldugunda iki model arasinda cok fark var\n",
        "\n",
        "ref3 resim => resnet for 2000 iter\n",
        "ref3 vs ref3 te gorundugu uzere resnet9 + lstm daha iyi sonuc veriyor ama ayni modeli baska zaman calistirdigimizda cok daha dusuk rakamlar elde ediliyor.\n",
        "\n",
        "fikir resnet9 + lstm modelinde lr=0.01 yapilirsa model kendini dogru sekilde egitebilir mi?\n",
        "\n",
        "ref4 => (resnet9 + lstm) for 2000 iter with lr=0.01\n",
        "\n",
        "ref1 vs ref4 de goruldugu uzere lr=0.01 oldugunda dagilmalar cok olmuyor ve daha duz sekilde model egitiliyor.\n",
        "\n",
        "loss dususu cok daha yavas oldugu goruluyor, bu bize biraz overfit olma durumunu sorgulatiyor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn7UMu_TdsGB"
      },
      "source": [
        "ResNet9 modeli, parametre sayısının az olması ve modelin çok büyük olmaması nedeniyle, öğrenme oranı (lr) 0.1 olduğunda fazla dallanma göstermiyor ve her çalıştırıldığında benzer sonuçlar elde ediliyor. Model, ulaşabileceği maksimum doğruluk noktasına hızlıca erişebiliyor.\n",
        "\n",
        "Ancak, ResNet9 modeline LSTM eklendiğinde, modelin performansı tutarsızlık gösterebiliyor. Bir çalıştırmada çok iyi sonuçlar verirken, başka bir gün aynı model çok daha düşük sonuçlar verebiliyor. Bu durumu aşağıdaki referanslarla inceledik:\n",
        "\n",
        "*\tReferans 1: Day 1 - ResNet9 + LSTM, 2000 iterasyon, lr=0.1\n",
        "*\tReferans 2: Day 2 - ResNet9 + LSTM, 2000 iterasyon, lr=0.1\n",
        "\n",
        "Bu iki referansta görüldüğü üzere, aynı model ve öğrenme oranı kullanıldığında bile sonuçlar arasında büyük farklılıklar oluşuyor.\n",
        "\n",
        "Ayrıca, sadece ResNet9 modelini incelediğimizde:\n",
        "*\tReferans 3: ResNet9, 2000 iterasyon, lr=0.1\n",
        "\n",
        "Referans 3 ile ResNet9 + LSTM modellerini karşılaştırdığımızda, LSTM eklenmiş modelin genellikle daha iyi sonuçlar verdiğini gözlemliyoruz. Ancak, bu modelin farklı zamanlarda tutarsız performans sergilemesi, öğrenme oranının yüksek olmasından kaynaklanıyor olabilir.\n",
        "\n",
        "Bu nedenle, şu soruyu sormaya başladık: “ResNet9 + LSTM modelinde öğrenme oranı lr=0.01 olarak ayarlanırsa, model kendini daha tutarlı ve doğru bir şekilde eğitebilir mi?”\n",
        "\n",
        "Bu hipotezi test etmek için:\n",
        "* Referans 4: ResNet9 + LSTM, 2000 iterasyon, lr=0.01\n",
        "\n",
        "Referans 1 ile Referans 4'ü karşılaştırdığımızda, lr=0.01 olduğunda modelin eğitim sürecinin daha stabil olduğunu ve aşırı dalgalanmaların azaldığını görüyoruz. Eğitim eğrileri daha düz bir şekilde ilerliyor, ancak loss değeri düşüşü daha yavaş gerçekleşiyor. Bu durum, modelin bir yerlerde uzun süre tıkalı kalma riskini düşündürüyor ve daha uzun bir eğitim süresine ihtiyaç duyabileceğini gösteriyor. 2000 iterasyon için bu karşılaştırmayı yapmak adil olmadığı için lr=0.01 olduğu zaman 3000 iterasyon yaparak bir karşılaştırmaya gidelim:\n",
        "* Referans 5: (ResNet9 + LSTM, 3000 iterasyon, lr=0.01) vs (ResNet9 + LSTM, 2000 iterasyon, lr=0.1)\n",
        "\n",
        "Sonuç olarak, öğrenme oranının düşürülmesi, ResNet9 + LSTM modelinin daha tutarlı bir performans sergilemesine yardımcı olabilir. Ancak, bu ayarın loss düşüş hızını etkilediği ve potansiyel olarak overfitting riskini artırdığı da göz önünde bulundurulmalıdır. 2000 iterasyon için bu şekilde bir durum söz konusu değil.\n",
        "\n",
        "Yukarıdaki analiz, çeşitli log ve grafiklerle desteklenmiştir ve modeller arasındaki farkları daha net bir şekilde ortaya koymayı amaçlamaktadır."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lIVkdlGcZEt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}