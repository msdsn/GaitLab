{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msdsn/GaitLab/blob/main/OpenGait_Framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjEAYJIjdbNH"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYVtSgOrdjmf",
        "outputId": "911d3ae0-8a4c-4d2e-f1d4-57c4e6e10062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1d-DGCtaex6YZNg2ad2aPzRWW0xn4bVj3\n",
            "From (redirected): https://drive.google.com/uc?id=1d-DGCtaex6YZNg2ad2aPzRWW0xn4bVj3&confirm=t&uuid=fc71e33d-dc78-46b8-8ef8-abe05a1dca4e\n",
            "To: /content/GaitDatasetB-silh.zip\n",
            "100% 659M/659M [00:10<00:00, 64.4MB/s]\n",
            "Archive:  /content/GaitDatasetB-silh.zip\n",
            "   creating: /content/GaitDatasetB/GaitDatasetB-silh/\n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/001.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/002.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/003.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/004.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/005.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/006.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/007.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/008.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/009.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/010.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/011.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/012.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/013.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/014.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/015.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/016.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/017.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/018.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/019.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/020.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/021.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/022.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/023.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/024.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/025.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/026.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/027.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/028.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/029.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/030.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/031.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/032.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/033.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/034.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/035.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/036.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/037.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/038.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/039.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/040.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/041.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/042.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/043.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/044.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/045.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/046.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/047.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/048.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/049.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/050.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/051.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/052.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/053.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/054.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/055.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/056.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/057.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/058.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/059.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/060.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/061.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/062.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/063.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/064.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/065.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/066.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/067.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/068.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/069.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/070.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/071.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/072.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/073.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/074.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/075.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/076.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/077.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/078.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/079.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/080.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/081.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/082.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/083.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/084.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/085.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/086.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/087.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/088.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/089.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/090.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/091.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/092.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/093.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/094.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/095.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/096.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/097.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/098.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/099.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/100.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/101.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/102.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/103.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/104.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/105.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/106.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/107.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/108.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/109.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/110.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/111.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/112.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/113.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/114.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/115.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/116.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/117.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/118.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/119.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/120.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/121.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/122.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/123.tar.gz  \n",
            " extracting: /content/GaitDatasetB/GaitDatasetB-silh/124.tar.gz  \n"
          ]
        }
      ],
      "source": [
        "!gdown 1d-DGCtaex6YZNg2ad2aPzRWW0xn4bVj3\n",
        "!unzip \"/content/GaitDatasetB-silh.zip\" -d \"/content/GaitDatasetB\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwP-PDlBdlXh"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "import os\n",
        "\n",
        "def unzip_tar_gz(folder_path):\n",
        "  for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".tar.gz\"):\n",
        "      filepath = os.path.join(folder_path, filename)\n",
        "      try:\n",
        "        with tarfile.open(filepath, \"r:gz\") as tar:\n",
        "          tar.extractall(folder_path)\n",
        "          #print(f\"Extracted: {filename}\")\n",
        "        # remove tar.gz\n",
        "        os.remove(filepath)\n",
        "      except Exception as e:\n",
        "        print(f\"Error extracting {filename}: {e}\")\n",
        "\n",
        "unzip_tar_gz(\"/content/GaitDatasetB/GaitDatasetB-silh\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRPyzXUCdr7S",
        "outputId": "38863b12-9e7f-4cbd-840e-7fea30036f95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'OpenGait'...\n",
            "remote: Enumerating objects: 1848, done.\u001b[K\n",
            "remote: Counting objects: 100% (485/485), done.\u001b[K\n",
            "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
            "remote: Total 1848 (delta 385), reused 362 (delta 344), pack-reused 1363 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1848/1848), 20.23 MiB | 11.51 MiB/s, done.\n",
            "Resolving deltas: 100% (1163/1163), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ShiqiYu/OpenGait.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqUtnw6Sdw85",
        "outputId": "c39438b7-c5b6-46cf-83f1-e49a5e2fa3c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretreating: 100% 13593/13593 [08:04<00:00, 28.08folder/s]\n"
          ]
        }
      ],
      "source": [
        "!python \"/content/OpenGait/datasets/pretreatment.py\" --input_path \"/content/GaitDatasetB/GaitDatasetB-silh\" --output_path CASIA-B-pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb-TXUMBd0vF",
        "outputId": "8b1def2c-3e9d-4a80-9559-1979b9e9643e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Collecting kornia\n",
            "  Downloading kornia-0.7.4-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.67.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.26.4)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\n",
            "Collecting kornia-rs>=0.1.0 (from kornia)\n",
            "  Downloading kornia_rs-0.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (24.2)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.5.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9.1->kornia) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Downloading kornia-0.7.4-py2.py3-none-any.whl (899 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.4/899.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kornia-rs, kornia\n",
            "Successfully installed kornia-0.7.4 kornia-rs-0.1.7\n"
          ]
        }
      ],
      "source": [
        "pip install tqdm pyyaml tensorboard opencv-python kornia einops torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBUB8E-Gd346"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import os.path as osp\n",
        "import json\n",
        "import copy\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import torch.utils.data as tordata\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision.models.resnet import ResNet, BasicBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBeK28oYd9Ax"
      },
      "outputs": [],
      "source": [
        "class DataSet(tordata.Dataset):\n",
        "    def __init__(self, data_set):\n",
        "        \"\"\"\n",
        "            seqs_info: the list with each element indicating\n",
        "                            a certain gait sequence presented as [label, type, view, paths];\n",
        "        \"\"\"\n",
        "        self.data_set = data_set\n",
        "        self.__dataset_parser()\n",
        "        self.label_list = [seq_info[0] for seq_info in self.seqs_info]\n",
        "        self.types_list = [seq_info[1] for seq_info in self.seqs_info]\n",
        "        self.views_list = [seq_info[2] for seq_info in self.seqs_info]\n",
        "\n",
        "        self.label_set = sorted(list(set(self.label_list)))\n",
        "        self.types_set = sorted(list(set(self.types_list)))\n",
        "        self.views_set = sorted(list(set(self.views_list)))\n",
        "        self.seqs_data = [None] * len(self)\n",
        "        self.indices_dict = {label: [] for label in self.label_set}\n",
        "        for i, seq_info in enumerate(self.seqs_info):\n",
        "            self.indices_dict[seq_info[0]].append(i)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs_info)\n",
        "\n",
        "    def __loader__(self, pth):\n",
        "        if pth.endswith('.pkl'):\n",
        "            with open(pth, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            f.close()\n",
        "        return data\n",
        "    def __getitem__(self, idx):\n",
        "        if self.seqs_data[idx] is None:\n",
        "            data = self.__loader__(self.seqs_info[idx][-1])\n",
        "            self.seqs_data[idx] = data\n",
        "        else:\n",
        "            data = self.seqs_data[idx]\n",
        "        seq_info = self.seqs_info[idx]\n",
        "        return data, seq_info\n",
        "\n",
        "    def __load_all_data(self):\n",
        "        for idx in range(len(self)):\n",
        "            self.__getitem__(idx)\n",
        "\n",
        "    def __dataset_parser(self):\n",
        "        dataset_root = '/content/CASIA-B-pkl'\n",
        "\n",
        "\n",
        "        def get_seqs_info_list(label_set):\n",
        "            seqs_info_list = []\n",
        "            for lab in label_set:\n",
        "                for typ in sorted(os.listdir(osp.join(dataset_root, lab))):\n",
        "                    for vie in sorted(os.listdir(osp.join(dataset_root, lab, typ))):\n",
        "                        seq_info = [lab, typ, vie]\n",
        "                        seq_path = osp.join(dataset_root, *seq_info)\n",
        "                        dir = os.listdir(seq_path)\n",
        "                        flag = False\n",
        "                        for file in dir:\n",
        "                            if file.endswith(\".pkl\"):\n",
        "                                pth = osp.join(seq_path, file)\n",
        "                                seqs_info_list.append([*seq_info, pth])\n",
        "                                flag = True\n",
        "                                break;\n",
        "                        if not flag:\n",
        "                            print(\n",
        "                                'Find no .pkl file in %s-%s-%s.' % (lab, typ, vie))\n",
        "            return seqs_info_list\n",
        "\n",
        "        self.seqs_info = get_seqs_info_list(self.data_set) #if training else get_seqs_info_list(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SIi_MeXd_CN"
      },
      "outputs": [],
      "source": [
        "class CollateFn(object):\n",
        "    def __init__(self, label_set):\n",
        "        self.label_set = label_set\n",
        "        self.frames_num = 30\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # list of (data, seq_info), data if array of pkl files in a view, seq_info is => (lab, type, view, paths-(pkl files))\n",
        "        batch_size = len(batch)\n",
        "        seqs_batch, labs_batch, typs_batch, vies_batch = [], [], [], []\n",
        "        for bt in batch:\n",
        "            seq_len = len(bt[0])\n",
        "            replace= seq_len < self.frames_num\n",
        "            indices = list(range(seq_len))\n",
        "            indices = np.random.choice(indices, size=self.frames_num, replace=replace)\n",
        "            # numpy.ndarray with numpy.array() before converting to a tensor\n",
        "            seq_numpy = np.array([bt[0][i] for i in indices])\n",
        "            seq_tensor = torch.tensor(seq_numpy)\n",
        "            seqs_batch.append(seq_tensor)\n",
        "            lab_numpy = np.array([self.label_set.index(bt[1][0])])\n",
        "            lab_tensor = torch.tensor(lab_numpy)\n",
        "            labs_batch.append(lab_tensor)\n",
        "            typs_batch.append(bt[1][1])\n",
        "            vies_batch.append(bt[1][2])\n",
        "        seqs_batch = torch.stack(seqs_batch).float()\n",
        "        seqs_batch = seqs_batch.unsqueeze(2)\n",
        "        seqs_batch = seqs_batch / 255.0\n",
        "\n",
        "        labs_batch = torch.stack(labs_batch).squeeze(1).long()\n",
        "\n",
        "        batch = [seqs_batch, labs_batch, typs_batch, vies_batch]\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-R9RM1xeD_H"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch.utils.data as tordata\n",
        "class TripletSampler(tordata.sampler.Sampler):\n",
        "    def __init__(self, dataset, batch_size, batch_shuffle=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        if len(self.batch_size) != 2:\n",
        "            raise ValueError(\n",
        "                \"batch_size should be (P x K) not {}\".format(batch_size))\n",
        "        self.batch_shuffle = batch_shuffle\n",
        "        self.world_size = 1\n",
        "        self.rank = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            sample_indices = []\n",
        "            pid_list = random_sample_list(\n",
        "                self.dataset.label_set, self.batch_size[0])\n",
        "\n",
        "            for pid in pid_list:\n",
        "                indices = self.dataset.indices_dict[pid]\n",
        "                indices = random_sample_list(\n",
        "                    indices, k=self.batch_size[1])\n",
        "                sample_indices += indices\n",
        "\n",
        "            if self.batch_shuffle:\n",
        "                sample_indices = random_sample_list(\n",
        "                    sample_indices, len(sample_indices))\n",
        "\n",
        "            total_batch_size = self.batch_size[0] * self.batch_size[1]\n",
        "            sample_indices += sample_indices[:(total_batch_size - len(sample_indices))]\n",
        "            yield sample_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "def random_sample_list(obj_list, k):\n",
        "    if len(obj_list) < k:\n",
        "        idx = random.choices(range(len(obj_list)), k=k)\n",
        "    else:\n",
        "        idx = random.sample(range(len(obj_list)), k=k)\n",
        "    return [obj_list[i] for i in idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-hCSXaweH5p"
      },
      "outputs": [],
      "source": [
        "with open('/content/OpenGait/datasets/CASIA-B/CASIA-B.json', \"rb\") as f:\n",
        "    partition = json.load(f)\n",
        "train_set = partition[\"TRAIN_SET\"]\n",
        "test_set = partition[\"TEST_SET\"]\n",
        "label_list = os.listdir('/content/CASIA-B-pkl')\n",
        "train_set = [label for label in train_set if label in label_list]\n",
        "test_set = [label for label in test_set if label in label_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi4IMVtGeL8g"
      },
      "outputs": [],
      "source": [
        "dataset = DataSet(data_set=train_set)\n",
        "sampler = TripletSampler(dataset, batch_size=[8, 16])\n",
        "collate_fn = CollateFn(dataset.label_set)\n",
        "train_loader = tordata.DataLoader(dataset, batch_sampler=sampler, collate_fn=collate_fn, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53IizD2c3BWP",
        "outputId": "6f5acf61-2e62-4a79-bbdd-4894a4d3b0fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 30, 1, 64, 64])\n",
            "torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "for batch in train_loader:\n",
        "  seqs, labs, types, views = batch\n",
        "  print(seqs.shape) # (n, s, c, h, w)\n",
        "  print(labs.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qa7rVfO3BvP"
      },
      "source": [
        "# Create Your Own Model Here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXv-IUyY36KJ"
      },
      "outputs": [],
      "source": [
        "class HorizontalPoolingPyramid():\n",
        "    \"\"\"\n",
        "        Horizontal Pyramid Matching for Person Re-identification\n",
        "        Arxiv: https://arxiv.org/abs/1804.05275\n",
        "        Github: https://github.com/SHI-Labs/Horizontal-Pyramid-Matching\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.bin_num = [16]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "            x  : [n, c, h, w]\n",
        "            ret: [n, c, p]\n",
        "        \"\"\"\n",
        "        n, c = x.size()[:2]\n",
        "        features = []\n",
        "        for b in self.bin_num:\n",
        "            z = x.view(n, c, b, -1)\n",
        "            z = z.mean(-1) + z.max(-1)[0]\n",
        "            features.append(z)\n",
        "        return torch.cat(features, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDw993KNePEw"
      },
      "outputs": [],
      "source": [
        "class CustomModel(ResNet):\n",
        "    def __init__(self):\n",
        "      super(CustomModel, self).__init__(BasicBlock, [1, 1, 1, 1])\n",
        "      layers = [1, 1, 1, 1]\n",
        "      self.inplanes = 64\n",
        "      self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "      self.layer1 = self._make_layer(BasicBlock, 64, layers[0])\n",
        "      self.layer2 = self._make_layer(BasicBlock, 128, layers[1], dilate=False)\n",
        "      self.layer3 = self._make_layer(BasicBlock, 256, layers[2], dilate=False)\n",
        "      self.layer4 = self._make_layer(BasicBlock, 512, layers[3], dilate=False)\n",
        "      self.hpp = HorizontalPoolingPyramid()\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # x = torch.Size([128*30, 1, 64, 64]) (n*s, c, h, w)\n",
        "        x = self.conv1(x) # torch.Size([3840, 64, 32, 32]) (n*s, c=64, h, w)\n",
        "        x = self.bn1(x) # torch.Size([3840, 64, 32, 32])\n",
        "        x = self.relu(x) # torch.Size([3840, 64, 32, 32])\n",
        "        x = self.maxpool(x) # torch.Size([3840, 64, 16, 16])\n",
        "\n",
        "        x = self.layer1(x) # torch.Size([3840, 64, 16, 16])\n",
        "        x = self.layer2(x) # torch.Size([3840, 128, 16, 16])\n",
        "        x = self.layer3(x) # torch.Size([3840, 256, 16, 16])\n",
        "        x = self.layer4(x) # torch.Size([3840, 512, 16, 16]) (n*s, c=512, h, w)\n",
        "        x = x.reshape(128, 30, 512, 16, 16).transpose(1, 2).contiguous() # (n, c, s, h, w) = torch.Size([64, 512, 30, 16, 16]),\n",
        "\n",
        "        x = torch.max(x, dim=2)[0] # (n, c, h, w) = torch.Size([128, 512, 16, 16]) butun pikseller için framelerdeki max nokta\n",
        "\n",
        "\n",
        "        x = self.hpp(x) # # [n, c=512, p=16]\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLv-g9QJenqc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Kd4hMl4ent1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImiuWkH3eoNX"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laj-m2ChepP7"
      },
      "outputs": [],
      "source": [
        "# Iterative Triplet Loss Implementation\n",
        "class TripletLossIterative(nn.Module):\n",
        "    def __init__(self, margin=0.2):\n",
        "        super(TripletLossIterative, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, embeddings, labels):\n",
        "        n, c, p = embeddings.size()\n",
        "        embeddings = embeddings.permute(2, 0, 1).contiguous().float()  # [p, n, c]\n",
        "        device = embeddings.device\n",
        "        total_loss = 0.0\n",
        "        total_triplets = 0\n",
        "\n",
        "        for part in range(p):  # Loop over each part\n",
        "            emb_part = embeddings[part]  # [n, c]\n",
        "            dist_mat = torch.cdist(emb_part, emb_part, p=2)  # [n, n]\n",
        "\n",
        "            for i in range(n):  # Loop over each anchor\n",
        "                anchor_label = labels[i]\n",
        "                anchor_dist = dist_mat[i]  # Distances from anchor to all samples\n",
        "\n",
        "                # Positive and negative indices\n",
        "                pos_indices = torch.where((labels == anchor_label) & (torch.arange(n).to(device) != i))[0]\n",
        "                neg_indices = torch.where(labels != anchor_label)[0]\n",
        "\n",
        "                if len(pos_indices) == 0 or len(neg_indices) == 0:\n",
        "                    continue  # Skip if no valid positives or negatives\n",
        "\n",
        "                ap_distances = anchor_dist[pos_indices]  # Distances to positives\n",
        "                an_distances = anchor_dist[neg_indices]  # Distances to negatives\n",
        "\n",
        "                # Compute losses for all combinations of positives and negatives\n",
        "                ap_expanded = ap_distances.unsqueeze(1)  # [num_pos, 1]\n",
        "                an_expanded = an_distances.unsqueeze(0)  # [1, num_neg]\n",
        "                loss_values = F.relu(ap_expanded - an_expanded + self.margin)  # [num_pos, num_neg]\n",
        "\n",
        "                total_loss += loss_values.sum()\n",
        "                total_triplets += loss_values.numel()\n",
        "\n",
        "        if total_triplets > 0:\n",
        "            triplet_loss = total_loss / total_triplets\n",
        "        else:\n",
        "            triplet_loss = torch.tensor(0.0, requires_grad=True).to(device)\n",
        "        return triplet_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVQsJei94WDz"
      },
      "source": [
        "# Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfCDgrXa5E2V"
      },
      "outputs": [],
      "source": [
        "test_dataset = DataSet(data_set=test_set)\n",
        "test_collate_fn = CollateFn(test_dataset.label_set)\n",
        "test_loader = tordata.DataLoader(test_dataset, collate_fn=test_collate_fn, num_workers=2, drop_last=True, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh6VDB1ASEGv"
      },
      "outputs": [],
      "source": [
        "def eval(probe_seq, gallery_seq, features, labels, views, seq_types):\n",
        "    import numpy as np\n",
        "    from scipy.spatial.distance import cdist\n",
        "\n",
        "    # Get a list of unique views present in the dataset\n",
        "    view_list = np.unique(views)\n",
        "    acc_list = []\n",
        "    print(f\"features: {features.shape}\")\n",
        "    print(f\"labels: {labels.shape}\")\n",
        "    print(f\"views: {views.shape}\")\n",
        "    print(f\"seq_types: {seq_types.shape}\")\n",
        "    # Loop over each view in the view list\n",
        "    for probe_view in view_list:\n",
        "        print(f\"probe_view: {probe_view}\")\n",
        "        # Create a mask for the probe samples with the specified sequence types and view\n",
        "        probe_mask = np.isin(seq_types, probe_seq) & (views == probe_view)\n",
        "        probe_features = features[probe_mask]\n",
        "        probe_labels = labels[probe_mask]\n",
        "        print(f\"probe_features: {probe_features.shape}\")\n",
        "        print(f\"probe_labels: {probe_labels.shape}\")\n",
        "\n",
        "        # Create a mask for the gallery samples with the specified sequence types\n",
        "        gallery_mask = np.isin(seq_types, gallery_seq)\n",
        "        gallery_features = features[gallery_mask]\n",
        "        gallery_labels = labels[gallery_mask]\n",
        "        gallery_views = views[gallery_mask]\n",
        "        print(f\"gallery_features: {gallery_features.shape}\")\n",
        "        print(f\"gallery_labels: {gallery_labels.shape}\")\n",
        "        print(f\"gallery_views: {gallery_views.shape}\")\n",
        "\n",
        "        # Exclude gallery samples that have the same view as the probe view\n",
        "        non_identical_view_mask = gallery_views != probe_view\n",
        "        gallery_features = gallery_features[non_identical_view_mask]\n",
        "        gallery_labels = gallery_labels[non_identical_view_mask]\n",
        "\n",
        "        # Proceed only if there are probe samples for this view\n",
        "        if len(probe_features) == 0:\n",
        "            continue\n",
        "\n",
        "        # Compute pairwise distances between probe features and gallery features\n",
        "        # Using Euclidean distance (L2 norm)\n",
        "        print(f\"probe_features: {probe_features.shape}\")\n",
        "        print(f\"gallery_features: {gallery_features.shape}\")\n",
        "        distances = cdist(probe_features, gallery_features, metric='euclidean')  # Shape: [num_probe_samples, num_gallery_samples]\n",
        "        print(f\"distances: {distances.shape}\")\n",
        "\n",
        "        # For each probe sample, find the index of the gallery sample with the minimum distance (nearest neighbor)\n",
        "        min_indices = np.argmin(distances, axis=1)  # Shape: [num_probe_samples]\n",
        "\n",
        "        print(f\"min_indices: {min_indices.shape}\")\n",
        "        # Get the predicted labels from the gallery labels using the indices of the nearest neighbors\n",
        "        predicted_labels = gallery_labels[min_indices]  # Shape: [num_probe_samples]\n",
        "\n",
        "        # Compare the predicted labels with the true probe labels\n",
        "        correct = (predicted_labels == probe_labels)\n",
        "\n",
        "        # Compute accuracy for this probe view\n",
        "        acc = np.mean(correct)  # Fraction of correct predictions\n",
        "\n",
        "        # Store the accuracy in the list\n",
        "        acc_list.append(acc)\n",
        "\n",
        "    # Compute the mean accuracy over all views\n",
        "    if len(acc_list) > 0:\n",
        "        mean_acc = np.mean(acc_list)\n",
        "    else:\n",
        "        mean_acc = 0.0  # Handle the case where acc_list is empty\n",
        "\n",
        "    return mean_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1v3Ko17tFOm"
      },
      "outputs": [],
      "source": [
        "def eval(probe_seq, gallery_seq, features, labels, views, seq_types):\n",
        "\n",
        "    view_list = np.unique(views)\n",
        "    acc_list = []\n",
        "\n",
        "    for probe_view in view_list:\n",
        "        # Create masks for probe and gallery samples\n",
        "        probe_mask = np.isin(seq_types, probe_seq) & (views == probe_view)\n",
        "        gallery_mask = np.isin(seq_types, gallery_seq)\n",
        "\n",
        "        probe_features = features[probe_mask]  # [num_probe_samples, feature_dim, num_parts]\n",
        "        probe_labels = labels[probe_mask]\n",
        "\n",
        "        gallery_features = features[gallery_mask]\n",
        "        gallery_labels = labels[gallery_mask]\n",
        "        gallery_views = views[gallery_mask]\n",
        "\n",
        "        # Exclude gallery samples with the same view as the probe\n",
        "        non_identical_view_mask = gallery_views != probe_view\n",
        "        gallery_features = gallery_features[non_identical_view_mask]\n",
        "        gallery_labels = gallery_labels[non_identical_view_mask]\n",
        "\n",
        "        if len(probe_features) == 0:\n",
        "            continue  # Skip if no probe samples for this view\n",
        "\n",
        "        num_parts = probe_features.shape[2]\n",
        "        distances_per_part = []\n",
        "\n",
        "        for part in range(num_parts):\n",
        "            # Extract features for the current part\n",
        "            probe_part_features = probe_features[:, :, part]  # [num_probe_samples, feature_dim]\n",
        "            gallery_part_features = gallery_features[:, :, part]  # [num_gallery_samples, feature_dim]\n",
        "\n",
        "            # Compute distances for this part\n",
        "            distances_part = cdist(probe_part_features, gallery_part_features, metric='euclidean')  # [num_probe_samples, num_gallery_samples]\n",
        "            distances_per_part.append(distances_part)\n",
        "\n",
        "        # Aggregate distances over parts (e.g., mean)\n",
        "        distances = np.mean(distances_per_part, axis=0)  # [num_probe_samples, num_gallery_samples]\n",
        "\n",
        "        # Find nearest neighbors\n",
        "        min_indices = np.argmin(distances, axis=1)  # [num_probe_samples]\n",
        "        predicted_labels = gallery_labels[min_indices]  # [num_probe_samples]\n",
        "\n",
        "        # Compute accuracy\n",
        "        correct = (predicted_labels == probe_labels)\n",
        "        acc = np.mean(np.array(correct))\n",
        "        acc_list.append(acc)\n",
        "\n",
        "    # Compute mean accuracy over all views\n",
        "    mean_acc = np.mean(acc_list) if acc_list else 0.0\n",
        "    return mean_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hLquXp0-4UEd",
        "outputId": "61112178-362f-451d-8347-8e22ce6096ab"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-20-a3d2c28789b4>:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-20-a3d2c28789b4>:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean loss for 10 iterations: 0.9347247004508972\n",
            "Mean loss for 20 iterations: 0.5847357340157032\n",
            "Accuracy Bag: 0.20694667347499054\n",
            "Accuracy NM: 0.4109461966604824\n",
            "Accuracy CL: 0.10946196660482373\n",
            "Mean loss for 30 iterations: 0.45373559445142747\n",
            "Mean loss for 40 iterations: 0.3803983472287655\n",
            "Accuracy Bag: 0.46045704641232826\n",
            "Accuracy NM: 0.7448979591836735\n",
            "Accuracy CL: 0.17996289424860853\n",
            "Mean loss for 50 iterations: 0.3353702300786972\n",
            "Mean loss for 60 iterations: 0.3033991670856873\n",
            "Accuracy Bag: 0.5858519205018067\n",
            "Accuracy NM: 0.8441558441558442\n",
            "Accuracy CL: 0.26901669758812613\n",
            "Mean loss for 70 iterations: 0.27917078584432603\n",
            "Mean loss for 80 iterations: 0.25936577739194033\n",
            "Accuracy Bag: 0.6433668011523146\n",
            "Accuracy NM: 0.8506493506493507\n",
            "Accuracy CL: 0.2458256029684601\n",
            "Mean loss for 90 iterations: 0.24495514473981328\n",
            "Mean loss for 100 iterations: 0.23254361547529698\n",
            "Accuracy Bag: 0.6128014069505568\n",
            "Accuracy NM: 0.8803339517625233\n",
            "Accuracy CL: 0.2606679035250463\n",
            "Mean loss for 110 iterations: 0.2222847776656801\n",
            "Mean loss for 120 iterations: 0.21385477017611265\n",
            "Accuracy Bag: 0.6554829707862242\n",
            "Accuracy NM: 0.8775510204081631\n",
            "Accuracy CL: 0.26994434137291284\n",
            "Mean loss for 130 iterations: 0.2065403839143423\n",
            "Mean loss for 140 iterations: 0.19962161795369215\n",
            "Accuracy Bag: 0.6481375544168421\n",
            "Accuracy NM: 0.8589981447124303\n",
            "Accuracy CL: 0.19109461966604827\n",
            "Mean loss for 150 iterations: 0.19335832208395004\n",
            "Mean loss for 160 iterations: 0.18794215749949217\n",
            "Accuracy Bag: 0.6510065558130894\n",
            "Accuracy NM: 0.8738404452690168\n",
            "Accuracy CL: 0.22077922077922077\n",
            "Mean loss for 170 iterations: 0.18306698154877213\n",
            "Mean loss for 180 iterations: 0.17894755088620715\n",
            "Accuracy Bag: 0.6592796159672581\n",
            "Accuracy NM: 0.8710575139146569\n",
            "Accuracy CL: 0.2235621521335807\n",
            "Mean loss for 190 iterations: 0.17497645526340133\n",
            "Mean loss for 200 iterations: 0.17089016865938902\n",
            "Accuracy Bag: 0.7047829441054773\n",
            "Accuracy NM: 0.9007421150278293\n",
            "Accuracy CL: 0.2541743970315399\n",
            "Mean loss for 210 iterations: 0.16744130370872362\n",
            "Mean loss for 220 iterations: 0.16438166197728027\n",
            "Accuracy Bag: 0.7363790536236648\n",
            "Accuracy NM: 0.9090909090909093\n",
            "Accuracy CL: 0.3172541743970316\n",
            "Mean loss for 230 iterations: 0.16141398053454317\n",
            "Mean loss for 240 iterations: 0.15872186596194904\n",
            "Accuracy Bag: 0.7084749721648905\n",
            "Accuracy NM: 0.9137291280148422\n",
            "Accuracy CL: 0.29035250463821893\n",
            "Mean loss for 250 iterations: 0.15598402413725854\n",
            "Mean loss for 260 iterations: 0.15345053687118568\n",
            "Accuracy Bag: 0.7382439045312256\n",
            "Accuracy NM: 0.9053803339517625\n",
            "Accuracy CL: 0.3376623376623377\n",
            "Mean loss for 270 iterations: 0.15137838334948928\n",
            "Mean loss for 280 iterations: 0.14920460823923348\n",
            "Accuracy Bag: 0.7252482941999611\n",
            "Accuracy NM: 0.9044526901669759\n",
            "Accuracy CL: 0.2727272727272727\n",
            "Mean loss for 290 iterations: 0.14706379087834523\n",
            "Mean loss for 300 iterations: 0.1451052081833283\n",
            "Accuracy Bag: 0.729914623541615\n",
            "Accuracy NM: 0.9137291280148422\n",
            "Accuracy CL: 0.3311688311688312\n",
            "Mean loss for 310 iterations: 0.14317241296652825\n",
            "Mean loss for 320 iterations: 0.14145564795471727\n",
            "Accuracy Bag: 0.7400142812513948\n",
            "Accuracy NM: 0.896103896103896\n",
            "Accuracy CL: 0.30055658627087195\n",
            "Mean loss for 330 iterations: 0.139696685902097\n",
            "Mean loss for 340 iterations: 0.1379528589108411\n",
            "Accuracy Bag: 0.7568166796979221\n",
            "Accuracy NM: 0.9137291280148422\n",
            "Accuracy CL: 0.33116883116883117\n",
            "Mean loss for 350 iterations: 0.13623523917581354\n",
            "Mean loss for 360 iterations: 0.13452918026596308\n",
            "Accuracy Bag: 0.7791377037594738\n",
            "Accuracy NM: 0.914656771799629\n",
            "Accuracy CL: 0.3896103896103896\n",
            "Mean loss for 370 iterations: 0.1331217372135536\n",
            "Mean loss for 380 iterations: 0.131677645866416\n",
            "Accuracy Bag: 0.7494527162488068\n",
            "Accuracy NM: 0.901669758812616\n",
            "Accuracy CL: 0.3589981447124304\n",
            "Mean loss for 390 iterations: 0.13026323951780797\n",
            "Mean loss for 400 iterations: 0.12865052781999112\n",
            "Accuracy Bag: 0.7744801649608184\n",
            "Accuracy NM: 0.9267161410018553\n",
            "Accuracy CL: 0.4025974025974026\n",
            "Mean loss for 410 iterations: 0.12743392167840062\n",
            "Mean loss for 420 iterations: 0.1261895285564519\n",
            "Accuracy Bag: 0.7437923793526953\n",
            "Accuracy NM: 0.9183673469387755\n",
            "Accuracy CL: 0.43506493506493504\n",
            "Mean loss for 430 iterations: 0.12498895622963128\n",
            "Mean loss for 440 iterations: 0.12376806117932905\n",
            "Accuracy Bag: 0.7400898219615585\n",
            "Accuracy NM: 0.9025974025974025\n",
            "Accuracy CL: 0.2820037105751391\n",
            "Mean loss for 450 iterations: 0.12273035518825054\n",
            "Mean loss for 460 iterations: 0.12163470730833385\n",
            "Accuracy Bag: 0.7058522991713514\n",
            "Accuracy NM: 0.8905380333951762\n",
            "Accuracy CL: 0.29499072356215206\n",
            "Mean loss for 470 iterations: 0.12054255485851714\n",
            "Mean loss for 480 iterations: 0.11938564493320883\n",
            "Accuracy Bag: 0.727045815344115\n",
            "Accuracy NM: 0.9137291280148424\n",
            "Accuracy CL: 0.33209647495361777\n",
            "Mean loss for 490 iterations: 0.1182386511974797\n",
            "Mean loss for 500 iterations: 0.11739784417301416\n",
            "Accuracy Bag: 0.7669641540976392\n",
            "Accuracy NM: 0.9508348794063081\n",
            "Accuracy CL: 0.3896103896103896\n",
            "Mean loss for 510 iterations: 0.11636541669129156\n",
            "Mean loss for 520 iterations: 0.11531270392812215\n",
            "Accuracy Bag: 0.7716687367912431\n",
            "Accuracy NM: 0.9211502782931354\n",
            "Accuracy CL: 0.383116883116883\n",
            "Mean loss for 530 iterations: 0.11431717682137804\n",
            "Mean loss for 540 iterations: 0.11340995857974998\n",
            "Accuracy Bag: 0.7541479288031705\n",
            "Accuracy NM: 0.9332096474953618\n",
            "Accuracy CL: 0.4461966604823747\n",
            "Mean loss for 550 iterations: 0.11240839904682202\n",
            "Mean loss for 560 iterations: 0.1115522881238056\n",
            "Accuracy Bag: 0.7317131106795052\n",
            "Accuracy NM: 0.8942486085343228\n",
            "Accuracy CL: 0.33024118738404457\n",
            "Mean loss for 570 iterations: 0.1107046011116421\n",
            "Mean loss for 580 iterations: 0.10986621390771249\n",
            "Accuracy Bag: 0.7708636776115689\n",
            "Accuracy NM: 0.914656771799629\n",
            "Accuracy CL: 0.38775510204081626\n",
            "Mean loss for 590 iterations: 0.10912796528536384\n",
            "Mean loss for 600 iterations: 0.1084111520772179\n",
            "Accuracy Bag: 0.7559272892650856\n",
            "Accuracy NM: 0.9397031539888684\n",
            "Accuracy CL: 0.4387755102040817\n",
            "Mean loss for 610 iterations: 0.10773005352401342\n",
            "Mean loss for 620 iterations: 0.10686850983709578\n",
            "Accuracy Bag: 0.7837097521124835\n",
            "Accuracy NM: 0.9220779220779222\n",
            "Accuracy CL: 0.49072356215213353\n",
            "Mean loss for 630 iterations: 0.10605577731593734\n",
            "Mean loss for 640 iterations: 0.1053840290143853\n",
            "Accuracy Bag: 0.7930053166363247\n",
            "Accuracy NM: 0.9341372912801483\n",
            "Accuracy CL: 0.4192949907235622\n",
            "Mean loss for 650 iterations: 0.10470478761368072\n",
            "Mean loss for 660 iterations: 0.10403387364508077\n",
            "Accuracy Bag: 0.7828480856998731\n",
            "Accuracy NM: 0.9174397031539889\n",
            "Accuracy CL: 0.4128014842300557\n",
            "Mean loss for 670 iterations: 0.10331466755371041\n",
            "Mean loss for 680 iterations: 0.10262500974897515\n",
            "Accuracy Bag: 0.7828107983416593\n",
            "Accuracy NM: 0.9267161410018553\n",
            "Accuracy CL: 0.4174397031539888\n",
            "Mean loss for 690 iterations: 0.10202103423197632\n",
            "Mean loss for 700 iterations: 0.10136876118768538\n",
            "Accuracy Bag: 0.8098457752360549\n",
            "Accuracy NM: 0.9294990723562152\n",
            "Accuracy CL: 0.4907235621521336\n",
            "Mean loss for 710 iterations: 0.10071026228735565\n",
            "Mean loss for 720 iterations: 0.10012794626721491\n",
            "Accuracy Bag: 0.7159990007760793\n",
            "Accuracy NM: 0.901669758812616\n",
            "Accuracy CL: 0.41558441558441556\n",
            "Mean loss for 730 iterations: 0.09945218902183314\n",
            "Mean loss for 740 iterations: 0.09893417382814192\n",
            "Accuracy Bag: 0.806106510082946\n",
            "Accuracy NM: 0.9350649350649352\n",
            "Accuracy CL: 0.4860853432282004\n",
            "Mean loss for 750 iterations: 0.0984515807852149\n",
            "Mean loss for 760 iterations: 0.09780656008883135\n",
            "Accuracy Bag: 0.7327447919896709\n",
            "Accuracy NM: 0.9025974025974026\n",
            "Accuracy CL: 0.33766233766233766\n",
            "Mean loss for 770 iterations: 0.09717344232629259\n",
            "Mean loss for 780 iterations: 0.09663853588251349\n",
            "Accuracy Bag: 0.8144357910725564\n",
            "Accuracy NM: 0.9257884972170687\n",
            "Accuracy CL: 0.4907235621521336\n",
            "Mean loss for 790 iterations: 0.09606071857240381\n",
            "Mean loss for 800 iterations: 0.09552048640325665\n",
            "Accuracy Bag: 0.7355286893377669\n",
            "Accuracy NM: 0.9072356215213359\n",
            "Accuracy CL: 0.3116883116883117\n",
            "Mean loss for 810 iterations: 0.09502840272990273\n",
            "Mean loss for 820 iterations: 0.09455285074598178\n",
            "Accuracy Bag: 0.7772252293607227\n",
            "Accuracy NM: 0.9257884972170687\n",
            "Accuracy CL: 0.4777365491651205\n",
            "Mean loss for 830 iterations: 0.0940493594540889\n",
            "Mean loss for 840 iterations: 0.09359721381305938\n",
            "Accuracy Bag: 0.7922293338681076\n",
            "Accuracy NM: 0.9192949907235622\n",
            "Accuracy CL: 0.4332096474953618\n",
            "Mean loss for 850 iterations: 0.09311248153886374\n",
            "Mean loss for 860 iterations: 0.09266913304495257\n",
            "Accuracy Bag: 0.7967637084654089\n",
            "Accuracy NM: 0.932282003710575\n",
            "Accuracy CL: 0.4304267161410019\n",
            "Mean loss for 870 iterations: 0.09221374496774769\n",
            "Mean loss for 880 iterations: 0.09170890026937493\n",
            "Accuracy Bag: 0.8125902600397179\n",
            "Accuracy NM: 0.9276437847866421\n",
            "Accuracy CL: 0.4768089053803339\n",
            "Mean loss for 890 iterations: 0.09120505411345302\n",
            "Mean loss for 900 iterations: 0.09074664150054255\n",
            "Accuracy Bag: 0.7624112324206045\n",
            "Accuracy NM: 0.9174397031539889\n",
            "Accuracy CL: 0.42671614100185523\n",
            "Mean loss for 910 iterations: 0.09030003630566401\n",
            "Mean loss for 920 iterations: 0.08983510083356953\n",
            "Accuracy Bag: 0.7094485006521424\n",
            "Accuracy NM: 0.8877551020408163\n",
            "Accuracy CL: 0.2606679035250463\n",
            "Mean loss for 930 iterations: 0.08944931740682292\n",
            "Mean loss for 940 iterations: 0.08897489508455421\n",
            "Accuracy Bag: 0.780917257420136\n",
            "Accuracy NM: 0.9452690166975883\n",
            "Accuracy CL: 0.4888682745825603\n",
            "Mean loss for 950 iterations: 0.08850234984371223\n",
            "Mean loss for 960 iterations: 0.08804382982780226\n",
            "Accuracy Bag: 0.7808606501872\n",
            "Accuracy NM: 0.9350649350649349\n",
            "Accuracy CL: 0.4109461966604824\n",
            "Mean loss for 970 iterations: 0.08758373602879109\n",
            "Mean loss for 980 iterations: 0.08715518934043999\n",
            "Accuracy Bag: 0.8088230776676352\n",
            "Accuracy NM: 0.9452690166975881\n",
            "Accuracy CL: 0.5194805194805194\n",
            "Mean loss for 990 iterations: 0.08671239351646767\n",
            "Mean loss for 1000 iterations: 0.08622581649757921\n",
            "Accuracy Bag: 0.8163385089345726\n",
            "Accuracy NM: 0.9294990723562152\n",
            "Accuracy CL: 0.5213358070500926\n",
            "Mean loss for 1010 iterations: 0.08579796526839237\n",
            "Mean loss for 1020 iterations: 0.08535082725552368\n",
            "Accuracy Bag: 0.8069586131575689\n",
            "Accuracy NM: 0.9387755102040817\n",
            "Accuracy CL: 0.4814471243042673\n",
            "Mean loss for 1030 iterations: 0.08491076608052821\n",
            "Mean loss for 1040 iterations: 0.08449714010258993\n",
            "Accuracy Bag: 0.7940176780717682\n",
            "Accuracy NM: 0.9248608534322821\n",
            "Accuracy CL: 0.4489795918367347\n",
            "Mean loss for 1050 iterations: 0.08410965857583852\n",
            "Mean loss for 1060 iterations: 0.08371800718793891\n",
            "Accuracy Bag: 0.7819018948353531\n",
            "Accuracy NM: 0.9406307977736549\n",
            "Accuracy CL: 0.4907235621521336\n",
            "Mean loss for 1070 iterations: 0.08331003950682478\n",
            "Mean loss for 1080 iterations: 0.08294369677530118\n",
            "Accuracy Bag: 0.7902787993161536\n",
            "Accuracy NM: 0.9406307977736549\n",
            "Accuracy CL: 0.47959183673469385\n",
            "Mean loss for 1090 iterations: 0.08257726611518258\n",
            "Mean loss for 1100 iterations: 0.08222268763082949\n",
            "Accuracy Bag: 0.7977007223894357\n",
            "Accuracy NM: 0.9267161410018553\n",
            "Accuracy CL: 0.4332096474953618\n",
            "Mean loss for 1110 iterations: 0.08183648614800192\n",
            "Mean loss for 1120 iterations: 0.08147868463836078\n",
            "Accuracy Bag: 0.8003800026159109\n",
            "Accuracy NM: 0.9350649350649349\n",
            "Accuracy CL: 0.45083487940630795\n",
            "Mean loss for 1130 iterations: 0.08108881999598404\n",
            "Mean loss for 1140 iterations: 0.08071917963720728\n",
            "Accuracy Bag: 0.7892478908009768\n",
            "Accuracy NM: 0.9285714285714285\n",
            "Accuracy CL: 0.45547309833024113\n",
            "Mean loss for 1150 iterations: 0.0803426716087953\n",
            "Mean loss for 1160 iterations: 0.07998985472970224\n",
            "Accuracy Bag: 0.8124959790510733\n",
            "Accuracy NM: 0.9471243042671613\n",
            "Accuracy CL: 0.5148423005565862\n",
            "Mean loss for 1170 iterations: 0.07972730933887581\n",
            "Mean loss for 1180 iterations: 0.07937127769119659\n",
            "Accuracy Bag: 0.7354232994211573\n",
            "Accuracy NM: 0.9192949907235621\n",
            "Accuracy CL: 0.29962894248608535\n",
            "Mean loss for 1190 iterations: 0.07904076598615957\n",
            "Mean loss for 1200 iterations: 0.07871499869972468\n",
            "Accuracy Bag: 0.8283410777051159\n",
            "Accuracy NM: 0.9471243042671614\n",
            "Accuracy CL: 0.5528756957328386\n",
            "Mean loss for 1210 iterations: 0.0784384693350058\n",
            "Mean loss for 1220 iterations: 0.07811401702738444\n",
            "Accuracy Bag: 0.7994326525589077\n",
            "Accuracy NM: 0.9257884972170687\n",
            "Accuracy CL: 0.34044526901669764\n",
            "Mean loss for 1230 iterations: 0.07776268899471052\n",
            "Mean loss for 1240 iterations: 0.07745544305822301\n",
            "Accuracy Bag: 0.7930626966642497\n",
            "Accuracy NM: 0.9341372912801483\n",
            "Accuracy CL: 0.5111317254174396\n",
            "Mean loss for 1250 iterations: 0.0770896465063095\n",
            "Mean loss for 1260 iterations: 0.07674546246490781\n",
            "Accuracy Bag: 0.775360378453162\n",
            "Accuracy NM: 0.9341372912801486\n",
            "Accuracy CL: 0.474025974025974\n",
            "Mean loss for 1270 iterations: 0.0763787727304331\n",
            "Mean loss for 1280 iterations: 0.0760919818203547\n",
            "Accuracy Bag: 0.8237036315761714\n",
            "Accuracy NM: 0.9406307977736549\n",
            "Accuracy CL: 0.5333951762523191\n",
            "Mean loss for 1290 iterations: 0.07584908165961735\n",
            "Mean loss for 1300 iterations: 0.07553278645643821\n",
            "Accuracy Bag: 0.7995468330185159\n",
            "Accuracy NM: 0.9397031539888684\n",
            "Accuracy CL: 0.4044526901669759\n",
            "Mean loss for 1310 iterations: 0.0752166579712097\n",
            "Mean loss for 1320 iterations: 0.0749275727705522\n",
            "Accuracy Bag: 0.8264492755723176\n",
            "Accuracy NM: 0.9350649350649349\n",
            "Accuracy CL: 0.5686456400742116\n",
            "Mean loss for 1330 iterations: 0.07460467935700837\n",
            "Mean loss for 1340 iterations: 0.0743132564823018\n",
            "Accuracy Bag: 0.8162340850116991\n",
            "Accuracy NM: 0.9350649350649352\n",
            "Accuracy CL: 0.4517625231910946\n",
            "Mean loss for 1350 iterations: 0.07407024281168426\n",
            "Mean loss for 1360 iterations: 0.07378950400469715\n",
            "Accuracy Bag: 0.8357350835594243\n",
            "Accuracy NM: 0.9471243042671614\n",
            "Accuracy CL: 0.45825602968460116\n",
            "Mean loss for 1370 iterations: 0.0735257164187675\n",
            "Mean loss for 1380 iterations: 0.0732361863920654\n",
            "Accuracy Bag: 0.8125162649195318\n",
            "Accuracy NM: 0.9443413729128014\n",
            "Accuracy CL: 0.5064935064935066\n",
            "Mean loss for 1390 iterations: 0.07300697628730278\n",
            "Mean loss for 1400 iterations: 0.0727793543426586\n",
            "Accuracy Bag: 0.7652216945964436\n",
            "Accuracy NM: 0.9192949907235621\n",
            "Accuracy CL: 0.3395176252319109\n",
            "Mean loss for 1410 iterations: 0.07254071395368653\n",
            "Mean loss for 1420 iterations: 0.07227724213808985\n",
            "Accuracy Bag: 0.7827824947251912\n",
            "Accuracy NM: 0.937847866419295\n",
            "Accuracy CL: 0.461038961038961\n",
            "Mean loss for 1430 iterations: 0.07203121449720193\n",
            "Mean loss for 1440 iterations: 0.07173471500088151\n",
            "Accuracy Bag: 0.8051601260196789\n",
            "Accuracy NM: 0.922077922077922\n",
            "Accuracy CL: 0.4452690166975881\n",
            "Mean loss for 1450 iterations: 0.07143709075219672\n",
            "Mean loss for 1460 iterations: 0.0711871588691967\n",
            "Accuracy Bag: 0.8367005942986662\n",
            "Accuracy NM: 0.9517625231910946\n",
            "Accuracy CL: 0.5807050092764379\n",
            "Mean loss for 1470 iterations: 0.07091956940561939\n",
            "Mean loss for 1480 iterations: 0.0706329578684794\n",
            "Accuracy Bag: 0.7690181465787304\n",
            "Accuracy NM: 0.9220779220779222\n",
            "Accuracy CL: 0.3246753246753247\n",
            "Mean loss for 1490 iterations: 0.07038036842874233\n",
            "Mean loss for 1500 iterations: 0.07013541166484356\n",
            "Accuracy Bag: 0.8032099778652196\n",
            "Accuracy NM: 0.953617810760668\n",
            "Accuracy CL: 0.5037105751391465\n",
            "Mean loss for 1510 iterations: 0.06988670925628271\n",
            "Mean loss for 1520 iterations: 0.06967233205949397\n",
            "Accuracy Bag: 0.8330095322329893\n",
            "Accuracy NM: 0.9387755102040815\n",
            "Accuracy CL: 0.5176252319109463\n",
            "Mean loss for 1530 iterations: 0.06947191162339222\n",
            "Mean loss for 1540 iterations: 0.0692213955577221\n",
            "Accuracy Bag: 0.8246027785457428\n",
            "Accuracy NM: 0.9443413729128017\n",
            "Accuracy CL: 0.5222634508348795\n",
            "Mean loss for 1550 iterations: 0.06901703886807926\n",
            "Mean loss for 1560 iterations: 0.06880844123542118\n",
            "Accuracy Bag: 0.8561914499121621\n",
            "Accuracy NM: 0.9564007421150279\n",
            "Accuracy CL: 0.5807050092764379\n",
            "Mean loss for 1570 iterations: 0.06856646226469879\n",
            "Mean loss for 1580 iterations: 0.068362324432576\n",
            "Accuracy Bag: 0.7920591257718047\n",
            "Accuracy NM: 0.9536178107606678\n",
            "Accuracy CL: 0.4452690166975881\n",
            "Mean loss for 1590 iterations: 0.06812052456720633\n",
            "Mean loss for 1600 iterations: 0.06789672723622062\n",
            "Accuracy Bag: 0.8116538257119327\n",
            "Accuracy NM: 0.9452690166975881\n",
            "Accuracy CL: 0.49814471243042674\n",
            "Mean loss for 1610 iterations: 0.06766295596980346\n",
            "Mean loss for 1620 iterations: 0.06744727584314936\n",
            "Accuracy Bag: 0.7901847115262567\n",
            "Accuracy NM: 0.9499072356215215\n",
            "Accuracy CL: 0.43042671614100186\n",
            "Mean loss for 1630 iterations: 0.06720347719529837\n",
            "Mean loss for 1640 iterations: 0.06698081129026122\n",
            "Accuracy Bag: 0.8023678245260786\n",
            "Accuracy NM: 0.961038961038961\n",
            "Accuracy CL: 0.5491651205936919\n",
            "Mean loss for 1650 iterations: 0.0667238377000798\n",
            "Mean loss for 1660 iterations: 0.0664795434102416\n",
            "Accuracy Bag: 0.8570722430007475\n",
            "Accuracy NM: 0.9573283858998146\n",
            "Accuracy CL: 0.6131725417439703\n",
            "Mean loss for 1670 iterations: 0.06625985870595107\n",
            "Mean loss for 1680 iterations: 0.06601693877795091\n",
            "Accuracy Bag: 0.8087180741485199\n",
            "Accuracy NM: 0.9424860853432283\n",
            "Accuracy CL: 0.47773654916512065\n",
            "Mean loss for 1690 iterations: 0.06579545812802555\n",
            "Mean loss for 1700 iterations: 0.06556050686823095\n",
            "Accuracy Bag: 0.8338696530556217\n",
            "Accuracy NM: 0.9461966604823747\n",
            "Accuracy CL: 0.49999999999999994\n",
            "Mean loss for 1710 iterations: 0.06533708342770386\n",
            "Mean loss for 1720 iterations: 0.0650962790615077\n",
            "Accuracy Bag: 0.8385090311720382\n",
            "Accuracy NM: 0.9628942486085343\n",
            "Accuracy CL: 0.5473098330241188\n",
            "Mean loss for 1730 iterations: 0.06486537822527606\n",
            "Mean loss for 1740 iterations: 0.06462640593031785\n",
            "Accuracy Bag: 0.834769572820182\n",
            "Accuracy NM: 0.9480519480519479\n",
            "Accuracy CL: 0.4749536178107606\n",
            "Mean loss for 1750 iterations: 0.06439237279711026\n",
            "Mean loss for 1760 iterations: 0.06417316990131935\n",
            "Accuracy Bag: 0.8459772253452799\n",
            "Accuracy NM: 0.9573283858998145\n",
            "Accuracy CL: 0.5120593692022264\n",
            "Mean loss for 1770 iterations: 0.06395332832152477\n",
            "Mean loss for 1780 iterations: 0.06370142865612098\n",
            "Accuracy Bag: 0.8570814199412407\n",
            "Accuracy NM: 0.9536178107606678\n",
            "Accuracy CL: 0.5102040816326532\n",
            "Mean loss for 1790 iterations: 0.06348943931451223\n",
            "Mean loss for 1800 iterations: 0.06327740386024945\n",
            "Accuracy Bag: 0.8384510715478716\n",
            "Accuracy NM: 0.9471243042671613\n",
            "Accuracy CL: 0.41187384044526903\n",
            "Mean loss for 1810 iterations: 0.06309165649568838\n",
            "Mean loss for 1820 iterations: 0.06288631814992526\n",
            "Accuracy Bag: 0.8236755211584508\n",
            "Accuracy NM: 0.9415584415584416\n",
            "Accuracy CL: 0.42115027829313545\n",
            "Mean loss for 1830 iterations: 0.0626882478543888\n",
            "Mean loss for 1840 iterations: 0.06252329243428033\n",
            "Accuracy Bag: 0.8487320462819191\n",
            "Accuracy NM: 0.953617810760668\n",
            "Accuracy CL: 0.5769944341372913\n",
            "Mean loss for 1850 iterations: 0.062334697768092154\n",
            "Mean loss for 1860 iterations: 0.06212387915740731\n",
            "Accuracy Bag: 0.8393978420086333\n",
            "Accuracy NM: 0.9619666048237476\n",
            "Accuracy CL: 0.5278293135435992\n",
            "Mean loss for 1870 iterations: 0.06191747712051088\n",
            "Mean loss for 1880 iterations: 0.061711571040622734\n",
            "Accuracy Bag: 0.8449929743275572\n",
            "Accuracy NM: 0.945269016697588\n",
            "Accuracy CL: 0.5556586270871985\n",
            "Mean loss for 1890 iterations: 0.06149430947141751\n",
            "Mean loss for 1900 iterations: 0.06129827720407201\n",
            "Accuracy Bag: 0.8291752132962469\n",
            "Accuracy NM: 0.9545454545454546\n",
            "Accuracy CL: 0.4972170686456401\n",
            "Mean loss for 1910 iterations: 0.061121184300358226\n",
            "Mean loss for 1920 iterations: 0.0609619913273491\n",
            "Accuracy Bag: 0.8561336834867429\n",
            "Accuracy NM: 0.9628942486085343\n",
            "Accuracy CL: 0.5538033395176253\n",
            "Mean loss for 1930 iterations: 0.06077187950940021\n",
            "Mean loss for 1940 iterations: 0.06059811685435935\n",
            "Accuracy Bag: 0.8774706497293191\n",
            "Accuracy NM: 0.9721706864564008\n",
            "Accuracy CL: 0.575139146567718\n",
            "Mean loss for 1950 iterations: 0.06046420056468401\n",
            "Mean loss for 1960 iterations: 0.060317136503147836\n",
            "Accuracy Bag: 0.8571198664919376\n",
            "Accuracy NM: 0.9610389610389608\n",
            "Accuracy CL: 0.5658627087198514\n",
            "Mean loss for 1970 iterations: 0.06014113677098288\n",
            "Mean loss for 1980 iterations: 0.059961903415563886\n",
            "Accuracy Bag: 0.8635833305802513\n",
            "Accuracy NM: 0.9721706864564008\n",
            "Accuracy CL: 0.6094619666048238\n",
            "Mean loss for 1990 iterations: 0.05978520811680498\n",
            "Mean loss for 2000 iterations: 0.05959578874381259\n",
            "Accuracy Bag: 0.8356972166049688\n",
            "Accuracy NM: 0.9564007421150277\n",
            "Accuracy CL: 0.5185528756957328\n",
            "Mean loss for 2010 iterations: 0.05941371736695888\n",
            "Mean loss for 2020 iterations: 0.059241928700542096\n",
            "Accuracy Bag: 0.8794395381622588\n",
            "Accuracy NM: 0.9675324675324675\n",
            "Accuracy CL: 0.6326530612244898\n",
            "Mean loss for 2030 iterations: 0.059084190106193715\n",
            "Mean loss for 2040 iterations: 0.05889079038912029\n",
            "Accuracy Bag: 0.8542510582944375\n",
            "Accuracy NM: 0.9619666048237476\n",
            "Accuracy CL: 0.6187384044526901\n",
            "Mean loss for 2050 iterations: 0.05870613236116563\n",
            "Mean loss for 2060 iterations: 0.05853495268530927\n",
            "Accuracy Bag: 0.8775182732205092\n",
            "Accuracy NM: 0.9554730983302413\n",
            "Accuracy CL: 0.5983302411873841\n",
            "Mean loss for 2070 iterations: 0.058356120573257765\n",
            "Mean loss for 2080 iterations: 0.058172035858017175\n",
            "Accuracy Bag: 0.8365485468846026\n",
            "Accuracy NM: 0.9554730983302412\n",
            "Accuracy CL: 0.44155844155844154\n",
            "Mean loss for 2090 iterations: 0.057996424827635576\n",
            "Mean loss for 2100 iterations: 0.0578385488511551\n",
            "Accuracy Bag: 0.8599213449260311\n",
            "Accuracy NM: 0.9684601113172541\n",
            "Accuracy CL: 0.6159554730983303\n",
            "Mean loss for 2110 iterations: 0.05767082610383842\n",
            "Mean loss for 2120 iterations: 0.057489534467019425\n",
            "Accuracy Bag: 0.8023012675576606\n",
            "Accuracy NM: 0.9675324675324675\n",
            "Accuracy CL: 0.5380333951762523\n",
            "Mean loss for 2130 iterations: 0.057334333929307583\n",
            "Mean loss for 2140 iterations: 0.0571846009545446\n",
            "Accuracy Bag: 0.8766101425091919\n",
            "Accuracy NM: 0.9675324675324675\n",
            "Accuracy CL: 0.6094619666048238\n",
            "Mean loss for 2150 iterations: 0.05701493874755363\n",
            "Mean loss for 2160 iterations: 0.05689131679849837\n",
            "Accuracy Bag: 0.8543265990046013\n",
            "Accuracy NM: 0.963821892393321\n",
            "Accuracy CL: 0.546382189239332\n",
            "Mean loss for 2170 iterations: 0.05673884548427117\n",
            "Mean loss for 2180 iterations: 0.05658628558611856\n",
            "Accuracy Bag: 0.8459672756097981\n",
            "Accuracy NM: 0.9526901669758812\n",
            "Accuracy CL: 0.48701298701298706\n",
            "Mean loss for 2190 iterations: 0.0564345972956962\n",
            "Mean loss for 2200 iterations: 0.05629111507568847\n",
            "Accuracy Bag: 0.8617007053879461\n",
            "Accuracy NM: 0.9749536178107607\n",
            "Accuracy CL: 0.5899814471243042\n",
            "Mean loss for 2210 iterations: 0.05612822104060003\n",
            "Mean loss for 2220 iterations: 0.05597276238699418\n",
            "Accuracy Bag: 0.8598819323815978\n",
            "Accuracy NM: 0.9675324675324675\n",
            "Accuracy CL: 0.601113172541744\n",
            "Mean loss for 2230 iterations: 0.055828452746763894\n",
            "Mean loss for 2240 iterations: 0.05569687530265323\n",
            "Accuracy Bag: 0.8431367207642477\n",
            "Accuracy NM: 0.9675324675324675\n",
            "Accuracy CL: 0.5807050092764379\n",
            "Mean loss for 2250 iterations: 0.05554743313623799\n",
            "Mean loss for 2260 iterations: 0.05538860627169828\n",
            "Accuracy Bag: 0.8440364473300609\n",
            "Accuracy NM: 0.9628942486085343\n",
            "Accuracy CL: 0.4684601113172542\n",
            "Mean loss for 2270 iterations: 0.05525306543204031\n",
            "Mean loss for 2280 iterations: 0.05510094345122445\n",
            "Accuracy Bag: 0.879392301068563\n",
            "Accuracy NM: 0.984230055658627\n",
            "Accuracy CL: 0.5769944341372913\n",
            "Mean loss for 2290 iterations: 0.05493621671425919\n",
            "Mean loss for 2300 iterations: 0.0547724390734473\n",
            "Accuracy Bag: 0.8765912090319642\n",
            "Accuracy NM: 0.9805194805194805\n",
            "Accuracy CL: 0.6270871985157699\n",
            "Mean loss for 2310 iterations: 0.054621808768100875\n",
            "Mean loss for 2320 iterations: 0.05449332128026009\n",
            "Accuracy Bag: 0.8627224369626297\n",
            "Accuracy NM: 0.9684601113172541\n",
            "Accuracy CL: 0.5658627087198517\n",
            "Mean loss for 2330 iterations: 0.05435566260167617\n",
            "Mean loss for 2340 iterations: 0.05421029134856489\n",
            "Accuracy Bag: 0.8635075966713401\n",
            "Accuracy NM: 0.9749536178107608\n",
            "Accuracy CL: 0.5927643784786641\n",
            "Mean loss for 2350 iterations: 0.054065064246706825\n",
            "Mean loss for 2360 iterations: 0.05390767038235504\n",
            "Accuracy Bag: 0.8589261781790902\n",
            "Accuracy NM: 0.9628942486085342\n",
            "Accuracy CL: 0.5380333951762523\n",
            "Mean loss for 2370 iterations: 0.05375178339576765\n",
            "Mean loss for 2380 iterations: 0.053595417075665244\n",
            "Accuracy Bag: 0.8534278384325245\n",
            "Accuracy NM: 0.963821892393321\n",
            "Accuracy CL: 0.5742115027829313\n",
            "Mean loss for 2390 iterations: 0.05344507044870952\n",
            "Mean loss for 2400 iterations: 0.053305582845544756\n",
            "Accuracy Bag: 0.8970278691124857\n",
            "Accuracy NM: 0.9684601113172543\n",
            "Accuracy CL: 0.62152133580705\n",
            "Mean loss for 2410 iterations: 0.053160093019666205\n",
            "Mean loss for 2420 iterations: 0.05300692219983725\n",
            "Accuracy Bag: 0.8366816608214386\n",
            "Accuracy NM: 0.9601113172541744\n",
            "Accuracy CL: 0.5194805194805195\n",
            "Mean loss for 2430 iterations: 0.05286920319633259\n",
            "Mean loss for 2440 iterations: 0.05272478560027743\n",
            "Accuracy Bag: 0.8654587108195355\n",
            "Accuracy NM: 0.9656771799628941\n",
            "Accuracy CL: 0.6020408163265306\n",
            "Mean loss for 2450 iterations: 0.0526140479825209\n",
            "Mean loss for 2460 iterations: 0.05248165349608545\n",
            "Accuracy Bag: 0.8590501151754332\n",
            "Accuracy NM: 0.9740259740259741\n",
            "Accuracy CL: 0.6419294990723562\n",
            "Mean loss for 2470 iterations: 0.05235713401051457\n",
            "Mean loss for 2480 iterations: 0.0522250565683334\n",
            "Accuracy Bag: 0.8524340240768141\n",
            "Accuracy NM: 0.9703153988868274\n",
            "Accuracy CL: 0.5426716141001855\n",
            "Mean loss for 2490 iterations: 0.05209115832673767\n",
            "Mean loss for 2500 iterations: 0.051964270461909476\n",
            "Accuracy Bag: 0.8747257785281618\n",
            "Accuracy NM: 0.9656771799628943\n",
            "Accuracy CL: 0.6298701298701297\n",
            "Mean loss for 2510 iterations: 0.051842391142710685\n",
            "Mean loss for 2520 iterations: 0.05172695245035732\n",
            "Accuracy Bag: 0.8747538889458825\n",
            "Accuracy NM: 0.9740259740259739\n",
            "Accuracy CL: 0.6038961038961039\n",
            "Mean loss for 2530 iterations: 0.05159878490709888\n",
            "Mean loss for 2540 iterations: 0.05145962186484033\n",
            "Accuracy Bag: 0.8636414834031652\n",
            "Accuracy NM: 0.9758812615955473\n",
            "Accuracy CL: 0.5909090909090909\n",
            "Mean loss for 2550 iterations: 0.05132502141554712\n",
            "Mean loss for 2560 iterations: 0.051195342058963436\n",
            "Accuracy Bag: 0.8885930050075183\n",
            "Accuracy NM: 0.9749536178107607\n",
            "Accuracy CL: 0.5528756957328387\n",
            "Mean loss for 2570 iterations: 0.05106552522502666\n",
            "Mean loss for 2580 iterations: 0.05093634048740329\n",
            "Accuracy Bag: 0.891423946250563\n",
            "Accuracy NM: 0.9656771799628943\n",
            "Accuracy CL: 0.6567717996289426\n",
            "Mean loss for 2590 iterations: 0.05080887462645338\n",
            "Mean loss for 2600 iterations: 0.05066813649627595\n",
            "Accuracy Bag: 0.8802350340039456\n",
            "Accuracy NM: 0.978664192949907\n",
            "Accuracy CL: 0.5435992578849722\n",
            "Mean loss for 2610 iterations: 0.05053722403597415\n",
            "Mean loss for 2620 iterations: 0.05041050331414684\n",
            "Accuracy Bag: 0.88305660510775\n",
            "Accuracy NM: 0.9740259740259741\n",
            "Accuracy CL: 0.6094619666048238\n",
            "Mean loss for 2630 iterations: 0.050269187683905314\n",
            "Mean loss for 2640 iterations: 0.05013580004116661\n",
            "Accuracy Bag: 0.8942824183150866\n",
            "Accuracy NM: 0.9768089053803339\n",
            "Accuracy CL: 0.6790352504638218\n",
            "Mean loss for 2650 iterations: 0.04998908184908049\n",
            "Mean loss for 2660 iterations: 0.0498661709625512\n",
            "Accuracy Bag: 0.8812286551609084\n",
            "Accuracy NM: 0.9730983302411875\n",
            "Accuracy CL: 0.6326530612244897\n",
            "Mean loss for 2670 iterations: 0.04973774979149134\n",
            "Mean loss for 2680 iterations: 0.049611556209769765\n",
            "Accuracy Bag: 0.887770364741847\n",
            "Accuracy NM: 0.963821892393321\n",
            "Accuracy CL: 0.6326530612244898\n",
            "Mean loss for 2690 iterations: 0.049485365682728176\n",
            "Mean loss for 2700 iterations: 0.04935680404167484\n",
            "Accuracy Bag: 0.8794207978837784\n",
            "Accuracy NM: 0.9684601113172541\n",
            "Accuracy CL: 0.5927643784786643\n",
            "Mean loss for 2710 iterations: 0.04923317852392243\n",
            "Mean loss for 2720 iterations: 0.0491178497668657\n",
            "Accuracy Bag: 0.8765808728989878\n",
            "Accuracy NM: 0.9860853432282005\n",
            "Accuracy CL: 0.6029684601113172\n",
            "Mean loss for 2730 iterations: 0.04899760148805249\n",
            "Mean loss for 2740 iterations: 0.04888553995822631\n",
            "Accuracy Bag: 0.8913853065011188\n",
            "Accuracy NM: 0.9712430426716142\n",
            "Accuracy CL: 0.62152133580705\n",
            "Mean loss for 2750 iterations: 0.048765396793626924\n",
            "Mean loss for 2760 iterations: 0.04865650883572095\n",
            "Accuracy Bag: 0.8690648620358087\n",
            "Accuracy NM: 0.9740259740259739\n",
            "Accuracy CL: 0.5834879406307979\n",
            "Mean loss for 2770 iterations: 0.048548096658776276\n",
            "Mean loss for 2780 iterations: 0.048438160470184344\n",
            "Accuracy Bag: 0.8829699554696209\n",
            "Accuracy NM: 0.9740259740259739\n",
            "Accuracy CL: 0.5797773654916513\n",
            "Mean loss for 2790 iterations: 0.048322669498448836\n",
            "Mean loss for 2800 iterations: 0.048200706258482696\n",
            "Accuracy Bag: 0.8599217313235255\n",
            "Accuracy NM: 0.9768089053803339\n",
            "Accuracy CL: 0.6094619666048238\n",
            "Mean loss for 2810 iterations: 0.048089157045218975\n",
            "Mean loss for 2820 iterations: 0.047986256811067676\n",
            "Accuracy Bag: 0.8747927218940741\n",
            "Accuracy NM: 0.9675324675324675\n",
            "Accuracy CL: 0.5936920222634509\n",
            "Mean loss for 2830 iterations: 0.04787359997801246\n",
            "Mean loss for 2840 iterations: 0.04775978567072151\n",
            "Accuracy Bag: 0.8942260042808979\n",
            "Accuracy NM: 0.9684601113172541\n",
            "Accuracy CL: 0.6345083487940631\n",
            "Mean loss for 2850 iterations: 0.04764236923895384\n",
            "Mean loss for 2860 iterations: 0.04753001109351749\n",
            "Accuracy Bag: 0.8886597551746835\n",
            "Accuracy NM: 0.9777365491651205\n",
            "Accuracy CL: 0.5890538033395175\n",
            "Mean loss for 2870 iterations: 0.04740443998243569\n",
            "Mean loss for 2880 iterations: 0.04729125509313437\n",
            "Accuracy Bag: 0.895144084727697\n",
            "Accuracy NM: 0.9730983302411874\n",
            "Accuracy CL: 0.6437847866419295\n",
            "Mean loss for 2890 iterations: 0.047192174367522856\n",
            "Mean loss for 2900 iterations: 0.04709199846160181\n",
            "Accuracy Bag: 0.8421350818592752\n",
            "Accuracy NM: 0.9721706864564009\n",
            "Accuracy CL: 0.6224489795918366\n",
            "Mean loss for 2910 iterations: 0.04698317100588373\n",
            "Mean loss for 2920 iterations: 0.04687421955949665\n",
            "Accuracy Bag: 0.8895977350924466\n",
            "Accuracy NM: 0.9749536178107605\n",
            "Accuracy CL: 0.6363636363636362\n",
            "Mean loss for 2930 iterations: 0.04676072871258463\n",
            "Mean loss for 2940 iterations: 0.04664773186573721\n",
            "Accuracy Bag: 0.8942638712353534\n",
            "Accuracy NM: 0.9684601113172543\n",
            "Accuracy CL: 0.6623376623376623\n",
            "Mean loss for 2950 iterations: 0.04653847220432708\n",
            "Mean loss for 2960 iterations: 0.046422542364814794\n",
            "Accuracy Bag: 0.872880054296576\n",
            "Accuracy NM: 0.979591836734694\n",
            "Accuracy CL: 0.5658627087198516\n",
            "Mean loss for 2970 iterations: 0.04631265240755898\n",
            "Mean loss for 2980 iterations: 0.046203194047955656\n",
            "Accuracy Bag: 0.890497075260765\n",
            "Accuracy NM: 0.9795918367346939\n",
            "Accuracy CL: 0.6437847866419295\n",
            "Mean loss for 2990 iterations: 0.04609444118182745\n",
            "Mean loss for 3000 iterations: 0.04598393301138033\n",
            "Accuracy Bag: 0.8876563774809859\n",
            "Accuracy NM: 0.9768089053803339\n",
            "Accuracy CL: 0.673469387755102\n",
            "Mean loss for 3010 iterations: 0.04588277040256738\n",
            "Mean loss for 3020 iterations: 0.04577615712630556\n",
            "Accuracy Bag: 0.8533511385298774\n",
            "Accuracy NM: 0.9749536178107607\n",
            "Accuracy CL: 0.5500927643784788\n",
            "Mean loss for 3030 iterations: 0.045665849928618114\n",
            "Mean loss for 3040 iterations: 0.045563378746584546\n",
            "Accuracy Bag: 0.8932889903568708\n",
            "Accuracy NM: 0.9749536178107607\n",
            "Accuracy CL: 0.6048237476808906\n",
            "Mean loss for 3050 iterations: 0.04546346382237971\n",
            "Mean loss for 3060 iterations: 0.04535455330749791\n",
            "Accuracy Bag: 0.8961195452024211\n",
            "Accuracy NM: 0.9675324675324675\n",
            "Accuracy CL: 0.6122448979591837\n",
            "Mean loss for 3070 iterations: 0.0452445283375911\n",
            "Mean loss for 3080 iterations: 0.04513079421292362\n",
            "Accuracy Bag: 0.9090501441552454\n",
            "Accuracy NM: 0.9805194805194805\n",
            "Accuracy CL: 0.6131725417439703\n",
            "Mean loss for 3090 iterations: 0.04503466098216697\n",
            "Mean loss for 3100 iterations: 0.04492781920734072\n",
            "Accuracy Bag: 0.870977529633307\n",
            "Accuracy NM: 0.9795918367346939\n",
            "Accuracy CL: 0.6716141001855287\n",
            "Mean loss for 3110 iterations: 0.04482499663538368\n",
            "Mean loss for 3120 iterations: 0.044732517585044915\n",
            "Accuracy Bag: 0.8915375471139295\n",
            "Accuracy NM: 0.9721706864564006\n",
            "Accuracy CL: 0.6363636363636362\n",
            "Mean loss for 3130 iterations: 0.04463531511222807\n",
            "Mean loss for 3140 iterations: 0.04454260242563097\n",
            "Accuracy Bag: 0.9044583895300189\n",
            "Accuracy NM: 0.9721706864564008\n",
            "Accuracy CL: 0.5946196660482376\n",
            "Mean loss for 3150 iterations: 0.044445834995823955\n",
            "Mean loss for 3160 iterations: 0.044343988134236886\n",
            "Accuracy Bag: 0.886814224141845\n",
            "Accuracy NM: 0.9758812615955473\n",
            "Accuracy CL: 0.6512059369202227\n",
            "Mean loss for 3170 iterations: 0.04424406305008365\n",
            "Mean loss for 3180 iterations: 0.0441450484293226\n",
            "Accuracy Bag: 0.8654110873283454\n",
            "Accuracy NM: 0.9730983302411874\n",
            "Accuracy CL: 0.5621521335807049\n",
            "Mean loss for 3190 iterations: 0.04404370468445589\n",
            "Mean loss for 3200 iterations: 0.043956351963424824\n",
            "Accuracy Bag: 0.8886892179836349\n",
            "Accuracy NM: 0.9833024118738404\n",
            "Accuracy CL: 0.6122448979591836\n",
            "Mean loss for 3210 iterations: 0.043859501459093245\n",
            "Mean loss for 3220 iterations: 0.04376165101475296\n",
            "Accuracy Bag: 0.8691413687397085\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.673469387755102\n",
            "Mean loss for 3230 iterations: 0.043664877637845954\n",
            "Mean loss for 3240 iterations: 0.04355693866734473\n",
            "Accuracy Bag: 0.8756438589749608\n",
            "Accuracy NM: 0.9823747680890538\n",
            "Accuracy CL: 0.5658627087198517\n",
            "Mean loss for 3250 iterations: 0.04345578634015356\n",
            "Mean loss for 3260 iterations: 0.04335635641691178\n",
            "Accuracy Bag: 0.8932987468936058\n",
            "Accuracy NM: 0.9823747680890539\n",
            "Accuracy CL: 0.6549165120593692\n",
            "Mean loss for 3270 iterations: 0.043257054342711716\n",
            "Mean loss for 3280 iterations: 0.04316165455732006\n",
            "Accuracy Bag: 0.9238922515130842\n",
            "Accuracy NM: 0.9749536178107608\n",
            "Accuracy CL: 0.6011131725417439\n",
            "Mean loss for 3290 iterations: 0.043061789778191424\n",
            "Mean loss for 3300 iterations: 0.042965580828727756\n",
            "Accuracy Bag: 0.9109618457590076\n",
            "Accuracy NM: 0.9814471243042671\n",
            "Accuracy CL: 0.6706864564007421\n",
            "Mean loss for 3310 iterations: 0.04287214593355491\n",
            "Mean loss for 3320 iterations: 0.042777067592300204\n",
            "Accuracy Bag: 0.9016473477579382\n",
            "Accuracy NM: 0.9814471243042674\n",
            "Accuracy CL: 0.6502782931354361\n",
            "Mean loss for 3330 iterations: 0.04269168822055643\n",
            "Mean loss for 3340 iterations: 0.042588767214416116\n",
            "Accuracy Bag: 0.9025654282047374\n",
            "Accuracy NM: 0.9860853432282005\n",
            "Accuracy CL: 0.6781076066790352\n",
            "Mean loss for 3350 iterations: 0.04250277120898019\n",
            "Mean loss for 3360 iterations: 0.0424067561294318\n",
            "Accuracy Bag: 0.9174274350335403\n",
            "Accuracy NM: 0.9721706864564008\n",
            "Accuracy CL: 0.6159554730983303\n",
            "Mean loss for 3370 iterations: 0.042309024785051136\n",
            "Mean loss for 3380 iterations: 0.04221736874869479\n",
            "Accuracy Bag: 0.903454818637574\n",
            "Accuracy NM: 0.9805194805194806\n",
            "Accuracy CL: 0.6484230055658627\n",
            "Mean loss for 3390 iterations: 0.04212659762246441\n",
            "Mean loss for 3400 iterations: 0.04203062019406763\n",
            "Accuracy Bag: 0.9016758445731535\n",
            "Accuracy NM: 0.9833024118738405\n",
            "Accuracy CL: 0.6354359925788496\n",
            "Mean loss for 3410 iterations: 0.04193627984410485\n",
            "Mean loss for 3420 iterations: 0.04184058814694865\n",
            "Accuracy Bag: 0.8942447445593783\n",
            "Accuracy NM: 0.9795918367346939\n",
            "Accuracy CL: 0.6391465677179963\n",
            "Mean loss for 3430 iterations: 0.04174985475546297\n",
            "Mean loss for 3440 iterations: 0.04166461533770712\n",
            "Accuracy Bag: 0.9062291526476826\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.6855287569573284\n",
            "Mean loss for 3450 iterations: 0.04157785733088689\n",
            "Mean loss for 3460 iterations: 0.04149405747360006\n",
            "Accuracy Bag: 0.8794123937382742\n",
            "Accuracy NM: 0.9721706864564008\n",
            "Accuracy CL: 0.6484230055658626\n",
            "Mean loss for 3470 iterations: 0.041417487595238396\n",
            "Mean loss for 3480 iterations: 0.04133012441218424\n",
            "Accuracy Bag: 0.9043641085413747\n",
            "Accuracy NM: 0.976808905380334\n",
            "Accuracy CL: 0.6827458256029686\n",
            "Mean loss for 3490 iterations: 0.04124627223866791\n",
            "Mean loss for 3500 iterations: 0.04115295178962073\n",
            "Accuracy Bag: 0.902613244894675\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.6178107606679035\n",
            "Mean loss for 3510 iterations: 0.0410641833678806\n",
            "Mean loss for 3520 iterations: 0.04097499373992006\n",
            "Accuracy Bag: 0.9118805058020482\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.6706864564007421\n",
            "Mean loss for 3530 iterations: 0.040886680609361924\n",
            "Mean loss for 3540 iterations: 0.04079910268958416\n",
            "Accuracy Bag: 0.9080936171577491\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.647495361781076\n",
            "Mean loss for 3550 iterations: 0.040715468133878434\n",
            "Mean loss for 3560 iterations: 0.040629704143613936\n",
            "Accuracy Bag: 0.9044111524363231\n",
            "Accuracy NM: 0.9786641929499073\n",
            "Accuracy CL: 0.6743970315398887\n",
            "Mean loss for 3570 iterations: 0.04054504155686141\n",
            "Mean loss for 3580 iterations: 0.040459356862237394\n",
            "Accuracy Bag: 0.8934119613594778\n",
            "Accuracy NM: 0.9814471243042672\n",
            "Accuracy CL: 0.5834879406307978\n",
            "Mean loss for 3590 iterations: 0.040375781581934264\n",
            "Mean loss for 3600 iterations: 0.040294399775448256\n",
            "Accuracy Bag: 0.9090688844337259\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.6391465677179963\n",
            "Mean loss for 3610 iterations: 0.040209941936778085\n",
            "Mean loss for 3620 iterations: 0.04012459145494845\n",
            "Accuracy Bag: 0.9110284027274254\n",
            "Accuracy NM: 0.9805194805194806\n",
            "Accuracy CL: 0.6280148423005566\n",
            "Mean loss for 3630 iterations: 0.040040391022608854\n",
            "Mean loss for 3640 iterations: 0.039958560896415654\n",
            "Accuracy Bag: 0.8896061392379507\n",
            "Accuracy NM: 0.9814471243042674\n",
            "Accuracy CL: 0.6363636363636364\n",
            "Mean loss for 3650 iterations: 0.03988023120724941\n",
            "Mean loss for 3660 iterations: 0.03979705541332308\n",
            "Accuracy Bag: 0.9174368051727805\n",
            "Accuracy NM: 0.9805194805194806\n",
            "Accuracy CL: 0.6428571428571429\n",
            "Mean loss for 3670 iterations: 0.03971909919305773\n",
            "Mean loss for 3680 iterations: 0.03964098477527327\n",
            "Accuracy Bag: 0.9081318705096989\n",
            "Accuracy NM: 0.9795918367346939\n",
            "Accuracy CL: 0.62708719851577\n",
            "Mean loss for 3690 iterations: 0.03956340744739353\n",
            "Mean loss for 3700 iterations: 0.03948208857623457\n",
            "Accuracy Bag: 0.9108864982475908\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.650278293135436\n",
            "Mean loss for 3710 iterations: 0.039396350535330665\n",
            "Mean loss for 3720 iterations: 0.039322321424110525\n",
            "Accuracy Bag: 0.9128169401298334\n",
            "Accuracy NM: 0.9833024118738407\n",
            "Accuracy CL: 0.6447124304267162\n",
            "Mean loss for 3730 iterations: 0.039239313076787075\n",
            "Mean loss for 3740 iterations: 0.0391604576431673\n",
            "Accuracy Bag: 0.9118332687083524\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.6669758812615955\n",
            "Mean loss for 3750 iterations: 0.03908015535076459\n",
            "Mean loss for 3760 iterations: 0.03900507742729771\n",
            "Accuracy Bag: 0.9045060130212093\n",
            "Accuracy NM: 0.9833024118738404\n",
            "Accuracy CL: 0.6623376623376623\n",
            "Mean loss for 3770 iterations: 0.03892929618906437\n",
            "Mean loss for 3780 iterations: 0.03885339994978889\n",
            "Accuracy Bag: 0.915638704432385\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.6938775510204082\n",
            "Mean loss for 3790 iterations: 0.03877102064280857\n",
            "Mean loss for 3800 iterations: 0.03869573854277596\n",
            "Accuracy Bag: 0.8970183057744979\n",
            "Accuracy NM: 0.9814471243042671\n",
            "Accuracy CL: 0.6623376623376624\n",
            "Mean loss for 3810 iterations: 0.03861159294711675\n",
            "Mean loss for 3820 iterations: 0.038529410866072096\n",
            "Accuracy Bag: 0.9100248318349805\n",
            "Accuracy NM: 0.9823747680890538\n",
            "Accuracy CL: 0.6614100185528757\n",
            "Mean loss for 3830 iterations: 0.03844758820684758\n",
            "Mean loss for 3840 iterations: 0.03837149464640485\n",
            "Accuracy Bag: 0.9182978919891494\n",
            "Accuracy NM: 0.9823747680890539\n",
            "Accuracy CL: 0.686456400742115\n",
            "Mean loss for 3850 iterations: 0.038291207891767966\n",
            "Mean loss for 3860 iterations: 0.03821143202014374\n",
            "Accuracy Bag: 0.9313803451572898\n",
            "Accuracy NM: 0.9823747680890538\n",
            "Accuracy CL: 0.6883116883116883\n",
            "Mean loss for 3870 iterations: 0.0381306780633957\n",
            "Mean loss for 3880 iterations: 0.03804978928684746\n",
            "Accuracy Bag: 0.9072325303413802\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.62708719851577\n",
            "Mean loss for 3890 iterations: 0.03797517893823932\n",
            "Mean loss for 3900 iterations: 0.037899484695890585\n",
            "Accuracy Bag: 0.9350247497255129\n",
            "Accuracy NM: 0.9823747680890538\n",
            "Accuracy CL: 0.6781076066790352\n",
            "Mean loss for 3910 iterations: 0.03782375273568665\n",
            "Mean loss for 3920 iterations: 0.037750838681276204\n",
            "Accuracy Bag: 0.923022374153717\n",
            "Accuracy NM: 0.978664192949907\n",
            "Accuracy CL: 0.6790352504638218\n",
            "Mean loss for 3930 iterations: 0.03767203953064997\n",
            "Mean loss for 3940 iterations: 0.037595338228177566\n",
            "Accuracy Bag: 0.8997643361681387\n",
            "Accuracy NM: 0.9860853432282005\n",
            "Accuracy CL: 0.7012987012987014\n",
            "Mean loss for 3950 iterations: 0.037518253920517296\n",
            "Mean loss for 3960 iterations: 0.037446479432136665\n",
            "Accuracy Bag: 0.9127698962348848\n",
            "Accuracy NM: 0.9814471243042672\n",
            "Accuracy CL: 0.6771799628942486\n",
            "Mean loss for 3970 iterations: 0.03737320509752123\n",
            "Mean loss for 3980 iterations: 0.037306950371373146\n",
            "Accuracy Bag: 0.9174551590537667\n",
            "Accuracy NM: 0.9795918367346939\n",
            "Accuracy CL: 0.6113172541743971\n",
            "Mean loss for 3990 iterations: 0.037237710293258695\n",
            "Mean loss for 4000 iterations: 0.03716486811527284\n",
            "Accuracy Bag: 0.8969993722972703\n",
            "Accuracy NM: 0.9823747680890538\n",
            "Accuracy CL: 0.6549165120593692\n",
            "Mean loss for 4010 iterations: 0.03709939548129128\n",
            "Mean loss for 4020 iterations: 0.037031363553555206\n",
            "Accuracy Bag: 0.9007009636946711\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.634508348794063\n",
            "Mean loss for 4030 iterations: 0.0369574232683829\n",
            "Mean loss for 4040 iterations: 0.03688623097737891\n",
            "Accuracy Bag: 0.9128645636210236\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.673469387755102\n",
            "Mean loss for 4050 iterations: 0.03681452825245804\n",
            "Mean loss for 4060 iterations: 0.03673927471755082\n",
            "Accuracy Bag: 0.9202197365271405\n",
            "Accuracy NM: 0.9823747680890539\n",
            "Accuracy CL: 0.6493506493506495\n",
            "Mean loss for 4070 iterations: 0.03666867781499131\n",
            "Mean loss for 4080 iterations: 0.036597485307492225\n",
            "Accuracy Bag: 0.8978990988630834\n",
            "Accuracy NM: 0.9823747680890538\n",
            "Accuracy CL: 0.6270871985157698\n",
            "Mean loss for 4090 iterations: 0.036522371458768625\n",
            "Mean loss for 4100 iterations: 0.0364538336516835\n",
            "Accuracy Bag: 0.9108954819893366\n",
            "Accuracy NM: 0.9814471243042671\n",
            "Accuracy CL: 0.6818181818181818\n",
            "Mean loss for 4110 iterations: 0.03638024026530267\n",
            "Mean loss for 4120 iterations: 0.03631157074383577\n",
            "Accuracy Bag: 0.8959959946035727\n",
            "Accuracy NM: 0.9777365491651205\n",
            "Accuracy CL: 0.6818181818181818\n",
            "Mean loss for 4130 iterations: 0.0362418869974263\n",
            "Mean loss for 4140 iterations: 0.036170712102020094\n",
            "Accuracy Bag: 0.9118137556348831\n",
            "Accuracy NM: 0.9833024118738405\n",
            "Accuracy CL: 0.6447124304267161\n",
            "Mean loss for 4150 iterations: 0.03610384659404047\n",
            "Mean loss for 4160 iterations: 0.03603975837406604\n",
            "Accuracy Bag: 0.9220180304662833\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6307977736549165\n",
            "Mean loss for 4170 iterations: 0.03597460686557477\n",
            "Mean loss for 4180 iterations: 0.03591054350184499\n",
            "Accuracy Bag: 0.9137067169601646\n",
            "Accuracy NM: 0.979591836734694\n",
            "Accuracy CL: 0.6168831168831169\n",
            "Mean loss for 4190 iterations: 0.03584452659521821\n",
            "Mean loss for 4200 iterations: 0.03577678177483557\n",
            "Accuracy Bag: 0.8821576513369258\n",
            "Accuracy NM: 0.9851576994434137\n",
            "Accuracy CL: 0.6317254174397031\n",
            "Mean loss for 4210 iterations: 0.03571171005977641\n",
            "Mean loss for 4220 iterations: 0.03564086865282825\n",
            "Accuracy Bag: 0.9416226801418929\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7087198515769945\n",
            "Mean loss for 4230 iterations: 0.035574310018084324\n",
            "Mean loss for 4240 iterations: 0.035500730471553735\n",
            "Accuracy Bag: 0.9322611382458752\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.6938775510204082\n",
            "Mean loss for 4250 iterations: 0.035431401048074754\n",
            "Mean loss for 4260 iterations: 0.035359621755210374\n",
            "Accuracy Bag: 0.914654260215915\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.6679035250463822\n",
            "Mean loss for 4270 iterations: 0.035291760793311276\n",
            "Mean loss for 4280 iterations: 0.0352237321297686\n",
            "Accuracy Bag: 0.9239406477992634\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7115027829313543\n",
            "Mean loss for 4290 iterations: 0.03515713864612873\n",
            "Mean loss for 4300 iterations: 0.03508774278382229\n",
            "Accuracy Bag: 0.9248013482181378\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7170686456400742\n",
            "Mean loss for 4310 iterations: 0.03501952733572253\n",
            "Mean loss for 4320 iterations: 0.034951086649164576\n",
            "Accuracy Bag: 0.9350727596141977\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6948051948051949\n",
            "Mean loss for 4330 iterations: 0.0348826391662215\n",
            "Mean loss for 4340 iterations: 0.03481826530018401\n",
            "Accuracy Bag: 0.9239590016802496\n",
            "Accuracy NM: 0.9860853432282003\n",
            "Accuracy CL: 0.6604823747680891\n",
            "Mean loss for 4350 iterations: 0.03475308455521477\n",
            "Mean loss for 4360 iterations: 0.034686608924081874\n",
            "Accuracy Bag: 0.9053487459565918\n",
            "Accuracy NM: 0.9879406307977735\n",
            "Accuracy CL: 0.6437847866419296\n",
            "Mean loss for 4370 iterations: 0.03462164615183958\n",
            "Mean loss for 4380 iterations: 0.03455884491867432\n",
            "Accuracy Bag: 0.9304142548218061\n",
            "Accuracy NM: 0.9823747680890538\n",
            "Accuracy CL: 0.7096474953617811\n",
            "Mean loss for 4390 iterations: 0.03449466780484665\n",
            "Mean loss for 4400 iterations: 0.03443214667553548\n",
            "Accuracy Bag: 0.9313422850040872\n",
            "Accuracy NM: 0.9897959183673468\n",
            "Accuracy CL: 0.6345083487940631\n",
            "Mean loss for 4410 iterations: 0.03437067228269102\n",
            "Mean loss for 4420 iterations: 0.034306055294693244\n",
            "Accuracy Bag: 0.9090683048374842\n",
            "Accuracy NM: 0.9823747680890539\n",
            "Accuracy CL: 0.6957328385899815\n",
            "Mean loss for 4430 iterations: 0.034243898483731154\n",
            "Mean loss for 4440 iterations: 0.034179310392124816\n",
            "Accuracy Bag: 0.9461757950176749\n",
            "Accuracy NM: 0.9925788497217067\n",
            "Accuracy CL: 0.7133580705009276\n",
            "Mean loss for 4450 iterations: 0.034117724218384854\n",
            "Mean loss for 4460 iterations: 0.03405667331530149\n",
            "Accuracy Bag: 0.9044115388338176\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6400742115027831\n",
            "Mean loss for 4470 iterations: 0.033993816923014654\n",
            "Mean loss for 4480 iterations: 0.03393254767652252\n",
            "Accuracy Bag: 0.9396731115836752\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.6846011131725418\n",
            "Mean loss for 4490 iterations: 0.03386919632116395\n",
            "Mean loss for 4500 iterations: 0.03380604057533977\n",
            "Accuracy Bag: 0.933131595201484\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.6975881261595548\n",
            "Mean loss for 4510 iterations: 0.033744592869271235\n",
            "Mean loss for 4520 iterations: 0.03368501282916239\n",
            "Accuracy Bag: 0.9239498247397564\n",
            "Accuracy NM: 0.9833024118738405\n",
            "Accuracy CL: 0.6753246753246753\n",
            "Mean loss for 4530 iterations: 0.033624387917234246\n",
            "Mean loss for 4540 iterations: 0.033565255047267276\n",
            "Accuracy Bag: 0.9397201554786239\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.692022263450835\n",
            "Mean loss for 4550 iterations: 0.033502522241273015\n",
            "Mean loss for 4560 iterations: 0.03344330731741042\n",
            "Accuracy Bag: 0.9193016560803412\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.7003710575139146\n",
            "Mean loss for 4570 iterations: 0.033381332910091856\n",
            "Mean loss for 4580 iterations: 0.033325541149835027\n",
            "Accuracy Bag: 0.914587703247497\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.6651205936920223\n",
            "Mean loss for 4590 iterations: 0.033265482404417394\n",
            "Mean loss for 4600 iterations: 0.033204993209093\n",
            "Accuracy Bag: 0.9387921252963429\n",
            "Accuracy NM: 0.9935064935064937\n",
            "Accuracy CL: 0.7133580705009277\n",
            "Mean loss for 4610 iterations: 0.033143655797810076\n",
            "Mean loss for 4620 iterations: 0.03308477741903001\n",
            "Accuracy Bag: 0.9202289134676337\n",
            "Accuracy NM: 0.9888682745825604\n",
            "Accuracy CL: 0.7198515769944343\n",
            "Mean loss for 4630 iterations: 0.03302594848066673\n",
            "Mean loss for 4640 iterations: 0.03296780026776732\n",
            "Accuracy Bag: 0.915619384557663\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.6484230055658627\n",
            "Mean loss for 4650 iterations: 0.032910276535818336\n",
            "Mean loss for 4660 iterations: 0.0328523984434215\n",
            "Accuracy Bag: 0.9267425126308511\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.6873840445269017\n",
            "Mean loss for 4670 iterations: 0.032792005475163775\n",
            "Mean loss for 4680 iterations: 0.032732973518755495\n",
            "Accuracy Bag: 0.9350253293217545\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.6855287569573284\n",
            "Mean loss for 4690 iterations: 0.03267465607612642\n",
            "Mean loss for 4700 iterations: 0.0326127905574221\n",
            "Accuracy Bag: 0.9350442627989825\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7300556586270872\n",
            "Mean loss for 4710 iterations: 0.03255244831523943\n",
            "Mean loss for 4720 iterations: 0.032492962302699276\n",
            "Accuracy Bag: 0.910962425355249\n",
            "Accuracy NM: 0.9879406307977738\n",
            "Accuracy CL: 0.666048237476809\n",
            "Mean loss for 4730 iterations: 0.032435676535536355\n",
            "Mean loss for 4740 iterations: 0.03237704084660536\n",
            "Accuracy Bag: 0.9193020424778358\n",
            "Accuracy NM: 0.9833024118738404\n",
            "Accuracy CL: 0.6679035250463822\n",
            "Mean loss for 4750 iterations: 0.03231828164770023\n",
            "Mean loss for 4760 iterations: 0.03226007871220217\n",
            "Accuracy Bag: 0.9248485853118334\n",
            "Accuracy NM: 0.9953617810760668\n",
            "Accuracy CL: 0.6892393320964749\n",
            "Mean loss for 4770 iterations: 0.032202124145161166\n",
            "Mean loss for 4780 iterations: 0.032141900644272324\n",
            "Accuracy Bag: 0.9211854404651298\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.7300556586270872\n",
            "Mean loss for 4790 iterations: 0.03208341952226181\n",
            "Mean loss for 4800 iterations: 0.032024328816114576\n",
            "Accuracy Bag: 0.939663741444435\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.699443413729128\n",
            "Mean loss for 4810 iterations: 0.031967472804496314\n",
            "Mean loss for 4820 iterations: 0.03190985323380122\n",
            "Accuracy Bag: 0.9118424456488454\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.6456400742115028\n",
            "Mean loss for 4830 iterations: 0.03185202062460661\n",
            "Mean loss for 4840 iterations: 0.0317949162492594\n",
            "Accuracy Bag: 0.940675716482384\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7133580705009277\n",
            "Mean loss for 4850 iterations: 0.03173890312492713\n",
            "Mean loss for 4860 iterations: 0.03168508527465567\n",
            "Accuracy Bag: 0.9414993227417916\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7059369202226345\n",
            "Mean loss for 4870 iterations: 0.03163077634846738\n",
            "Mean loss for 4880 iterations: 0.03157801828058494\n",
            "Accuracy Bag: 0.9285406133712467\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.666048237476809\n",
            "Mean loss for 4890 iterations: 0.03152574814962231\n",
            "Mean loss for 4900 iterations: 0.03147371224101576\n",
            "Accuracy Bag: 0.9257389417384061\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6771799628942485\n",
            "Mean loss for 4910 iterations: 0.0314193907739403\n",
            "Mean loss for 4920 iterations: 0.031365335637602264\n",
            "Accuracy Bag: 0.935981083524262\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.7152133580705009\n",
            "Mean loss for 4930 iterations: 0.031309485059509215\n",
            "Mean loss for 4940 iterations: 0.03125396181163774\n",
            "Accuracy Bag: 0.9359141401583496\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.6716141001855288\n",
            "Mean loss for 4950 iterations: 0.031202090040714753\n",
            "Mean loss for 4960 iterations: 0.031148563558517414\n",
            "Accuracy Bag: 0.9369083409115543\n",
            "Accuracy NM: 0.9916512059369204\n",
            "Accuracy CL: 0.692022263450835\n",
            "Mean loss for 4970 iterations: 0.031095093448076705\n",
            "Mean loss for 4980 iterations: 0.03104693609828421\n",
            "Accuracy Bag: 0.8978231717554251\n",
            "Accuracy NM: 0.9833024118738404\n",
            "Accuracy CL: 0.6280148423005566\n",
            "Mean loss for 4990 iterations: 0.030998402894875838\n",
            "Mean loss for 5000 iterations: 0.030951315328851342\n",
            "Accuracy Bag: 0.9155149606347894\n",
            "Accuracy NM: 0.9795918367346939\n",
            "Accuracy CL: 0.6252319109461966\n",
            "Mean loss for 5010 iterations: 0.030907130915162243\n",
            "Mean loss for 5020 iterations: 0.03085987440113765\n",
            "Accuracy Bag: 0.9229841208017672\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.6641929499072357\n",
            "Mean loss for 5030 iterations: 0.03081224955281402\n",
            "Mean loss for 5040 iterations: 0.030764280233184792\n",
            "Accuracy Bag: 0.9220849738321957\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.6827458256029685\n",
            "Mean loss for 5050 iterations: 0.030716276364723717\n",
            "Mean loss for 5060 iterations: 0.030665748242542826\n",
            "Accuracy Bag: 0.9174750585247305\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.7012987012987013\n",
            "Mean loss for 5070 iterations: 0.030614940089557532\n",
            "Mean loss for 5080 iterations: 0.03056362952592904\n",
            "Accuracy Bag: 0.9173985518208307\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.7003710575139146\n",
            "Mean loss for 5090 iterations: 0.030511898286288426\n",
            "Mean loss for 5100 iterations: 0.030461189740807658\n",
            "Accuracy Bag: 0.914644310480433\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.6669758812615956\n",
            "Mean loss for 5110 iterations: 0.03041039834406369\n",
            "Mean loss for 5120 iterations: 0.0303620553726887\n",
            "Accuracy Bag: 0.9156002578816879\n",
            "Accuracy NM: 0.9833024118738404\n",
            "Accuracy CL: 0.6484230055658627\n",
            "Mean loss for 5130 iterations: 0.030311425319204714\n",
            "Mean loss for 5140 iterations: 0.030259452716144823\n",
            "Accuracy Bag: 0.9184028955082645\n",
            "Accuracy NM: 0.9851576994434136\n",
            "Accuracy CL: 0.7170686456400742\n",
            "Mean loss for 5150 iterations: 0.030208910488957557\n",
            "Mean loss for 5160 iterations: 0.030158572018646468\n",
            "Accuracy Bag: 0.9276412732029281\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.692022263450835\n",
            "Mean loss for 5170 iterations: 0.03010640812020414\n",
            "Mean loss for 5180 iterations: 0.030053963023350255\n",
            "Accuracy Bag: 0.9378931715255185\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.7421150278293136\n",
            "Mean loss for 5190 iterations: 0.030002949414604303\n",
            "Mean loss for 5200 iterations: 0.029952614845197578\n",
            "Accuracy Bag: 0.9443298775873417\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.6029684601113173\n",
            "Mean loss for 5210 iterations: 0.029902784323336328\n",
            "Mean loss for 5220 iterations: 0.029851950507245734\n",
            "Accuracy Bag: 0.9415373828949943\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6781076066790352\n",
            "Mean loss for 5230 iterations: 0.029801448699419458\n",
            "Mean loss for 5240 iterations: 0.029751228310093587\n",
            "Accuracy Bag: 0.9387353248646595\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7059369202226344\n",
            "Mean loss for 5250 iterations: 0.02970200272196061\n",
            "Mean loss for 5260 iterations: 0.02965179131218691\n",
            "Accuracy Bag: 0.9396731115836752\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.7087198515769945\n",
            "Mean loss for 5270 iterations: 0.029600792406628305\n",
            "Mean loss for 5280 iterations: 0.029550195787867942\n",
            "Accuracy Bag: 0.9471504826974101\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7402597402597402\n",
            "Mean loss for 5290 iterations: 0.02949938767422158\n",
            "Mean loss for 5300 iterations: 0.02944773177308786\n",
            "Accuracy Bag: 0.9359996306039954\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7328385899814471\n",
            "Mean loss for 5310 iterations: 0.029397087672248697\n",
            "Mean loss for 5320 iterations: 0.029346415082627777\n",
            "Accuracy Bag: 0.9276317098649405\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7458256029684601\n",
            "Mean loss for 5330 iterations: 0.02929634618467833\n",
            "Mean loss for 5340 iterations: 0.029246877144081675\n",
            "Accuracy Bag: 0.9183745918917964\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.7059369202226345\n",
            "Mean loss for 5350 iterations: 0.02919722679640149\n",
            "Mean loss for 5360 iterations: 0.029148392829904595\n",
            "Accuracy Bag: 0.9424747832165157\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.6781076066790352\n",
            "Mean loss for 5370 iterations: 0.02909870883174591\n",
            "Mean loss for 5380 iterations: 0.029050955141169194\n",
            "Accuracy Bag: 0.9303865308015797\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.6985157699443415\n",
            "Mean loss for 5390 iterations: 0.029002364474873133\n",
            "Mean loss for 5400 iterations: 0.02895427252840751\n",
            "Accuracy Bag: 0.9137168598943937\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.7003710575139146\n",
            "Mean loss for 5410 iterations: 0.02890810286458375\n",
            "Mean loss for 5420 iterations: 0.028861125869327653\n",
            "Accuracy Bag: 0.9415283991532486\n",
            "Accuracy NM: 0.9888682745825604\n",
            "Accuracy CL: 0.6706864564007422\n",
            "Mean loss for 5430 iterations: 0.028813276858292828\n",
            "Mean loss for 5440 iterations: 0.028767481295204456\n",
            "Accuracy Bag: 0.9118523953843275\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.6994434137291281\n",
            "Mean loss for 5450 iterations: 0.028721005715933893\n",
            "Mean loss for 5460 iterations: 0.028675921364591864\n",
            "Accuracy Bag: 0.9378549181735686\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7096474953617811\n",
            "Mean loss for 5470 iterations: 0.02863115655061222\n",
            "Mean loss for 5480 iterations: 0.02858448796567803\n",
            "Accuracy Bag: 0.9350913066939309\n",
            "Accuracy NM: 0.9888682745825604\n",
            "Accuracy CL: 0.7152133580705009\n",
            "Mean loss for 5490 iterations: 0.028540184462968028\n",
            "Mean loss for 5500 iterations: 0.02849482897171666\n",
            "Accuracy Bag: 0.9231067054068794\n",
            "Accuracy NM: 0.9823747680890538\n",
            "Accuracy CL: 0.6966604823747681\n",
            "Mean loss for 5510 iterations: 0.02845062436030995\n",
            "Mean loss for 5520 iterations: 0.028405835065443197\n",
            "Accuracy Bag: 0.9202101731891531\n",
            "Accuracy NM: 0.9870129870129869\n",
            "Accuracy CL: 0.6873840445269017\n",
            "Mean loss for 5530 iterations: 0.028362349635091145\n",
            "Mean loss for 5540 iterations: 0.028316476934636518\n",
            "Accuracy Bag: 0.9359240898938315\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.6623376623376624\n",
            "Mean loss for 5550 iterations: 0.02827295010124946\n",
            "Mean loss for 5560 iterations: 0.02823008729665031\n",
            "Accuracy Bag: 0.9517225310504197\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.6771799628942486\n",
            "Mean loss for 5570 iterations: 0.02818703925848606\n",
            "Mean loss for 5580 iterations: 0.028143121588576292\n",
            "Accuracy Bag: 0.9471032456037143\n",
            "Accuracy NM: 0.9916512059369204\n",
            "Accuracy CL: 0.6846011131725418\n",
            "Mean loss for 5590 iterations: 0.028100442015778354\n",
            "Mean loss for 5600 iterations: 0.02805684109303651\n",
            "Accuracy Bag: 0.9155153470322838\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6938775510204082\n",
            "Mean loss for 5610 iterations: 0.028011479237990092\n",
            "Mean loss for 5620 iterations: 0.02796739226646698\n",
            "Accuracy Bag: 0.9266943095434192\n",
            "Accuracy NM: 0.9916512059369204\n",
            "Accuracy CL: 0.6753246753246752\n",
            "Mean loss for 5630 iterations: 0.02792328762318145\n",
            "Mean loss for 5640 iterations: 0.027877774701518972\n",
            "Accuracy Bag: 0.9229653805232867\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.7031539888682745\n",
            "Mean loss for 5650 iterations: 0.027833395061676546\n",
            "Mean loss for 5660 iterations: 0.027789290837203925\n",
            "Accuracy Bag: 0.9434022338025551\n",
            "Accuracy NM: 0.9907235621521338\n",
            "Accuracy CL: 0.6957328385899816\n",
            "Mean loss for 5670 iterations: 0.02774587208663838\n",
            "Mean loss for 5680 iterations: 0.027702023934910704\n",
            "Accuracy Bag: 0.9340875426027389\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.6957328385899815\n",
            "Mean loss for 5690 iterations: 0.027657095323898217\n",
            "Mean loss for 5700 iterations: 0.02761380482149482\n",
            "Accuracy Bag: 0.9434871446519592\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7170686456400742\n",
            "Mean loss for 5710 iterations: 0.027569755997874524\n",
            "Mean loss for 5720 iterations: 0.027526040241640008\n",
            "Accuracy Bag: 0.9461754086201803\n",
            "Accuracy NM: 0.9888682745825604\n",
            "Accuracy CL: 0.7124304267161411\n",
            "Mean loss for 5730 iterations: 0.02748358808900758\n",
            "Mean loss for 5740 iterations: 0.027440410820489022\n",
            "Accuracy Bag: 0.9554908726149858\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7207792207792209\n",
            "Mean loss for 5750 iterations: 0.027397366968388466\n",
            "Mean loss for 5760 iterations: 0.02735397532350261\n",
            "Accuracy Bag: 0.9276602066801558\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7133580705009277\n",
            "Mean loss for 5770 iterations: 0.027310991689459496\n",
            "Mean loss for 5780 iterations: 0.027267981445604925\n",
            "Accuracy Bag: 0.9554711663427689\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7179962894248608\n",
            "Mean loss for 5790 iterations: 0.02722588635369651\n",
            "Mean loss for 5800 iterations: 0.027184834806017723\n",
            "Accuracy Bag: 0.9202013826461545\n",
            "Accuracy NM: 0.9935064935064936\n",
            "Accuracy CL: 0.6892393320964749\n",
            "Mean loss for 5810 iterations: 0.027144092573792706\n",
            "Mean loss for 5820 iterations: 0.0271033589358828\n",
            "Accuracy Bag: 0.958216810338915\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.7458256029684601\n",
            "Mean loss for 5830 iterations: 0.027062924749864502\n",
            "Mean loss for 5840 iterations: 0.027022486051143074\n",
            "Accuracy Bag: 0.9388020750318247\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.7217068645640073\n",
            "Mean loss for 5850 iterations: 0.026980171800614933\n",
            "Mean loss for 5860 iterations: 0.026937952629408127\n",
            "Accuracy Bag: 0.9322611382458752\n",
            "Accuracy NM: 0.9907235621521338\n",
            "Accuracy CL: 0.7300556586270872\n",
            "Mean loss for 5870 iterations: 0.026896059374291063\n",
            "Mean loss for 5880 iterations: 0.026854972608711572\n",
            "Accuracy Bag: 0.9294776272952735\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.6985157699443415\n",
            "Mean loss for 5890 iterations: 0.026813810891436227\n",
            "Mean loss for 5900 iterations: 0.026772734941697632\n",
            "Accuracy Bag: 0.9388018818330774\n",
            "Accuracy NM: 0.9916512059369204\n",
            "Accuracy CL: 0.692022263450835\n",
            "Mean loss for 5910 iterations: 0.026731669746556788\n",
            "Mean loss for 5920 iterations: 0.02669096115173857\n",
            "Accuracy Bag: 0.9452577145708755\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.6725417439703155\n",
            "Mean loss for 5930 iterations: 0.026650940337372257\n",
            "Mean loss for 5940 iterations: 0.02660988932191269\n",
            "Accuracy Bag: 0.9415180630202723\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6846011131725418\n",
            "Mean loss for 5950 iterations: 0.026570087644103287\n",
            "Mean loss for 5960 iterations: 0.02653008610527375\n",
            "Accuracy Bag: 0.9322802649218501\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.6855287569573284\n",
            "Mean loss for 5970 iterations: 0.026490426863069058\n",
            "Mean loss for 5980 iterations: 0.02644983663609723\n",
            "Accuracy Bag: 0.9415375760937416\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.6799628942486087\n",
            "Mean loss for 5990 iterations: 0.026410423085656724\n",
            "Mean loss for 6000 iterations: 0.026370769184150655\n",
            "Accuracy Bag: 0.9331598988179521\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.7263450834879406\n",
            "Mean loss for 6010 iterations: 0.02633086383268555\n",
            "Mean loss for 6020 iterations: 0.02629079531758917\n",
            "Accuracy Bag: 0.9396729183849281\n",
            "Accuracy NM: 0.9888682745825604\n",
            "Accuracy CL: 0.6957328385899814\n",
            "Mean loss for 6030 iterations: 0.026251133479813893\n",
            "Mean loss for 6040 iterations: 0.026210314410835787\n",
            "Accuracy Bag: 0.9350815501571962\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7458256029684601\n",
            "Mean loss for 6050 iterations: 0.02616924412320041\n",
            "Mean loss for 6060 iterations: 0.02612900497866222\n",
            "Accuracy Bag: 0.9387827551571024\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7448979591836733\n",
            "Mean loss for 6070 iterations: 0.026088735867753707\n",
            "Mean loss for 6080 iterations: 0.026048375724971456\n",
            "Accuracy Bag: 0.9341259891534359\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6576994434137291\n",
            "Mean loss for 6090 iterations: 0.026008851890162027\n",
            "Mean loss for 6100 iterations: 0.02596910981513888\n",
            "Accuracy Bag: 0.9295627313434249\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.6892393320964749\n",
            "Mean loss for 6110 iterations: 0.02592998127560299\n",
            "Mean loss for 6120 iterations: 0.025891842654133156\n",
            "Accuracy Bag: 0.9397014152001433\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.7300556586270872\n",
            "Mean loss for 6130 iterations: 0.025853061994088003\n",
            "Mean loss for 6140 iterations: 0.025814153226445116\n",
            "Accuracy Bag: 0.9368993571698084\n",
            "Accuracy NM: 0.9907235621521334\n",
            "Accuracy CL: 0.6447124304267162\n",
            "Mean loss for 6150 iterations: 0.02577587987353071\n",
            "Mean loss for 6160 iterations: 0.025737818200088275\n",
            "Accuracy Bag: 0.9247915916814029\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7050092764378478\n",
            "Mean loss for 6170 iterations: 0.025699216620406026\n",
            "Mean loss for 6180 iterations: 0.025661953485299212\n",
            "Accuracy Bag: 0.9258429792637854\n",
            "Accuracy NM: 0.9935064935064936\n",
            "Accuracy CL: 0.6966604823747681\n",
            "Mean loss for 6190 iterations: 0.025624655916286716\n",
            "Mean loss for 6200 iterations: 0.025587672025684433\n",
            "Accuracy Bag: 0.9220754104942082\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.6493506493506493\n",
            "Mean loss for 6210 iterations: 0.025553191635040072\n",
            "Mean loss for 6220 iterations: 0.025518052388110626\n",
            "Accuracy Bag: 0.9229456742510699\n",
            "Accuracy NM: 0.9944341372912803\n",
            "Accuracy CL: 0.6215213358070502\n",
            "Mean loss for 6230 iterations: 0.025484434580284637\n",
            "Mean loss for 6240 iterations: 0.025453518515207225\n",
            "Accuracy Bag: 0.9322791057293668\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.6910946196660483\n",
            "Mean loss for 6250 iterations: 0.025423309849295764\n",
            "Mean loss for 6260 iterations: 0.025397527591602805\n",
            "Accuracy Bag: 0.91091402906907\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6419294990723562\n",
            "Mean loss for 6270 iterations: 0.02537064321847496\n",
            "Mean loss for 6280 iterations: 0.025343676628326185\n",
            "Accuracy Bag: 0.9063431399085435\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.5742115027829313\n",
            "Mean loss for 6290 iterations: 0.025316576667064106\n",
            "Mean loss for 6300 iterations: 0.02529362497575921\n",
            "Accuracy Bag: 0.901572193445269\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.6159554730983302\n",
            "Mean loss for 6310 iterations: 0.025273525621612863\n",
            "Mean loss for 6320 iterations: 0.025252309008416737\n",
            "Accuracy Bag: 0.9099487115285748\n",
            "Accuracy NM: 0.9721706864564008\n",
            "Accuracy CL: 0.5482374768089054\n",
            "Mean loss for 6330 iterations: 0.025230444606638546\n",
            "Mean loss for 6340 iterations: 0.025212964347482155\n",
            "Accuracy Bag: 0.9127507695589099\n",
            "Accuracy NM: 0.9823747680890539\n",
            "Accuracy CL: 0.6558441558441557\n",
            "Mean loss for 6350 iterations: 0.02519260609293298\n",
            "Mean loss for 6360 iterations: 0.025172774289985798\n",
            "Accuracy Bag: 0.8737788148686529\n",
            "Accuracy NM: 0.9721706864564006\n",
            "Accuracy CL: 0.5398886827458256\n",
            "Mean loss for 6370 iterations: 0.025152019857450168\n",
            "Mean loss for 6380 iterations: 0.025127724548754338\n",
            "Accuracy Bag: 0.9155526343904977\n",
            "Accuracy NM: 0.9786641929499073\n",
            "Accuracy CL: 0.6345083487940631\n",
            "Mean loss for 6390 iterations: 0.02510344842323124\n",
            "Mean loss for 6400 iterations: 0.025076507677167684\n",
            "Accuracy Bag: 0.9443488110645696\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7115027829313544\n",
            "Mean loss for 6410 iterations: 0.025048749901477086\n",
            "Mean loss for 6420 iterations: 0.02501904598077849\n",
            "Accuracy Bag: 0.9072608339578483\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.673469387755102\n",
            "Mean loss for 6430 iterations: 0.024991040882197516\n",
            "Mean loss for 6440 iterations: 0.02496087763649021\n",
            "Accuracy Bag: 0.9452382014974063\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.6901669758812615\n",
            "Mean loss for 6450 iterations: 0.024932478564924945\n",
            "Mean loss for 6460 iterations: 0.02489946016954581\n",
            "Accuracy Bag: 0.9340971059407263\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.6697588126159554\n",
            "Mean loss for 6470 iterations: 0.02486725626065334\n",
            "Mean loss for 6480 iterations: 0.02483337366205955\n",
            "Accuracy Bag: 0.9313428646003288\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.7003710575139146\n",
            "Mean loss for 6490 iterations: 0.024799369631345183\n",
            "Mean loss for 6500 iterations: 0.024764623147489215\n",
            "Accuracy Bag: 0.944330070786089\n",
            "Accuracy NM: 0.9870129870129872\n",
            "Accuracy CL: 0.6846011131725417\n",
            "Mean loss for 6510 iterations: 0.02473052114737542\n",
            "Mean loss for 6520 iterations: 0.024696647414978325\n",
            "Accuracy Bag: 0.9378836081875312\n",
            "Accuracy NM: 0.9944341372912802\n",
            "Accuracy CL: 0.7337662337662337\n",
            "Mean loss for 6530 iterations: 0.024662376588722477\n",
            "Mean loss for 6540 iterations: 0.02462750193863197\n",
            "Accuracy Bag: 0.9415848131874376\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.719851576994434\n",
            "Mean loss for 6550 iterations: 0.024592885892360136\n",
            "Mean loss for 6560 iterations: 0.024558228934070142\n",
            "Accuracy Bag: 0.9276030198509783\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7346938775510204\n",
            "Mean loss for 6570 iterations: 0.024523231009807425\n",
            "Mean loss for 6580 iterations: 0.02448912568361708\n",
            "Accuracy Bag: 0.9554338789845552\n",
            "Accuracy NM: 0.9925788497217067\n",
            "Accuracy CL: 0.7244897959183674\n",
            "Mean loss for 6590 iterations: 0.024454599352004443\n",
            "Mean loss for 6600 iterations: 0.024419984613143077\n",
            "Accuracy Bag: 0.9443013807721266\n",
            "Accuracy NM: 0.9925788497217067\n",
            "Accuracy CL: 0.7319109461966604\n",
            "Mean loss for 6610 iterations: 0.024385478632202664\n",
            "Mean loss for 6620 iterations: 0.024351752832991964\n",
            "Accuracy Bag: 0.9396631618481933\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7309833024118738\n",
            "Mean loss for 6630 iterations: 0.024317886492493456\n",
            "Mean loss for 6640 iterations: 0.024283993770573525\n",
            "Accuracy Bag: 0.9294873838320082\n",
            "Accuracy NM: 0.9935064935064936\n",
            "Accuracy CL: 0.7319109461966605\n",
            "Mean loss for 6650 iterations: 0.02424965517059287\n",
            "Mean loss for 6660 iterations: 0.024215460812137917\n",
            "Accuracy Bag: 0.9322708947826098\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.7059369202226345\n",
            "Mean loss for 6670 iterations: 0.024181948199672197\n",
            "Mean loss for 6680 iterations: 0.02414755491927335\n",
            "Accuracy Bag: 0.9545435225579824\n",
            "Accuracy NM: 0.9935064935064937\n",
            "Accuracy CL: 0.7235621521335807\n",
            "Mean loss for 6690 iterations: 0.02411374401505003\n",
            "Mean loss for 6700 iterations: 0.02408009632888038\n",
            "Accuracy Bag: 0.9368804236925807\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.7198515769944343\n",
            "Mean loss for 6710 iterations: 0.024047540228819265\n",
            "Mean loss for 6720 iterations: 0.02401409123249756\n",
            "Accuracy Bag: 0.9340785588609929\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7393320964749536\n",
            "Mean loss for 6730 iterations: 0.023980706842360363\n",
            "Mean loss for 6740 iterations: 0.023947311028150203\n",
            "Accuracy Bag: 0.922075217295461\n",
            "Accuracy NM: 0.9953617810760669\n",
            "Accuracy CL: 0.7012987012987013\n",
            "Mean loss for 6750 iterations: 0.023914641909081387\n",
            "Mean loss for 6760 iterations: 0.02388103297833142\n",
            "Accuracy Bag: 0.9564085666642904\n",
            "Accuracy NM: 0.9944341372912803\n",
            "Accuracy CL: 0.7504638218923932\n",
            "Mean loss for 6770 iterations: 0.02384782025938839\n",
            "Mean loss for 6780 iterations: 0.0238144215728954\n",
            "Accuracy Bag: 0.9313711682167969\n",
            "Accuracy NM: 0.9944341372912802\n",
            "Accuracy CL: 0.7495361781076066\n",
            "Mean loss for 6790 iterations: 0.023781721089256445\n",
            "Mean loss for 6800 iterations: 0.023748795666161802\n",
            "Accuracy Bag: 0.9378178240141021\n",
            "Accuracy NM: 0.9944341372912803\n",
            "Accuracy CL: 0.7309833024118738\n",
            "Mean loss for 6810 iterations: 0.023715747024476933\n",
            "Mean loss for 6820 iterations: 0.0236824968230306\n",
            "Accuracy Bag: 0.9508807641087732\n",
            "Accuracy NM: 0.9944341372912802\n",
            "Accuracy CL: 0.7152133580705009\n",
            "Mean loss for 6830 iterations: 0.02364918490902297\n",
            "Mean loss for 6840 iterations: 0.02361646621894467\n",
            "Accuracy Bag: 0.9378937511217603\n",
            "Accuracy NM: 0.9944341372912802\n",
            "Accuracy CL: 0.6855287569573284\n",
            "Mean loss for 6850 iterations: 0.02358367843763736\n",
            "Mean loss for 6860 iterations: 0.02355117327078862\n",
            "Accuracy Bag: 0.9396920450609031\n",
            "Accuracy NM: 0.9944341372912802\n",
            "Accuracy CL: 0.722634508348794\n",
            "Mean loss for 6870 iterations: 0.023518560400210303\n",
            "Mean loss for 6880 iterations: 0.023485727208546536\n",
            "Accuracy Bag: 0.9387925116938373\n",
            "Accuracy NM: 0.9953617810760669\n",
            "Accuracy CL: 0.7179962894248608\n",
            "Mean loss for 6890 iterations: 0.023452606287415523\n",
            "Mean loss for 6900 iterations: 0.023419782151818187\n",
            "Accuracy Bag: 0.9563995829225447\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.702226345083488\n",
            "Mean loss for 6910 iterations: 0.02338685303394175\n",
            "Mean loss for 6920 iterations: 0.023354569773599027\n",
            "Accuracy Bag: 0.9350345062622477\n",
            "Accuracy NM: 0.9860853432282005\n",
            "Accuracy CL: 0.7421150278293136\n",
            "Mean loss for 6930 iterations: 0.02332287163257886\n",
            "Mean loss for 6940 iterations: 0.023291312519955597\n",
            "Accuracy Bag: 0.9406290589849301\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.7217068645640073\n",
            "Mean loss for 6950 iterations: 0.02325973726269243\n",
            "Mean loss for 6960 iterations: 0.023228431045702134\n",
            "Accuracy Bag: 0.9489116824770863\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.741187384044527\n",
            "Mean loss for 6970 iterations: 0.023196837837595546\n",
            "Mean loss for 6980 iterations: 0.023165644672685094\n",
            "Accuracy Bag: 0.9332176652433716\n",
            "Accuracy NM: 0.9944341372912802\n",
            "Accuracy CL: 0.7319109461966604\n",
            "Mean loss for 6990 iterations: 0.023134541882174384\n",
            "Mean loss for 7000 iterations: 0.023103386861667137\n",
            "Accuracy Bag: 0.9591731441376642\n",
            "Accuracy NM: 0.9972170686456401\n",
            "Accuracy CL: 0.7300556586270872\n",
            "Mean loss for 7010 iterations: 0.023072006986444096\n",
            "Mean loss for 7020 iterations: 0.023040906116053788\n",
            "Accuracy Bag: 0.938716584586179\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7337662337662337\n",
            "Mean loss for 7030 iterations: 0.023010443005697427\n",
            "Mean loss for 7040 iterations: 0.02297964612859935\n",
            "Accuracy Bag: 0.9378744312470382\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7142857142857143\n",
            "Mean loss for 7050 iterations: 0.022949254905709218\n",
            "Mean loss for 7060 iterations: 0.022918540700485535\n",
            "Accuracy Bag: 0.9266667787219401\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.725417439703154\n",
            "Mean loss for 7070 iterations: 0.022887861754750514\n",
            "Mean loss for 7080 iterations: 0.022857543824401378\n",
            "Accuracy Bag: 0.942436723063313\n",
            "Accuracy NM: 0.9953617810760669\n",
            "Accuracy CL: 0.6725417439703153\n",
            "Mean loss for 7090 iterations: 0.022828014167093007\n",
            "Mean loss for 7100 iterations: 0.02279784923439032\n",
            "Accuracy Bag: 0.9434026202000496\n",
            "Accuracy NM: 0.9972170686456401\n",
            "Accuracy CL: 0.7031539888682746\n",
            "Mean loss for 7110 iterations: 0.022768083426485078\n",
            "Mean loss for 7120 iterations: 0.022738448535888443\n",
            "Accuracy Bag: 0.9285501767092342\n",
            "Accuracy NM: 0.9935064935064936\n",
            "Accuracy CL: 0.7448979591836735\n",
            "Mean loss for 7130 iterations: 0.022708944515860672\n",
            "Mean loss for 7140 iterations: 0.02267882757223764\n",
            "Accuracy Bag: 0.921167279782891\n",
            "Accuracy NM: 0.9953617810760669\n",
            "Accuracy CL: 0.7003710575139146\n",
            "Mean loss for 7150 iterations: 0.022649673505391522\n",
            "Mean loss for 7160 iterations: 0.02262051407525691\n",
            "Accuracy Bag: 0.9415943765254249\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.6910946196660482\n",
            "Mean loss for 7170 iterations: 0.02259099602784643\n",
            "Mean loss for 7180 iterations: 0.022561406634425244\n",
            "Accuracy Bag: 0.9360000170014898\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.725417439703154\n",
            "Mean loss for 7190 iterations: 0.02253231083474053\n",
            "Mean loss for 7200 iterations: 0.02250352344076115\n",
            "Accuracy Bag: 0.9508903274467606\n",
            "Accuracy NM: 0.9962894248608535\n",
            "Accuracy CL: 0.6576994434137292\n",
            "Mean loss for 7210 iterations: 0.02247477268584222\n",
            "Mean loss for 7220 iterations: 0.02244687965577312\n",
            "Accuracy Bag: 0.919263982324633\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.6743970315398887\n",
            "Mean loss for 7230 iterations: 0.022419701424886793\n",
            "Mean loss for 7240 iterations: 0.022392167343982115\n",
            "Accuracy Bag: 0.9192737388613679\n",
            "Accuracy NM: 0.9907235621521334\n",
            "Accuracy CL: 0.6985157699443414\n",
            "Mean loss for 7250 iterations: 0.02236543479763124\n",
            "Mean loss for 7260 iterations: 0.022338636206198317\n",
            "Accuracy Bag: 0.9118334619070996\n",
            "Accuracy NM: 0.9842300556586271\n",
            "Accuracy CL: 0.6103896103896105\n",
            "Mean loss for 7270 iterations: 0.022312435409315733\n",
            "Mean loss for 7280 iterations: 0.022285254503197564\n",
            "Accuracy Bag: 0.9451724173239772\n",
            "Accuracy NM: 0.9897959183673468\n",
            "Accuracy CL: 0.6855287569573284\n",
            "Mean loss for 7290 iterations: 0.02225875344924239\n",
            "Mean loss for 7300 iterations: 0.02223208142640444\n",
            "Accuracy Bag: 0.9219994833865499\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7003710575139146\n",
            "Mean loss for 7310 iterations: 0.022205770873716186\n",
            "Mean loss for 7320 iterations: 0.02217916329664646\n",
            "Accuracy Bag: 0.917351507925882\n",
            "Accuracy NM: 0.987012987012987\n",
            "Accuracy CL: 0.657699443413729\n",
            "Mean loss for 7330 iterations: 0.02215243982194072\n",
            "Mean loss for 7340 iterations: 0.02212588584487054\n",
            "Accuracy Bag: 0.9351004836344239\n",
            "Accuracy NM: 0.9925788497217067\n",
            "Accuracy CL: 0.6428571428571429\n",
            "Mean loss for 7350 iterations: 0.022100377670866613\n",
            "Mean loss for 7360 iterations: 0.022074909035712194\n",
            "Accuracy Bag: 0.9387925116938373\n",
            "Accuracy NM: 0.9935064935064936\n",
            "Accuracy CL: 0.6846011131725418\n",
            "Mean loss for 7370 iterations: 0.022049270417194785\n",
            "Mean loss for 7380 iterations: 0.022023653811254987\n",
            "Accuracy Bag: 0.9405623088177648\n",
            "Accuracy NM: 0.9925788497217067\n",
            "Accuracy CL: 0.640074211502783\n",
            "Mean loss for 7390 iterations: 0.021998967639343003\n",
            "Mean loss for 7400 iterations: 0.021973740636346297\n",
            "Accuracy Bag: 0.9526314345567257\n",
            "Accuracy NM: 0.9879406307977738\n",
            "Accuracy CL: 0.7059369202226345\n",
            "Mean loss for 7410 iterations: 0.021950490374958054\n",
            "Mean loss for 7420 iterations: 0.02192729226637086\n",
            "Accuracy Bag: 0.9248583418485683\n",
            "Accuracy NM: 0.9879406307977736\n",
            "Accuracy CL: 0.6261595547309834\n",
            "Mean loss for 7430 iterations: 0.021903738320669525\n",
            "Mean loss for 7440 iterations: 0.021880823005265348\n",
            "Accuracy Bag: 0.9072694313020997\n",
            "Accuracy NM: 0.9851576994434139\n",
            "Accuracy CL: 0.6790352504638218\n",
            "Mean loss for 7450 iterations: 0.02185902743409083\n",
            "Mean loss for 7460 iterations: 0.021838078662202942\n",
            "Accuracy Bag: 0.9156010306766766\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.6586270871985157\n",
            "Mean loss for 7470 iterations: 0.02182090767958506\n",
            "Mean loss for 7480 iterations: 0.021813479603360733\n",
            "Accuracy Bag: 0.8041368488550172\n",
            "Accuracy NM: 0.9526901669758814\n",
            "Accuracy CL: 0.44063079777365494\n",
            "Mean loss for 7490 iterations: 0.021811458156674554\n",
            "Mean loss for 7500 iterations: 0.021805874371988467\n",
            "Accuracy Bag: 0.8636590644891622\n",
            "Accuracy NM: 0.9536178107606678\n",
            "Accuracy CL: 0.41001855287569566\n",
            "Mean loss for 7510 iterations: 0.021808069451649214\n",
            "Mean loss for 7520 iterations: 0.02180033567422257\n",
            "Accuracy Bag: 0.8858861939595638\n",
            "Accuracy NM: 0.979591836734694\n",
            "Accuracy CL: 0.6187384044526901\n",
            "Mean loss for 7530 iterations: 0.02179252465881426\n",
            "Mean loss for 7540 iterations: 0.021781024430898608\n",
            "Accuracy Bag: 0.8729188872447675\n",
            "Accuracy NM: 0.979591836734694\n",
            "Accuracy CL: 0.5371057513914658\n",
            "Mean loss for 7550 iterations: 0.021765473434442906\n",
            "Mean loss for 7560 iterations: 0.021751222980329808\n",
            "Accuracy Bag: 0.9044006231045997\n",
            "Accuracy NM: 0.9814471243042672\n",
            "Accuracy CL: 0.614100185528757\n",
            "Mean loss for 7570 iterations: 0.021734025562453116\n",
            "Mean loss for 7580 iterations: 0.021715826013194668\n",
            "Accuracy Bag: 0.9090591278969913\n",
            "Accuracy NM: 0.9879406307977735\n",
            "Accuracy CL: 0.663265306122449\n",
            "Mean loss for 7590 iterations: 0.02169730330541356\n",
            "Mean loss for 7600 iterations: 0.021676517950953367\n",
            "Accuracy Bag: 0.909097381248941\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.6641929499072357\n",
            "Mean loss for 7610 iterations: 0.02165488424602048\n",
            "Mean loss for 7620 iterations: 0.021631240671671444\n",
            "Accuracy Bag: 0.9397299120153586\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.6558441558441559\n",
            "Mean loss for 7630 iterations: 0.021608315970572667\n",
            "Mean loss for 7640 iterations: 0.02158465461103352\n",
            "Accuracy Bag: 0.9378455480343284\n",
            "Accuracy NM: 0.9953617810760669\n",
            "Accuracy CL: 0.7142857142857142\n",
            "Mean loss for 7650 iterations: 0.021561087502961374\n",
            "Mean loss for 7660 iterations: 0.021536727302246705\n",
            "Accuracy Bag: 0.9267134362193942\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.735621521335807\n",
            "Mean loss for 7670 iterations: 0.02151212687834266\n",
            "Mean loss for 7680 iterations: 0.021486703969369123\n",
            "Accuracy Bag: 0.9471606256316392\n",
            "Accuracy NM: 0.9916512059369204\n",
            "Accuracy CL: 0.6985157699443414\n",
            "Mean loss for 7690 iterations: 0.021461450829071412\n",
            "Mean loss for 7700 iterations: 0.021435977167636233\n",
            "Accuracy Bag: 0.9517607844023697\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7300556586270872\n",
            "Mean loss for 7710 iterations: 0.021412398723748994\n",
            "Mean loss for 7720 iterations: 0.021387568698062968\n",
            "Accuracy Bag: 0.9406196888456897\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7235621521335808\n",
            "Mean loss for 7730 iterations: 0.021362724320030873\n",
            "Mean loss for 7740 iterations: 0.021337847676501837\n",
            "Accuracy Bag: 0.9331593192217106\n",
            "Accuracy NM: 0.9962894248608535\n",
            "Accuracy CL: 0.7133580705009277\n",
            "Mean loss for 7750 iterations: 0.02131331363654012\n",
            "Mean loss for 7760 iterations: 0.021288450180192327\n",
            "Accuracy Bag: 0.9444056114962529\n",
            "Accuracy NM: 0.9925788497217067\n",
            "Accuracy CL: 0.7207792207792209\n",
            "Mean loss for 7770 iterations: 0.021263509081571198\n",
            "Mean loss for 7780 iterations: 0.0212380464535251\n",
            "Accuracy Bag: 0.9554813092769983\n",
            "Accuracy NM: 0.9916512059369204\n",
            "Accuracy CL: 0.676252319109462\n",
            "Mean loss for 7790 iterations: 0.021213263306319993\n",
            "Mean loss for 7800 iterations: 0.021188335309048758\n",
            "Accuracy Bag: 0.9443394409253293\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.7003710575139146\n",
            "Mean loss for 7810 iterations: 0.021163497074703115\n",
            "Mean loss for 7820 iterations: 0.02113762624572989\n",
            "Accuracy Bag: 0.9378176308153546\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.6975881261595547\n",
            "Mean loss for 7830 iterations: 0.02111237021734961\n",
            "Mean loss for 7840 iterations: 0.02108723866013694\n",
            "Accuracy Bag: 0.9350532465407281\n",
            "Accuracy NM: 0.989795918367347\n",
            "Accuracy CL: 0.7077922077922078\n",
            "Mean loss for 7850 iterations: 0.02106144668125276\n",
            "Mean loss for 7860 iterations: 0.02103595971684696\n",
            "Accuracy Bag: 0.9443681309392918\n",
            "Accuracy NM: 0.9935064935064936\n",
            "Accuracy CL: 0.7105751391465677\n",
            "Mean loss for 7870 iterations: 0.02101059265386778\n",
            "Mean loss for 7880 iterations: 0.020985334763352667\n",
            "Accuracy Bag: 0.9489593059682764\n",
            "Accuracy NM: 0.9953617810760669\n",
            "Accuracy CL: 0.7031539888682746\n",
            "Mean loss for 7890 iterations: 0.020960123828548897\n",
            "Mean loss for 7900 iterations: 0.020935003312606337\n",
            "Accuracy Bag: 0.9647105100311687\n",
            "Accuracy NM: 0.9935064935064936\n",
            "Accuracy CL: 0.7328385899814471\n",
            "Mean loss for 7910 iterations: 0.020909777979903156\n",
            "Mean loss for 7920 iterations: 0.020884654312342744\n",
            "Accuracy Bag: 0.9350633894749574\n",
            "Accuracy NM: 0.9953617810760669\n",
            "Accuracy CL: 0.699443413729128\n",
            "Mean loss for 7930 iterations: 0.02085979862806696\n",
            "Mean loss for 7940 iterations: 0.020834883830646217\n",
            "Accuracy Bag: 0.9378560773660518\n",
            "Accuracy NM: 0.9953617810760669\n",
            "Accuracy CL: 0.7476808905380334\n",
            "Mean loss for 7950 iterations: 0.020809754800608267\n",
            "Mean loss for 7960 iterations: 0.02078444502086546\n",
            "Accuracy Bag: 0.9425027004354892\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7476808905380334\n",
            "Mean loss for 7970 iterations: 0.020759558215997852\n",
            "Mean loss for 7980 iterations: 0.02073477836474238\n",
            "Accuracy Bag: 0.9545628424327046\n",
            "Accuracy NM: 0.9953617810760669\n",
            "Accuracy CL: 0.7096474953617812\n",
            "Mean loss for 7990 iterations: 0.02070957163965256\n",
            "Mean loss for 8000 iterations: 0.020684866443063582\n",
            "Accuracy Bag: 0.9443394409253294\n",
            "Accuracy NM: 0.9944341372912802\n",
            "Accuracy CL: 0.7282003710575139\n",
            "Mean loss for 8010 iterations: 0.020660275043513827\n",
            "Mean loss for 8020 iterations: 0.02063608932348553\n",
            "Accuracy Bag: 0.931285677771151\n",
            "Accuracy NM: 0.9972170686456402\n",
            "Accuracy CL: 0.7263450834879406\n",
            "Mean loss for 8030 iterations: 0.020611622641610046\n",
            "Mean loss for 8040 iterations: 0.02058706933610086\n",
            "Accuracy Bag: 0.9387737714153567\n",
            "Accuracy NM: 0.9944341372912802\n",
            "Accuracy CL: 0.7291280148423005\n",
            "Mean loss for 8050 iterations: 0.020562546728795282\n",
            "Mean loss for 8060 iterations: 0.02053797753383794\n",
            "Accuracy Bag: 0.9480787060784382\n",
            "Accuracy NM: 0.9944341372912802\n",
            "Accuracy CL: 0.712430426716141\n",
            "Mean loss for 8070 iterations: 0.020513525145662338\n",
            "Mean loss for 8080 iterations: 0.02048899444105358\n",
            "Accuracy Bag: 0.9545345388162364\n",
            "Accuracy NM: 0.9935064935064936\n",
            "Accuracy CL: 0.7495361781076066\n",
            "Mean loss for 8090 iterations: 0.02046455780945697\n",
            "Mean loss for 8100 iterations: 0.02044038417591551\n",
            "Accuracy Bag: 0.9526694947099285\n",
            "Accuracy NM: 0.9935064935064937\n",
            "Accuracy CL: 0.7291280148423005\n",
            "Mean loss for 8110 iterations: 0.02041613083258959\n",
            "Mean loss for 8120 iterations: 0.02039227821091825\n",
            "Accuracy Bag: 0.9452575213721285\n",
            "Accuracy NM: 0.9953617810760669\n",
            "Accuracy CL: 0.7300556586270872\n",
            "Mean loss for 8130 iterations: 0.020368452294713398\n",
            "Mean loss for 8140 iterations: 0.02034451016122128\n",
            "Accuracy Bag: 0.948069142740451\n",
            "Accuracy NM: 0.9944341372912802\n",
            "Accuracy CL: 0.7291280148423005\n",
            "Mean loss for 8150 iterations: 0.0203206210309676\n",
            "Mean loss for 8160 iterations: 0.02029675660728839\n",
            "Accuracy Bag: 0.9499250099062657\n",
            "Accuracy NM: 0.9935064935064936\n",
            "Accuracy CL: 0.7244897959183674\n",
            "Mean loss for 8170 iterations: 0.0202730992615809\n",
            "Mean loss for 8180 iterations: 0.020249450485230084\n",
            "Accuracy Bag: 0.9452573281733813\n",
            "Accuracy NM: 0.9925788497217067\n",
            "Accuracy CL: 0.7727272727272727\n",
            "Mean loss for 8190 iterations: 0.0202257399615933\n",
            "Mean loss for 8200 iterations: 0.02020226071393664\n",
            "Accuracy Bag: 0.935981083524262\n",
            "Accuracy NM: 0.9935064935064936\n",
            "Accuracy CL: 0.7115027829313544\n",
            "Mean loss for 8210 iterations: 0.020178645730336453\n",
            "Mean loss for 8220 iterations: 0.02015523463515773\n",
            "Accuracy Bag: 0.9359623432457815\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.7374768089053803\n",
            "Mean loss for 8230 iterations: 0.020132101375523986\n",
            "Mean loss for 8240 iterations: 0.020108738854974346\n",
            "Accuracy Bag: 0.9434309238165176\n",
            "Accuracy NM: 0.9916512059369204\n",
            "Accuracy CL: 0.7300556586270872\n",
            "Mean loss for 8250 iterations: 0.020085598306554004\n",
            "Mean loss for 8260 iterations: 0.02006221422357961\n",
            "Accuracy Bag: 0.9285784803257022\n",
            "Accuracy NM: 0.9935064935064936\n",
            "Accuracy CL: 0.6948051948051948\n",
            "Mean loss for 8270 iterations: 0.02003909077009616\n",
            "Mean loss for 8280 iterations: 0.020015945803174544\n",
            "Accuracy Bag: 0.9526794444454105\n",
            "Accuracy NM: 0.9944341372912802\n",
            "Accuracy CL: 0.7439703153988867\n",
            "Mean loss for 8290 iterations: 0.01999317486503988\n",
            "Mean loss for 8300 iterations: 0.019970041232203563\n",
            "Accuracy Bag: 0.9452955815253311\n",
            "Accuracy NM: 0.9907235621521336\n",
            "Accuracy CL: 0.7263450834879407\n",
            "Mean loss for 8310 iterations: 0.019947093202396706\n",
            "Mean loss for 8320 iterations: 0.01992394098123651\n",
            "Accuracy Bag: 0.9229368837080714\n",
            "Accuracy NM: 0.9935064935064936\n",
            "Accuracy CL: 0.7161410018552875\n",
            "Mean loss for 8330 iterations: 0.019901130382061878\n",
            "Mean loss for 8340 iterations: 0.019878243812381784\n",
            "Accuracy Bag: 0.9350060094470323\n",
            "Accuracy NM: 0.9916512059369204\n",
            "Accuracy CL: 0.7300556586270872\n",
            "Mean loss for 8350 iterations: 0.019855635413313758\n",
            "Mean loss for 8360 iterations: 0.019833036392887343\n",
            "Accuracy Bag: 0.952669881107423\n",
            "Accuracy NM: 0.9944341372912803\n",
            "Accuracy CL: 0.7606679035250464\n",
            "Mean loss for 8370 iterations: 0.019810482939662692\n",
            "Mean loss for 8380 iterations: 0.019788232645605995\n",
            "Accuracy Bag: 0.9461666180771817\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7179962894248608\n",
            "Mean loss for 8390 iterations: 0.019766120213413188\n",
            "Mean loss for 8400 iterations: 0.01974394537592681\n",
            "Accuracy Bag: 0.9341734194458788\n",
            "Accuracy NM: 0.992578849721707\n",
            "Accuracy CL: 0.6948051948051949\n",
            "Mean loss for 8410 iterations: 0.019721982898184387\n",
            "Mean loss for 8420 iterations: 0.019699706548913117\n",
            "Accuracy Bag: 0.9350440696002352\n",
            "Accuracy NM: 0.9935064935064933\n",
            "Accuracy CL: 0.7532467532467533\n",
            "Mean loss for 8430 iterations: 0.019677678408372007\n",
            "Mean loss for 8440 iterations: 0.0196555757365312\n",
            "Accuracy Bag: 0.9378554977698103\n",
            "Accuracy NM: 0.9935064935064937\n",
            "Accuracy CL: 0.7105751391465677\n",
            "Mean loss for 8450 iterations: 0.01963379974774231\n",
            "Mean loss for 8460 iterations: 0.01961217394103662\n",
            "Accuracy Bag: 0.9443302639848362\n",
            "Accuracy NM: 0.9916512059369201\n",
            "Accuracy CL: 0.7346938775510203\n",
            "Mean loss for 8470 iterations: 0.01959038031864723\n",
            "Mean loss for 8480 iterations: 0.01957045098462982\n",
            "Accuracy Bag: 0.9305005180624406\n",
            "Accuracy NM: 0.9888682745825602\n",
            "Accuracy CL: 0.6948051948051948\n",
            "Mean loss for 8490 iterations: 0.019552882232045297\n",
            "Mean loss for 8500 iterations: 0.01953785466462175\n",
            "Accuracy Bag: 0.9193207827563163\n",
            "Accuracy NM: 0.9814471243042674\n",
            "Accuracy CL: 0.6632653061224489\n",
            "Mean loss for 8510 iterations: 0.01953046544307489\n",
            "Mean loss for 8520 iterations: 0.019562232359293004\n",
            "Accuracy Bag: 0.5906222873688397\n",
            "Accuracy NM: 0.8181818181818182\n",
            "Accuracy CL: 0.22727272727272724\n",
            "Mean loss for 8530 iterations: 0.019647775966488683\n",
            "Mean loss for 8540 iterations: 0.019711976411168078\n",
            "Accuracy Bag: 0.5422130568736541\n",
            "Accuracy NM: 0.7996289424860854\n",
            "Accuracy CL: 0.24397031539888683\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a3d2c28789b4>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mseqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-712c5dcdd382>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# torch.Size([3840, 64, 16, 16])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# torch.Size([3840, 128, 16, 16])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# torch.Size([3840, 256, 16, 16])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# torch.Size([3840, 512, 16, 16]) (n*s, c=512, h, w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (n, c, s, h, w) = torch.Size([64, 512, 30, 16, 16]),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from einops import rearrange\n",
        "from torch.cuda.amp import autocast\n",
        "from torch.cuda.amp import GradScaler\n",
        "from torch.optim import SGD\n",
        "\n",
        "model = CustomModel()\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "scaler = GradScaler()\n",
        "optimizer = SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.1,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "loss_fn = TripletLossIterative()\n",
        "iter = 0\n",
        "losses = np.array([])\n",
        "for batch in train_loader:\n",
        "  seqs, labs, _, _ = batch\n",
        "  if torch.cuda.is_available():\n",
        "    seqs = seqs.cuda()\n",
        "    labs = labs.cuda()\n",
        "  optimizer.zero_grad()\n",
        "  with autocast():\n",
        "    seqs = seqs.reshape(-1, 1, 64, 64)\n",
        "    output = model(seqs)\n",
        "\n",
        "  loss = loss_fn(output, labs)\n",
        "  scaler.scale(loss).backward()\n",
        "  scaler.step(optimizer)\n",
        "  scaler.update()\n",
        "  torch.cuda.empty_cache()\n",
        "  losses = np.append(losses, loss.item())\n",
        "  iter += 1\n",
        "  if iter % 10 == 0:\n",
        "    print(f\"Mean loss for {iter} iterations: {losses.mean()}\")\n",
        "  if iter % 20 == 0:\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "    seq_types = []\n",
        "    views = []\n",
        "    with torch.no_grad():\n",
        "      for batch in test_loader:\n",
        "        test_seqs, test_labs, batch_types, batch_views = batch\n",
        "        if torch.cuda.is_available():\n",
        "          test_seqs = test_seqs.cuda()\n",
        "          test_labs = test_labs.cuda()\n",
        "        test_seqs = test_seqs.reshape(-1, 1, 64, 64)\n",
        "        test_output = model(test_seqs)\n",
        "        features.append(test_output.cpu())\n",
        "        labels.append(test_labs.cpu())\n",
        "        seq_types.append(batch_types)\n",
        "        views.append(batch_views)\n",
        "    features = torch.cat(features, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "    views = np.array(views).reshape(-1)\n",
        "    seq_types = np.array(seq_types).reshape(-1)\n",
        "    gallery_seq = ['nm-01', 'nm-02', 'nm-03', 'nm-04']\n",
        "    probe_seq = ['bg-01', 'bg-02']\n",
        "    acc = eval(probe_seq, gallery_seq, features, labels, views, seq_types)\n",
        "    print(f\"Accuracy Bag: {acc}\")\n",
        "    probe_seq = ['nm-05', 'nm-06']\n",
        "    acc = eval(probe_seq, gallery_seq, features, labels, views, seq_types)\n",
        "    print(f\"Accuracy NM: {acc}\")\n",
        "    probe_seq = ['cl-01','cl-02']\n",
        "    acc = eval(probe_seq, gallery_seq, features, labels, views, seq_types)\n",
        "    print(f\"Accuracy CL: {acc}\")\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNJZm_RUntbf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyP2B/1l/UY092bEamzS2WEq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}